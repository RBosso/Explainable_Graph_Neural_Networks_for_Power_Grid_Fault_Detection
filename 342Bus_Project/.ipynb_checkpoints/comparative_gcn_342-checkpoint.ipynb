{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4b832",
   "metadata": {
    "id": "e4f4b832"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a039bf15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a039bf15",
    "outputId": "f6c0c2bd-edaa-4d5a-fe6a-14015c1e6e9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.6.0-py3-none-any.whl.metadata (63 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/63.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.5)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.11.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.8.30)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from multidict<7.0,>=4.5->aiohttp->torch_geometric) (4.12.2)\n",
      "Downloading torch_geometric-2.6.0-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RTS1XZctTHay",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTS1XZctTHay",
    "outputId": "d8d8b88f-243d-4a6f-9087-530ed5d02d57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available!\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available!\")\n",
    "else:\n",
    "    print(\"GPU is not available.\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc7b329",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 160
    },
    "id": "fdc7b329",
    "outputId": "e0db5584-8658-4f8f-fa08-de4430eaa423"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-4b644f96-a3e1-4464-8c34-dc0d3d3fef3c\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-4b644f96-a3e1-4464-8c34-dc0d3d3fef3c\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving numpy_data.zip to numpy_data.zip\n",
      "Archive:  numpy_data.zip\n",
      "  inflating: AdjacencyMatrix.npy     \n",
      "  inflating: new_data.npy            \n",
      "  inflating: new_labels.npy          \n",
      "  inflating: NodeIndex.npy           \n"
     ]
    }
   ],
   "source": [
    "\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "#jupytext --to ipynb Comparative_Cheb_GCN.py\n",
    "!unzip numpy_data.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475ec1b",
   "metadata": {
    "id": "d475ec1b"
   },
   "source": [
    "Comparative Chebyshev GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6af581",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e6af581",
    "outputId": "f7c379df-abfd-47b1-f51f-1127612af7b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1+cu121\n",
      "/content\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c013720e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c013720e",
    "outputId": "2c2acb66-5a60-47cd-e166-75fef64e8506"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[52.7097 -75.9 52.7097 164.1 52.7097 44.1]\n",
      "  [52.709 -75.9 52.7092 164.1 52.7092 44.1]\n",
      "  [52.7087 -75.9 52.7089 164.1 52.7089 44.1]\n",
      "  ...\n",
      "  [0.106712 -107.9 0.106748 132.1 0.106717 12.1]\n",
      "  [0.104521 -109.5 0.104336 130.4 0.104066 10.6]\n",
      "  [0.10452 -109.5 0.104335 130.4 0.104065 10.6]]\n",
      "\n",
      " [[85214.3 -7.3 500115.0 -33.9 416832.0 -5.1]\n",
      "  [211.546 -91.2 462267.0 -36.1 379804.0 -4.7]\n",
      "  [211.568 -91.1 462266.0 -36.1 379804.0 -4.7]\n",
      "  ...\n",
      "  [454.247 176.4 552.921 -35.0 288.794 90.0]\n",
      "  [450.326 175.5 548.222 -35.9 286.192 89.1]\n",
      "  [450.325 175.5 548.221 -35.9 286.191 89.1]]\n",
      "\n",
      " [[139420.0 0.0 138954.0 -120.1 139023.0 120.1]\n",
      "  [139419.0 0.0 138921.0 -120.1 138988.0 120.1]\n",
      "  [139419.0 0.0 138914.0 -120.1 138980.0 120.1]\n",
      "  ...\n",
      "  [252.616 -12.5 255.72 -169.3 102.452 87.5]\n",
      "  [250.213 -13.6 254.539 -169.7 104.581 86.1]\n",
      "  [250.529 -13.0 253.277 -170.0 100.968 87.1]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[138947.0 0.0 138946.0 -120.0 138948.0 120.0]\n",
      "  [138908.0 0.0 138910.0 -120.0 138911.0 120.0]\n",
      "  [138899.0 0.0 138902.0 -120.0 138903.0 120.0]\n",
      "  ...\n",
      "  [144.727 -43.0 144.495 -163.1 144.729 76.9]\n",
      "  [145.753 -44.9 145.093 -164.9 144.594 75.4]\n",
      "  [142.193 -44.7 141.791 -165.0 141.772 75.3]]\n",
      "\n",
      " [[138946.0 0.0 138945.0 -120.0 138946.0 120.0]\n",
      "  [138906.0 0.0 138909.0 -120.0 138910.0 120.0]\n",
      "  [138897.0 0.0 138900.0 -120.0 138901.0 120.0]\n",
      "  ...\n",
      "  [144.128 -43.2 143.913 -163.3 144.138 76.7]\n",
      "  [144.573 -45.4 143.873 -165.5 143.305 74.9]\n",
      "  [141.022 -45.2 140.583 -165.5 140.495 74.7]]\n",
      "\n",
      " [[138944.0 0.0 138943.0 -120.0 138945.0 120.0]\n",
      "  [138904.0 0.0 138907.0 -120.0 138908.0 120.0]\n",
      "  [138895.0 0.0 138899.0 -120.0 138899.0 120.0]\n",
      "  ...\n",
      "  [143.533 -43.5 143.337 -163.5 143.551 76.5]\n",
      "  [143.405 -45.9 142.666 -166.0 142.031 74.4]\n",
      "  [139.863 -45.8 139.388 -166.1 139.234 74.2]]]\n",
      "(31200, 390, 6)\n",
      "73008000\n",
      "[['New Fault.F1 phases=3 bus1=P1 bus2=P1.0.0.0 r=0.0001' 0\n",
      "  'Three Phase to Ground' 'P1']\n",
      " ['New Fault.F1 phases=1 bus1=P2.1 bus2=P2.0 r=0.0001' 1\n",
      "  'Single Line to Ground' 'P2']\n",
      " ['New Fault.F1 phases=1 bus1=P97.2 bus2=P97.3 r=0.0001' 0\n",
      "  'Single Line to Line' 'P97']\n",
      " ...\n",
      " ['New Fault.F1 phases=3 bus1=P128 bus2=P128.4.4.4 r=0.0001' 1\n",
      "  'Three Phase to Neutral' 'P128']\n",
      " ['New Fault.F1 phases=3 bus1=P128 bus2=P128.4.4.4 r=0.0001' 1\n",
      "  'Three Phase to Neutral' 'P128']\n",
      " ['New Fault.F1 phases=3 bus1=P128 bus2=P128.4.4.4 r=0.0001' 1\n",
      "  'Three Phase to Neutral' 'P128']]\n",
      "(31200, 4)\n",
      "124800\n",
      "[[  0   1]\n",
      " [  1   2]\n",
      " [  1   3]\n",
      " ...\n",
      " [170 387]\n",
      " [171 388]\n",
      " [173 389]]\n",
      "(780, 2)\n",
      "1560\n",
      "[[0. 1. 0. ... 0. 0. 0.]\n",
      " [1. 0. 1. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(390, 390)\n",
      "152100\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "#Extract Training Data\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "numpy_data = np.load(path+\"/new_data.npy\",allow_pickle = True)\n",
    "numpy_data = np.squeeze(numpy_data)\n",
    "print(numpy_data)\n",
    "print(numpy_data.shape)\n",
    "print(numpy_data.size)\n",
    "\n",
    "value_data = np.load(path+\"/new_labels.npy\",allow_pickle = True)\n",
    "print(value_data)\n",
    "print(value_data.shape)\n",
    "print(value_data.size)\n",
    "\n",
    "\n",
    "NodeIndex = np.load(path+\"/NodeIndex.npy\",allow_pickle = True)\n",
    "print(NodeIndex)\n",
    "print(NodeIndex.shape)\n",
    "print(NodeIndex.size)\n",
    "\n",
    "AdjacencyMatrix = np.load(path+\"/AdjacencyMatrix.npy\",allow_pickle = True)\n",
    "print(AdjacencyMatrix)\n",
    "print(AdjacencyMatrix.shape)\n",
    "print(AdjacencyMatrix.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4388427",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c4388427",
    "outputId": "08d7c782-8956-45ef-8d0d-8db6f7fa5464"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P1', 'P2', 'P3', 'P7', 'P6', 'P11', 'P27', 'P44', 'P45', 'P62', 'P10', 'P81', 'P97', 'P116', 'P117', 'P134', 'P12', 'P13', 'P14', 'P15', 'P16', 'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P28', 'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37', 'P38', 'P39', 'P40', 'P41', 'P42', 'P43', 'P46', 'P47', 'P48', 'P49', 'P50', 'P51', 'P52', 'P53', 'P54', 'P55', 'P56', 'P57', 'P58', 'P59', 'P60', 'P61', 'P63', 'P64', 'P65', 'P66', 'P67', 'P68', 'P69', 'P70', 'P71', 'P72', 'P73', 'P74', 'P75', 'P76', 'P77', 'P78', 'P79', 'P80', 'P82', 'P83', 'P84', 'P85', 'P86', 'P87', 'P88', 'P89', 'P90', 'P91', 'P92', 'P93', 'P94', 'P95', 'P96', 'P98', 'P99', 'P100', 'P101', 'P102', 'P103', 'P104', 'P105', 'P106', 'P107', 'P108', 'P109', 'P110', 'P111', 'P112', 'P113', 'P114', 'P115', 'P118', 'P119', 'P120', 'P121', 'P122', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P129', 'P130', 'P131', 'P132', 'P133', 'P135', 'P136', 'P137', 'P138', 'P139', 'P140', 'P141', 'P142', 'P143', 'P144', 'P145', 'P146', 'P147', 'P148', 'P149', 'P150', 'S194', 'S193', 'S195', 'S199', 'S198', 'S200', 'S204', 'S203', 'S205', 'S206', 'S211', 'S210', 'S212', 'S213', 'S218', 'S217', 'S219', 'S220', 'S225', 'S224', 'S226', 'S227', 'S232', 'S231', 'S233', 'S237', 'S236', 'S239', 'S1', 'S2', 'S3', 'S4', 'S5', 'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17', 'S18', 'S19', 'S20', 'S21', 'S42', 'S43', 'S44', 'S45', 'S46', 'S47', 'S48', 'S49', 'S50', 'S51', 'S52', 'S53', 'S54', 'S55', 'S56', 'S57', 'S58', 'S59', 'S60', 'S61', 'S62', 'S83', 'S84', 'S85', 'S86', 'S87', 'S88', 'S89', 'S90', 'S91', 'S92', 'S93', 'S94', 'S95', 'S96', 'S97', 'S98', 'S99', 'S100', 'S101', 'S102', 'S103', 'S124', 'S125', 'S126', 'S127', 'S128', 'S129', 'S130', 'S131', 'S132', 'S133', 'S134', 'S135', 'S136', 'S137', 'S138', 'S139', 'S140', 'S141', 'S142', 'S143', 'S144', 'S22', 'S27', 'S32', 'S37', 'S63', 'S68', 'S73', 'S78', 'S104', 'S109', 'S114', 'S119', 'S23', 'S28', 'S33', 'S38', 'S64', 'S69', 'S74', 'S79', 'S105', 'S110', 'S115', 'S120', 'S24', 'S29', 'S34', 'S39', 'S65', 'S70', 'S75', 'S80', 'S106', 'S111', 'S116', 'S121', 'S25', 'S30', 'S35', 'S40', 'S66', 'S71', 'S76', 'S81', 'S107', 'S112', 'S117', 'S122', 'S26', 'S31', 'S36', 'S41', 'S67', 'S72', 'S77', 'S82', 'S108', 'S113', 'S118', 'S123', 'P4', 'P5', 'P8', 'P9', 'S148', 'S145', 'S152', 'S149', 'S156', 'S153', 'S160', 'S157', 'S147', 'S146', 'S151', 'S150', 'S155', 'S154', 'S159', 'S158', 'S164', 'S161', 'S168', 'S165', 'S172', 'S169', 'S176', 'S173', 'S163', 'S162', 'S167', 'S166', 'S171', 'S170', 'S175', 'S174', 'S180', 'S177', 'S184', 'S181', 'S188', 'S185', 'S192', 'S189', 'S179', 'S178', 'S183', 'S182', 'S187', 'S186', 'S191', 'S190', 'S196', 'S197', 'S201', 'S202', 'S207', 'S208', 'S209', 'S214', 'S215', 'S216', 'S221', 'S222', 'S223', 'S228', 'S229', 'S230', 'S234', 'S235', 'S238', 'S240']\n",
      "[0, 1, 2, 3, 2, 4, 5, 6, 7, 8, 3, 9, 10, 11, 12, 13, 4, 14, 15, 14, 16, 17, 14, 18, 14, 19, 20, 14, 21, 14, 22, 5, 14, 23, 14, 24, 14, 25, 14, 26, 14, 27, 14, 28, 29, 14, 30, 7, 14, 31, 14, 32, 14, 33, 14, 34, 35, 36, 14, 37, 14, 38, 39, 8, 14, 40, 14, 41, 14, 42, 14, 43, 14, 44, 45, 14, 46, 14, 47, 14, 48, 9, 14, 49, 14, 50, 51, 14, 52, 14, 53, 54, 14, 55, 14, 56, 10, 14, 57, 14, 58, 14, 59, 14, 60, 61, 14, 62, 14, 63, 14, 64, 14, 65, 12, 14, 66, 14, 67, 14, 68, 14, 69, 14, 70, 14, 71, 72, 14, 73, 13, 14, 74, 14, 75, 76, 14, 77, 14, 78, 79, 80, 14, 81, 14, 82, 29, 83, 63, 39, 84, 72, 20, 85, 45, 78, 46, 86, 54, 80, 25, 87, 35, 61, 34, 88, 59, 68, 42, 89, 75, 16, 90, 51, 91, 92, 93, 73, 94, 95, 96, 97, 56, 98, 99, 100, 101, 38, 102, 103, 104, 105, 22, 106, 107, 108, 109, 55, 70, 110, 111, 112, 81, 44, 113, 114, 115, 21, 79, 116, 117, 118, 47, 27, 119, 120, 121, 122, 36, 74, 123, 124, 125, 77, 32, 126, 127, 128, 18, 58, 129, 130, 131, 33, 76, 132, 133, 134, 135, 40, 136, 137, 138, 139, 31, 140, 141, 142, 143, 23, 144, 145, 146, 147, 24, 148, 149, 150, 151, 82, 152, 153, 154, 62, 155, 156, 157, 17, 158, 159, 160, 65, 64, 161, 162, 19, 69, 163, 164, 67, 66, 165, 166, 48, 71, 167, 168, 53, 60, 169, 170, 50, 57, 171, 172, 30, 28, 173, 174, 52, 26, 175, 176, 15, 41, 177, 178, 179, 37, 180, 181, 182, 43, 183, 184, 185, 49, 186, 2, 2, 3, 3, 82, 73, 65, 56, 48, 38, 30, 22, 55, 64, 81, 71, 21, 28, 47, 37, 62, 70, 19, 44, 53, 79, 52, 27, 36, 69, 77, 60, 18, 26, 33, 43, 17, 74, 67, 32, 50, 58, 15, 76, 40, 66, 31, 57, 23, 41, 24, 49, 29, 63, 39, 72, 20, 45, 78, 46, 54, 80, 25, 35, 61, 34, 59, 68, 42, 75, 16, 51]\n",
      "390\n",
      "tensor([ 0,  1, 10,  ..., 70, 70, 70])\n",
      "torch.int64\n",
      "tensor([[[ 5.2710e+01, -7.5900e+01,  5.2710e+01,  1.6410e+02,  5.2710e+01,\n",
      "           4.4100e+01],\n",
      "         [ 5.2709e+01, -7.5900e+01,  5.2709e+01,  1.6410e+02,  5.2709e+01,\n",
      "           4.4100e+01],\n",
      "         [ 5.2709e+01, -7.5900e+01,  5.2709e+01,  1.6410e+02,  5.2709e+01,\n",
      "           4.4100e+01],\n",
      "         ...,\n",
      "         [ 1.0671e-01, -1.0790e+02,  1.0675e-01,  1.3210e+02,  1.0672e-01,\n",
      "           1.2100e+01],\n",
      "         [ 1.0452e-01, -1.0950e+02,  1.0434e-01,  1.3040e+02,  1.0407e-01,\n",
      "           1.0600e+01],\n",
      "         [ 1.0452e-01, -1.0950e+02,  1.0434e-01,  1.3040e+02,  1.0407e-01,\n",
      "           1.0600e+01]],\n",
      "\n",
      "        [[ 8.5214e+04, -7.3000e+00,  5.0012e+05, -3.3900e+01,  4.1683e+05,\n",
      "          -5.1000e+00],\n",
      "         [ 2.1155e+02, -9.1200e+01,  4.6227e+05, -3.6100e+01,  3.7980e+05,\n",
      "          -4.7000e+00],\n",
      "         [ 2.1157e+02, -9.1100e+01,  4.6227e+05, -3.6100e+01,  3.7980e+05,\n",
      "          -4.7000e+00],\n",
      "         ...,\n",
      "         [ 4.5425e+02,  1.7640e+02,  5.5292e+02, -3.5000e+01,  2.8879e+02,\n",
      "           9.0000e+01],\n",
      "         [ 4.5033e+02,  1.7550e+02,  5.4822e+02, -3.5900e+01,  2.8619e+02,\n",
      "           8.9100e+01],\n",
      "         [ 4.5033e+02,  1.7550e+02,  5.4822e+02, -3.5900e+01,  2.8619e+02,\n",
      "           8.9100e+01]],\n",
      "\n",
      "        [[ 1.3942e+05,  0.0000e+00,  1.3895e+05, -1.2010e+02,  1.3902e+05,\n",
      "           1.2010e+02],\n",
      "         [ 1.3942e+05,  0.0000e+00,  1.3892e+05, -1.2010e+02,  1.3899e+05,\n",
      "           1.2010e+02],\n",
      "         [ 1.3942e+05,  0.0000e+00,  1.3891e+05, -1.2010e+02,  1.3898e+05,\n",
      "           1.2010e+02],\n",
      "         ...,\n",
      "         [ 2.5262e+02, -1.2500e+01,  2.5572e+02, -1.6930e+02,  1.0245e+02,\n",
      "           8.7500e+01],\n",
      "         [ 2.5021e+02, -1.3600e+01,  2.5454e+02, -1.6970e+02,  1.0458e+02,\n",
      "           8.6100e+01],\n",
      "         [ 2.5053e+02, -1.3000e+01,  2.5328e+02, -1.7000e+02,  1.0097e+02,\n",
      "           8.7100e+01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.3895e+05,  0.0000e+00,  1.3895e+05, -1.2000e+02,  1.3895e+05,\n",
      "           1.2000e+02],\n",
      "         [ 1.3891e+05,  0.0000e+00,  1.3891e+05, -1.2000e+02,  1.3891e+05,\n",
      "           1.2000e+02],\n",
      "         [ 1.3890e+05,  0.0000e+00,  1.3890e+05, -1.2000e+02,  1.3890e+05,\n",
      "           1.2000e+02],\n",
      "         ...,\n",
      "         [ 1.4473e+02, -4.3000e+01,  1.4449e+02, -1.6310e+02,  1.4473e+02,\n",
      "           7.6900e+01],\n",
      "         [ 1.4575e+02, -4.4900e+01,  1.4509e+02, -1.6490e+02,  1.4459e+02,\n",
      "           7.5400e+01],\n",
      "         [ 1.4219e+02, -4.4700e+01,  1.4179e+02, -1.6500e+02,  1.4177e+02,\n",
      "           7.5300e+01]],\n",
      "\n",
      "        [[ 1.3895e+05,  0.0000e+00,  1.3894e+05, -1.2000e+02,  1.3895e+05,\n",
      "           1.2000e+02],\n",
      "         [ 1.3891e+05,  0.0000e+00,  1.3891e+05, -1.2000e+02,  1.3891e+05,\n",
      "           1.2000e+02],\n",
      "         [ 1.3890e+05,  0.0000e+00,  1.3890e+05, -1.2000e+02,  1.3890e+05,\n",
      "           1.2000e+02],\n",
      "         ...,\n",
      "         [ 1.4413e+02, -4.3200e+01,  1.4391e+02, -1.6330e+02,  1.4414e+02,\n",
      "           7.6700e+01],\n",
      "         [ 1.4457e+02, -4.5400e+01,  1.4387e+02, -1.6550e+02,  1.4330e+02,\n",
      "           7.4900e+01],\n",
      "         [ 1.4102e+02, -4.5200e+01,  1.4058e+02, -1.6550e+02,  1.4049e+02,\n",
      "           7.4700e+01]],\n",
      "\n",
      "        [[ 1.3894e+05,  0.0000e+00,  1.3894e+05, -1.2000e+02,  1.3894e+05,\n",
      "           1.2000e+02],\n",
      "         [ 1.3890e+05,  0.0000e+00,  1.3891e+05, -1.2000e+02,  1.3891e+05,\n",
      "           1.2000e+02],\n",
      "         [ 1.3890e+05,  0.0000e+00,  1.3890e+05, -1.2000e+02,  1.3890e+05,\n",
      "           1.2000e+02],\n",
      "         ...,\n",
      "         [ 1.4353e+02, -4.3500e+01,  1.4334e+02, -1.6350e+02,  1.4355e+02,\n",
      "           7.6500e+01],\n",
      "         [ 1.4340e+02, -4.5900e+01,  1.4267e+02, -1.6600e+02,  1.4203e+02,\n",
      "           7.4400e+01],\n",
      "         [ 1.3986e+02, -4.5800e+01,  1.3939e+02, -1.6610e+02,  1.3923e+02,\n",
      "           7.4200e+01]]])\n",
      "torch.float32\n",
      "tensor([[  0,   1,   1,  ..., 170, 171, 173],\n",
      "        [  1,   2,   3,  ..., 387, 388, 389]])\n",
      "torch.int64\n",
      "\n",
      "YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
      "tensor([[ 5.2710e+01, -7.5900e+01,  5.2710e+01,  1.6410e+02,  5.2710e+01,\n",
      "          4.4100e+01],\n",
      "        [ 5.2709e+01, -7.5900e+01,  5.2709e+01,  1.6410e+02,  5.2709e+01,\n",
      "          4.4100e+01],\n",
      "        [ 5.2709e+01, -7.5900e+01,  5.2709e+01,  1.6410e+02,  5.2709e+01,\n",
      "          4.4100e+01],\n",
      "        ...,\n",
      "        [ 1.0671e-01, -1.0790e+02,  1.0675e-01,  1.3210e+02,  1.0672e-01,\n",
      "          1.2100e+01],\n",
      "        [ 1.0452e-01, -1.0950e+02,  1.0434e-01,  1.3040e+02,  1.0407e-01,\n",
      "          1.0600e+01],\n",
      "        [ 1.0452e-01, -1.0950e+02,  1.0434e-01,  1.3040e+02,  1.0407e-01,\n",
      "          1.0600e+01]])\n",
      "\n",
      "===================\n",
      "Number of graphs: 31200\n",
      "Number of features: 6\n",
      "\n",
      "NormalizeFeatures()\n",
      "tensor([[ 5.2710e+01, -7.5900e+01,  5.2710e+01,  1.6410e+02,  5.2710e+01,\n",
      "          4.4100e+01],\n",
      "        [ 5.2709e+01, -7.5900e+01,  5.2709e+01,  1.6410e+02,  5.2709e+01,\n",
      "          4.4100e+01],\n",
      "        [ 5.2709e+01, -7.5900e+01,  5.2709e+01,  1.6410e+02,  5.2709e+01,\n",
      "          4.4100e+01],\n",
      "        ...,\n",
      "        [ 1.0671e-01, -1.0790e+02,  1.0675e-01,  1.3210e+02,  1.0672e-01,\n",
      "          1.2100e+01],\n",
      "        [ 1.0452e-01, -1.0950e+02,  1.0434e-01,  1.3040e+02,  1.0407e-01,\n",
      "          1.0600e+01],\n",
      "        [ 1.0452e-01, -1.0950e+02,  1.0434e-01,  1.3040e+02,  1.0407e-01,\n",
      "          1.0600e+01]])\n",
      "=============================================================\n",
      "Number of nodes: 390\n",
      "Number of edges: 780\n",
      "Average node degree: 2.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n",
      "Number of training graphs: 24960\n",
      "Number of test graphs: 6240\n",
      "tensor([[ 1.3939e+05,  0.0000e+00,  1.3937e+05, -1.2000e+02,  1.3939e+05,\n",
      "          1.2000e+02],\n",
      "        [ 1.3939e+05,  0.0000e+00,  1.3936e+05, -1.2000e+02,  1.3939e+05,\n",
      "          1.2000e+02],\n",
      "        [ 1.3938e+05,  0.0000e+00,  1.3936e+05, -1.2000e+02,  1.3939e+05,\n",
      "          1.2000e+02],\n",
      "        ...,\n",
      "        [ 2.8059e+02, -2.9600e+01,  2.7395e+02, -1.5040e+02,  2.7351e+02,\n",
      "          9.1300e+01],\n",
      "        [ 2.7974e+02, -3.0600e+01,  8.1249e+00,  1.2800e+02,  8.2258e+00,\n",
      "          8.0000e+00],\n",
      "        [ 2.7306e+02, -3.0500e+01,  2.4644e+01,  1.5200e+02,  1.7272e+01,\n",
      "          6.4300e+01]])\n",
      "Data(x=[390, 6], edge_index=[2, 780], y=16, is_undirected=True)\n",
      "tensor([[ 1.3938e+05,  0.0000e+00,  1.3939e+05, -1.2000e+02,  1.3940e+05,\n",
      "          1.2000e+02],\n",
      "        [ 1.3937e+05,  0.0000e+00,  1.3939e+05, -1.2000e+02,  1.3940e+05,\n",
      "          1.2000e+02],\n",
      "        [ 1.3937e+05,  0.0000e+00,  1.3939e+05, -1.2000e+02,  1.3939e+05,\n",
      "          1.2000e+02],\n",
      "        ...,\n",
      "        [ 2.7588e+02, -3.1400e+01,  2.7408e+02, -1.4980e+02,  2.8154e+02,\n",
      "          8.9600e+01],\n",
      "        [ 2.6872e+02, -3.3700e+01,  2.6606e+02, -1.5230e+02,  2.7266e+02,\n",
      "          8.7500e+01],\n",
      "        [ 2.6864e+02, -3.3700e+01,  2.6600e+02, -1.5230e+02,  2.7266e+02,\n",
      "          8.7500e+01]])\n",
      "YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
      "tensor([[ 5.7737e-01,  0.0000e+00,  5.7729e-01, -4.9706e-04,  5.7739e-01,\n",
      "          4.9706e-04],\n",
      "        [ 5.7738e-01,  0.0000e+00,  5.7729e-01, -4.9707e-04,  5.7739e-01,\n",
      "          4.9707e-04],\n",
      "        [ 5.7737e-01,  0.0000e+00,  5.7728e-01, -4.9708e-04,  5.7739e-01,\n",
      "          4.9708e-04],\n",
      "        ...,\n",
      "        [ 5.4984e-01, -5.8004e-02,  5.3683e-01, -2.9472e-01,  5.3596e-01,\n",
      "          1.7891e-01],\n",
      "        [ 9.0393e-01, -9.8879e-02,  2.6254e-02,  4.1361e-01,  2.6580e-02,\n",
      "          2.5851e-02],\n",
      "        [ 8.4821e-01, -9.4742e-02,  7.6551e-02,  4.7215e-01,  5.3653e-02,\n",
      "          1.9973e-01]])\n",
      "Data(x=[390, 6], edge_index=[2, 780], y=16, is_undirected=True)\n",
      "=============================================================\n",
      "YYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYYY\n",
      "tensor([[ 5.7729e-01,  0.0000e+00,  5.7737e-01, -4.9704e-04,  5.7738e-01,\n",
      "          4.9704e-04],\n",
      "        [ 5.7729e-01,  0.0000e+00,  5.7737e-01, -4.9705e-04,  5.7739e-01,\n",
      "          4.9705e-04],\n",
      "        [ 5.7729e-01,  0.0000e+00,  5.7738e-01, -4.9706e-04,  5.7739e-01,\n",
      "          4.9706e-04],\n",
      "        ...,\n",
      "        [ 5.3904e-01, -6.1351e-02,  5.3551e-01, -2.9269e-01,  5.5008e-01,\n",
      "          1.7507e-01],\n",
      "        [ 5.3817e-01, -6.7491e-02,  5.3284e-01, -3.0501e-01,  5.4605e-01,\n",
      "          1.7524e-01],\n",
      "        [ 5.3808e-01, -6.7501e-02,  5.3279e-01, -3.0506e-01,  5.4614e-01,\n",
      "          1.7526e-01]])\n",
      "Data(x=[390, 6], edge_index=[2, 780], y=62, is_undirected=True)\n",
      "Number of nodes: 390\n",
      "Number of edges: 780\n",
      "Average node degree: 2.00\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "encoder = ['P1', 'P2', 'P3', 'P7', 'P6', 'P11', 'P27', 'P44', 'P45', 'P62', 'P10',\n",
    "        'P81', 'P97', 'P116', 'P117', 'P134', 'P12', 'P13', 'P14', 'P15', 'P16',\n",
    "        'P17', 'P18', 'P19', 'P20', 'P21', 'P22', 'P23', 'P24', 'P25', 'P26', 'P28',\n",
    "        'P29', 'P30', 'P31', 'P32', 'P33', 'P34', 'P35', 'P36', 'P37', 'P38', 'P39',\n",
    "        'P40', 'P41', 'P42', 'P43', 'P46', 'P47', 'P48', 'P49', 'P50', 'P51', 'P52',\n",
    "        'P53', 'P54', 'P55', 'P56', 'P57', 'P58', 'P59', 'P60', 'P61', 'P63', 'P64',\n",
    "        'P65', 'P66', 'P67', 'P68', 'P69', 'P70', 'P71', 'P72', 'P73', 'P74', 'P75',\n",
    "        'P76', 'P77', 'P78', 'P79', 'P80', 'P82', 'P83', 'P84', 'P85', 'P86', 'P87',\n",
    "        'P88', 'P89', 'P90', 'P91', 'P92', 'P93', 'P94', 'P95', 'P96', 'P98', 'P99',\n",
    "        'P100', 'P101', 'P102', 'P103', 'P104', 'P105', 'P106', 'P107', 'P108', 'P109',\n",
    "        'P110', 'P111', 'P112', 'P113', 'P114', 'P115', 'P118', 'P119', 'P120', 'P121',\n",
    "        'P122', 'P123', 'P124', 'P125', 'P126', 'P127', 'P128', 'P129', 'P130', 'P131',\n",
    "        'P132', 'P133', 'P135', 'P136', 'P137', 'P138', 'P139', 'P140', 'P141', 'P142',\n",
    "        'P143', 'P144', 'P145', 'P146', 'P147', 'P148', 'P149', 'P150', 'S194', 'S193',\n",
    "        'S195', 'S199', 'S198', 'S200', 'S204', 'S203', 'S205', 'S206', 'S211', 'S210',\n",
    "        'S212', 'S213', 'S218', 'S217', 'S219', 'S220', 'S225', 'S224', 'S226', 'S227',\n",
    "        'S232', 'S231', 'S233', 'S237', 'S236', 'S239', 'S1', 'S2', 'S3', 'S4', 'S5',\n",
    "        'S6', 'S7', 'S8', 'S9', 'S10', 'S11', 'S12', 'S13', 'S14', 'S15', 'S16', 'S17',\n",
    "        'S18', 'S19', 'S20', 'S21', 'S42', 'S43', 'S44', 'S45', 'S46', 'S47', 'S48',\n",
    "        'S49', 'S50', 'S51', 'S52', 'S53', 'S54', 'S55', 'S56', 'S57', 'S58', 'S59',\n",
    "        'S60', 'S61', 'S62', 'S83', 'S84', 'S85', 'S86', 'S87', 'S88', 'S89', 'S90',\n",
    "        'S91', 'S92', 'S93', 'S94', 'S95', 'S96', 'S97', 'S98', 'S99', 'S100', 'S101',\n",
    "        'S102', 'S103', 'S124', 'S125', 'S126', 'S127', 'S128', 'S129', 'S130', 'S131',\n",
    "        'S132', 'S133', 'S134', 'S135', 'S136', 'S137', 'S138', 'S139', 'S140', 'S141',\n",
    "        'S142', 'S143', 'S144', 'S22', 'S27', 'S32', 'S37', 'S63', 'S68', 'S73', 'S78',\n",
    "        'S104', 'S109', 'S114', 'S119', 'S23', 'S28', 'S33', 'S38', 'S64', 'S69', 'S74',\n",
    "        'S79', 'S105', 'S110', 'S115', 'S120', 'S24', 'S29', 'S34', 'S39', 'S65', 'S70',\n",
    "        'S75', 'S80', 'S106', 'S111', 'S116', 'S121', 'S25', 'S30', 'S35', 'S40', 'S66',\n",
    "        'S71', 'S76', 'S81', 'S107', 'S112', 'S117', 'S122', 'S26', 'S31', 'S36', 'S41',\n",
    "        'S67', 'S72', 'S77', 'S82', 'S108', 'S113', 'S118', 'S123', 'P4', 'P5', 'P8',\n",
    "        'P9', 'S148', 'S145', 'S152', 'S149', 'S156', 'S153', 'S160', 'S157', 'S147',\n",
    "        'S146', 'S151', 'S150', 'S155', 'S154', 'S159', 'S158', 'S164', 'S161', 'S168',\n",
    "        'S165', 'S172', 'S169', 'S176', 'S173', 'S163', 'S162', 'S167', 'S166', 'S171',\n",
    "        'S170', 'S175', 'S174', 'S180', 'S177', 'S184', 'S181', 'S188', 'S185', 'S192',\n",
    "        'S189', 'S179', 'S178', 'S183', 'S182', 'S187', 'S186', 'S191', 'S190', 'S196',\n",
    "        'S197', 'S201', 'S202', 'S207', 'S208', 'S209', 'S214', 'S215', 'S216', 'S221',\n",
    "        'S222', 'S223', 'S228', 'S229', 'S230', 'S234', 'S235', 'S238', 'S240']\n",
    "#encoder = ['SourceBus', '800', '802', '806', '808', '810', '812', '814', '814r', '850',\n",
    "#'816', '818', '824', '820', '822', '826', '828', '830', '854', '832', '858',\n",
    "#'834', '860', '842', '836', '840', '862', '844', '846', '848', '852r', '888', '856', '852', '864', '838', '890']\n",
    "print(encoder)\n",
    "\n",
    "research_paper_decoder = [0, 1, 2, 3, 2, 4, 5, 6, 7, 8, 3, 9, 10, 11, 12, 13, 4, 14, 15, 14, 16,\n",
    "                          17, 14, 18, 14, 19, 20, 14, 21, 14, 22, 5, 14, 23, 14, 24, 14, 25, 14,\n",
    "                          26, 14, 27, 14, 28, 29, 14, 30, 7, 14, 31, 14, 32, 14, 33, 14, 34, 35,\n",
    "                          36, 14, 37, 14, 38, 39, 8, 14, 40, 14, 41, 14, 42, 14, 43, 14, 44, 45,\n",
    "                          14, 46, 14, 47, 14, 48, 9, 14, 49, 14, 50, 51, 14, 52, 14, 53, 54, 14,\n",
    "                          55, 14, 56, 10, 14, 57, 14, 58, 14, 59, 14, 60, 61, 14, 62, 14, 63, 14,\n",
    "                          64, 14, 65, 12, 14, 66, 14, 67, 14, 68, 14, 69, 14, 70, 14, 71, 72, 14,\n",
    "                          73, 13, 14, 74, 14, 75, 76, 14, 77, 14, 78, 79, 80, 14, 81, 14, 82, 29,\n",
    "                          83, 63, 39, 84, 72, 20, 85, 45, 78, 46, 86, 54, 80, 25, 87, 35, 61, 34,\n",
    "                          88, 59, 68, 42, 89, 75, 16, 90, 51, 91, 92, 93, 73, 94, 95, 96, 97, 56,\n",
    "                          98, 99, 100, 101, 38, 102, 103, 104, 105, 22, 106, 107, 108, 109, 55, 70,\n",
    "                          110, 111, 112, 81, 44, 113, 114, 115, 21, 79, 116, 117, 118, 47, 27, 119,\n",
    "                          120, 121, 122, 36, 74, 123, 124, 125, 77, 32, 126, 127, 128, 18, 58, 129,\n",
    "                          130, 131, 33, 76, 132, 133, 134, 135, 40, 136, 137, 138, 139, 31, 140,\n",
    "                          141, 142, 143, 23, 144, 145, 146, 147, 24, 148, 149, 150, 151, 82, 152,\n",
    "                          153, 154, 62, 155, 156, 157, 17, 158, 159, 160, 65, 64, 161, 162, 19, 69,\n",
    "                          163, 164, 67, 66, 165, 166, 48, 71, 167, 168, 53, 60, 169, 170, 50, 57,\n",
    "                          171, 172, 30, 28, 173, 174, 52, 26, 175, 176, 15, 41, 177, 178, 179, 37,\n",
    "                          180, 181, 182, 43, 183, 184, 185, 49, 186, 2, 2, 3, 3, 82, 73, 65, 56,\n",
    "                          48, 38, 30, 22, 55, 64, 81, 71, 21, 28, 47, 37, 62, 70, 19, 44, 53, 79,\n",
    "                          52, 27, 36, 69, 77, 60, 18, 26, 33, 43, 17, 74, 67, 32, 50, 58, 15, 76,\n",
    "                          40, 66, 31, 57, 23, 41, 24, 49, 29, 63, 39, 72, 20, 45, 78, 46, 54, 80,\n",
    "                          25, 35, 61, 34, 59, 68, 42, 75, 16, 51]\n",
    "\n",
    "# Previous Encoding\n",
    "# research_paper_decoder = [0, 1, 2, 3, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 4, 15, 16, 15,\n",
    "#                           17, 18, 19, 20, 19, 21, 22, 23, 24, 23, 25, 5, 15, 26, 15, 27, 19,\n",
    "#                           28, 19, 29, 19, 30, 23, 31, 32, 23, 33, 7, 34, 35, 34, 36, 34, 37,\n",
    "#                           38, 39, 40, 41, 42, 43, 42, 44, 45, 8, 34, 46, 34, 47, 34, 48, 38,\n",
    "#                           49, 38, 50, 51, 38, 52, 42, 53, 42, 54, 10, 55, 56, 55, 57, 58, 59,\n",
    "#                           60, 59, 61, 62, 63, 64, 63, 65, 11, 55, 66, 55, 67, 59, 68, 59, 69,\n",
    "#                           70, 59, 71, 63, 72, 63, 73, 63, 74, 13, 75, 76, 75, 77, 75, 78, 79,\n",
    "#                           80, 79, 81, 82, 83, 84, 82, 85, 14, 75, 86, 75, 87, 88, 79, 89, 79,\n",
    "#                           90, 91, 92, 82, 93, 82, 94, 32, 95, 72, 45, 96, 84, 22, 97, 51, 90,\n",
    "#                           52, 98, 62, 92, 28, 99, 40, 70, 39, 100, 68, 78, 48, 101, 87, 17,\n",
    "#                           102, 58, 103, 104, 105, 85, 106, 107, 108, 109, 65, 110, 111, 112,\n",
    "#                           113, 44, 114, 115, 116, 117, 25, 118, 119, 120, 121, 64, 81, 122,\n",
    "#                           123, 124, 93, 50, 125, 126, 127, 24, 91, 128, 129, 130, 53, 30, 131,\n",
    "#                           132, 133, 134, 41, 86, 135, 136, 137, 89, 36, 138, 139, 140, 20, 67,\n",
    "#                           141, 142, 143, 37, 88, 144, 145, 146, 147, 46, 148, 149, 150, 151, 35,\n",
    "#                           152, 153, 154, 155, 26, 156, 157, 158, 159, 27, 160, 161, 162, 163, 94,\n",
    "#                           164, 165, 166, 71, 167, 168, 169, 18, 170, 171, 172, 74, 73, 173, 174,\n",
    "#                           21, 80, 175, 176, 77, 76, 177, 178, 54, 83, 179, 180, 61, 69, 181, 182,\n",
    "#                           57, 66, 183, 184, 33, 31, 185, 186, 60, 29, 187, 188, 16, 47, 189, 190,\n",
    "#                           191, 43, 192, 193, 194, 49, 195, 196, 197, 56, 198, 2, 2, 3, 3, 94, 85,\n",
    "#                           74, 65, 54, 44, 33, 25, 64, 73, 93, 83, 24, 31, 53, 43, 71, 81, 21, 50,\n",
    "#                           61, 91, 60, 30, 41, 80, 89, 69, 20, 29, 37, 49, 18, 86, 77, 36, 57, 67,\n",
    "#                           16, 88, 46, 76, 35, 66, 26, 47, 27, 56, 32, 72, 45, 84, 22, 51, 90, 52,\n",
    "#                           62, 92, 28, 40, 70, 39, 68, 78, 48, 87, 17, 58]\n",
    "\n",
    "# Oldest Encoding\n",
    "# research_paper_decoder = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 5, 16, 17,\n",
    "#                           18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 6, 30, 31, 32, 33,\n",
    "#                           34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 8, 45, 46, 47, 48, 49,\n",
    "#                           50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 9, 60, 61, 62, 63, 64, 65,\n",
    "#                           66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 11, 77, 78, 79, 80, 81,\n",
    "#                           82, 83, 84, 85, 86, 87, 88, 89, 90, 12, 91, 92, 93, 94, 95, 96, 97,\n",
    "#                           98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 14, 108, 109, 110,\n",
    "#                           111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 15, 123,\n",
    "#                           124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137,\n",
    "#                           42, 138, 103, 59, 139, 120, 25, 140, 70, 131, 72, 141, 86, 133, 35,\n",
    "#                           142, 53, 99, 52, 143, 96, 113, 65, 144, 126, 19, 145, 81, 146, 147,\n",
    "#                           148, 122, 149, 150, 151, 152, 90, 153, 154, 155, 156, 58, 157, 158,\n",
    "#                           159, 160, 29, 161, 162, 163, 164, 88, 117, 165, 166, 167, 135, 69,\n",
    "#                           168, 169, 170, 27, 132, 171, 172, 173, 74, 39, 174, 175, 176, 177,\n",
    "#                           54, 124, 178, 179, 180, 129, 48, 181, 182, 183, 22, 94, 184, 185,\n",
    "#                           186, 50, 127, 187, 188, 189, 190, 61, 191, 192, 193, 194, 46, 195,\n",
    "#                           196, 197, 198, 31, 199, 200, 201, 202, 33, 203, 204, 205, 206, 137,\n",
    "#                           207, 208, 209, 101, 210, 211, 212, 20, 213, 214, 215, 107, 105, 216,\n",
    "#                           217, 24, 115, 218, 219, 111, 109, 220, 221, 76, 119, 222, 223, 85, 98,\n",
    "#                           224, 225, 80, 92, 226, 227, 44, 41, 228, 229, 83, 37, 230, 231, 17, 63,\n",
    "#                           232, 233, 234, 56, 235, 236, 237, 67, 238, 239, 240, 78, 241, 2, 2, 3, 3,\n",
    "#                           137, 122, 107, 90, 76, 58, 44, 29, 88, 105, 135, 119, 27, 41, 74, 56, 101,\n",
    "#                           117, 24, 69, 85, 132, 83, 39, 54, 115, 129, 98, 22, 37, 50, 67, 20, 124, 111,\n",
    "#                           48, 80, 94, 17, 127, 61, 109, 46, 92, 31, 63, 33, 78, 42, 103, 59, 120, 25,\n",
    "#                           70, 131, 72, 86, 133, 35, 53, 99, 52, 96, 113, 65, 126, 19, 81]\n",
    "\n",
    "#research_paper_decoder = [0,0,0,1,2,3,4,5,6,7,8,9,10,11,10,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,72,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,14,11,61,61,63,119]\n",
    "print(research_paper_decoder)\n",
    "print(len(research_paper_decoder))\n",
    "FaultLocationLabels = value_data[:,3]\n",
    "#\n",
    "for n in range(len(FaultLocationLabels)):\n",
    "    FaultLocationLabels[n]=research_paper_decoder[encoder.index(str(FaultLocationLabels[n]))]\n",
    "#\n",
    "y = FaultLocationLabels.astype(\"int64\")\n",
    "y = torch.from_numpy(y)\n",
    "x = numpy_data.astype(\"float32\")\n",
    "x = torch.from_numpy(x)\n",
    "#\n",
    "NodeIndex = NodeIndex.astype(\"int64\")\n",
    "NodeIndex = NodeIndex.T\n",
    "NodeIndex = torch.from_numpy(NodeIndex)\n",
    "#\n",
    "print(y)\n",
    "print(y.dtype)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "print(NodeIndex)\n",
    "print(NodeIndex.dtype)\n",
    "##\n",
    "#print(x[0])\n",
    "#print(normalize(x[0]))\n",
    "#\n",
    "result_translator = np.unique(FaultLocationLabels.astype(\"int64\")).tolist()\n",
    "print()\n",
    "total_data_list = []\n",
    "for n in range(len(x)):\n",
    "    #print(x[n])\n",
    "    #print(y[n])\n",
    "    DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True) #Testing with non-normalized data\n",
    "    #DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True)\n",
    "    DataObject.is_undirected = True\n",
    "    total_data_list.append(DataObject)\n",
    "print('Y'*888)\n",
    "print(total_data_list[0].x)\n",
    "\n",
    "print()\n",
    "#print(f'Dataset: {total_data_list}:')\n",
    "print('===================')\n",
    "print(f'Number of graphs: {len(total_data_list)}')\n",
    "print(f'Number of features: {total_data_list[0].num_features}')\n",
    "##print(f'Number of classes: {total_data_list[0].num_classes}')\n",
    "#\n",
    "data = total_data_list[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(NormalizeFeatures(data.x))\n",
    "print(data.x)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "from random import shuffle\n",
    "torch.manual_seed(12345)\n",
    "#total_data_list = total_data_list.shuffle()\n",
    "shuffle(total_data_list)\n",
    "\n",
    "train_dataset = total_data_list[:24960]\n",
    "test_dataset = total_data_list[24960:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "\n",
    "print(test_dataset[0].x)\n",
    "print(test_dataset[0])\n",
    "print(train_dataset[0].x)\n",
    "#\n",
    "for sample in range(len(test_dataset)):\n",
    "    #noise = np.random.normal(1,0.03, size = (37,6)) #0.09\n",
    "\n",
    "    #Uncomment for noise\n",
    "    # noise = np.random.normal(1,0.03, size = (390,6)) #0.09\n",
    "    # noise = noise.astype(\"float32\")\n",
    "    # noise = torch.from_numpy(noise)\n",
    "    # test_dataset[sample].x = noise*test_dataset[sample].x\n",
    "    test_dataset[sample].x = normalize(test_dataset[sample].x)\n",
    "    #print(sample)\n",
    "\n",
    "print('Y'*888)\n",
    "#print(train_dataset[0].x)\n",
    "print(test_dataset[0].x)\n",
    "print(test_dataset[0])\n",
    "##noise = np.random.normal(1,0.09, size = (16,6))\n",
    "##noise = noise.astype(\"float32\")\n",
    "##noise = torch.from_numpy(noise)\n",
    "print('=============================================================')\n",
    "###\n",
    "#print(train_dataset[0].x)\n",
    "#print(train_dataset[0])\n",
    "##print(train_dataset[0].x)\n",
    "\n",
    "for sample in range(len(train_dataset)):\n",
    "#    noise = np.random.normal(1,0.03, size = (390,6)) #0.09\n",
    "#    noise = noise.astype(\"float32\")\n",
    "#    noise = torch.from_numpy(noise)\n",
    "##    print(noise)\n",
    "##    X_test[sample] = X_test[sample]*noise[0]\n",
    "#    train_dataset[sample].x = noise*train_dataset[sample].x\n",
    "    train_dataset[sample].x = normalize(train_dataset[sample].x)\n",
    "    #print(sample)\n",
    "\n",
    "print('Y'*888)\n",
    "#print(train_dataset[0].x)\n",
    "print(train_dataset[0].x)\n",
    "print(train_dataset[0])\n",
    "##print(noise*test_dataset[0].x)\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d89feff7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d89feff7",
    "outputId": "cc162e3a-c7ef-46c1-c1dc-0aebfd5f404b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): ChebConv(6, 512, K=3, normalization=sym)\n",
      "  (conv2): ChebConv(512, 512, K=4, normalization=sym)\n",
      "  (conv3): ChebConv(512, 512, K=5, normalization=sym)\n",
      "  (lin): Linear(in_features=512, out_features=187, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.pool import global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        # self.conv1 = GCNConv(6, 512, improved = True)\n",
    "        # self.conv2 = GCNConv(512, 256, improved = True)\n",
    "\n",
    "        # self.conv1 = ChebConv(6, 512, K = 2)\n",
    "        # self.conv2 = ChebConv(512,256, K = 2)\n",
    "\n",
    "        # self.conv1 = ChebConv(6, 256, K = 3)\n",
    "        # self.conv2 = ChebConv(256,256, K = 4)\n",
    "        # self.conv3 = ChebConv(256, 256, K = 5)\n",
    "        self.conv1 = ChebConv(6, 512, K = 3)\n",
    "        self.conv2 = ChebConv(512,512, K = 4)\n",
    "        self.conv3 = ChebConv(512, 512, K = 5)\n",
    "\n",
    "\n",
    "        # self.fc1 = Linear(512, 512) #Uncomment for deep layers\n",
    "\n",
    "        # self.fc2 = Linear(512, 256)\n",
    "\n",
    "        # self.conv1 = GATConv(6, 256, heads = 4)\n",
    "        # self.conv2 = GATConv(256*4,64, heads = 1, concat=False)\n",
    "\n",
    "        # self.conv1 = TransformerConv(6, 200, heads = 3)\n",
    "        # self.conv2 = TransformerConv(200*3,256, heads = 1, concat=False)\n",
    "\n",
    "        # self.lin = Linear(256, 242)\n",
    "        self.lin = Linear(512, 187)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # x = global_max_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # x = self.fc1(x) #Uncomment for deep layers\n",
    "        # x = x.relu()\n",
    "        # x = self.fc2(x)\n",
    "        # x = x.relu()\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training) #0.5\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=464).to(device)\n",
    "print(model)\n",
    "\n",
    "from IPython.display import Javascript\n",
    "#display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=464).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         data = data.to(device)\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     model.to(device)\n",
    "     if model.training is True:\n",
    "         print(\"Model is Not Testing Properly\")\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         data = data.to(device)\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb4f8638",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cb4f8638",
    "outputId": "985383cf-4d30-4d53-8cc4-5740e7ef46f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186]\n",
      "Epoch: 000, Train Acc: 0.1440, Test Acc: 0.1429\n",
      "Epoch: 001, Train Acc: 0.1756, Test Acc: 0.1712\n",
      "Epoch: 002, Train Acc: 0.1979, Test Acc: 0.1877\n",
      "Epoch: 003, Train Acc: 0.2260, Test Acc: 0.2143\n",
      "Epoch: 004, Train Acc: 0.2395, Test Acc: 0.2221\n",
      "Epoch: 005, Train Acc: 0.2505, Test Acc: 0.2353\n",
      "Epoch: 006, Train Acc: 0.2566, Test Acc: 0.2401\n",
      "Epoch: 007, Train Acc: 0.2637, Test Acc: 0.2386\n",
      "Epoch: 008, Train Acc: 0.2768, Test Acc: 0.2599\n",
      "Epoch: 009, Train Acc: 0.2850, Test Acc: 0.2639\n",
      "Epoch: 010, Train Acc: 0.2961, Test Acc: 0.2777\n",
      "Epoch: 011, Train Acc: 0.3029, Test Acc: 0.2857\n",
      "Epoch: 012, Train Acc: 0.3177, Test Acc: 0.2963\n",
      "Epoch: 013, Train Acc: 0.3203, Test Acc: 0.2893\n",
      "Epoch: 014, Train Acc: 0.3306, Test Acc: 0.3040\n",
      "Epoch: 015, Train Acc: 0.3349, Test Acc: 0.3131\n",
      "Epoch: 016, Train Acc: 0.3458, Test Acc: 0.3205\n",
      "Epoch: 017, Train Acc: 0.3527, Test Acc: 0.3210\n",
      "Epoch: 018, Train Acc: 0.3611, Test Acc: 0.3380\n",
      "Epoch: 019, Train Acc: 0.3654, Test Acc: 0.3438\n",
      "Epoch: 020, Train Acc: 0.3718, Test Acc: 0.3511\n",
      "Epoch: 021, Train Acc: 0.3836, Test Acc: 0.3582\n",
      "Epoch: 022, Train Acc: 0.3998, Test Acc: 0.3614\n",
      "Epoch: 023, Train Acc: 0.3958, Test Acc: 0.3678\n",
      "Epoch: 024, Train Acc: 0.4035, Test Acc: 0.3748\n",
      "Epoch: 025, Train Acc: 0.4073, Test Acc: 0.3840\n",
      "Epoch: 026, Train Acc: 0.4238, Test Acc: 0.3877\n",
      "Epoch: 027, Train Acc: 0.4278, Test Acc: 0.3970\n",
      "Epoch: 028, Train Acc: 0.4281, Test Acc: 0.4011\n",
      "Epoch: 029, Train Acc: 0.4407, Test Acc: 0.4029\n",
      "Epoch: 030, Train Acc: 0.4326, Test Acc: 0.4056\n",
      "Epoch: 031, Train Acc: 0.4376, Test Acc: 0.4159\n",
      "Epoch: 032, Train Acc: 0.4497, Test Acc: 0.4189\n",
      "Epoch: 033, Train Acc: 0.4554, Test Acc: 0.4232\n",
      "Epoch: 034, Train Acc: 0.4613, Test Acc: 0.4341\n",
      "Epoch: 035, Train Acc: 0.4779, Test Acc: 0.4397\n",
      "Epoch: 036, Train Acc: 0.4764, Test Acc: 0.4407\n",
      "Epoch: 037, Train Acc: 0.4782, Test Acc: 0.4444\n",
      "Epoch: 038, Train Acc: 0.4839, Test Acc: 0.4455\n",
      "Epoch: 039, Train Acc: 0.4911, Test Acc: 0.4498\n",
      "Epoch: 040, Train Acc: 0.4977, Test Acc: 0.4655\n",
      "Epoch: 041, Train Acc: 0.5137, Test Acc: 0.4816\n",
      "Epoch: 042, Train Acc: 0.5136, Test Acc: 0.4731\n",
      "Epoch: 043, Train Acc: 0.5116, Test Acc: 0.4814\n",
      "Epoch: 044, Train Acc: 0.5383, Test Acc: 0.4962\n",
      "Epoch: 045, Train Acc: 0.5216, Test Acc: 0.4800\n",
      "Epoch: 046, Train Acc: 0.5388, Test Acc: 0.5024\n",
      "Epoch: 047, Train Acc: 0.5443, Test Acc: 0.5003\n",
      "Epoch: 048, Train Acc: 0.5362, Test Acc: 0.5071\n",
      "Epoch: 049, Train Acc: 0.5526, Test Acc: 0.5088\n",
      "Epoch: 050, Train Acc: 0.5539, Test Acc: 0.5095\n",
      "Epoch: 051, Train Acc: 0.5566, Test Acc: 0.5212\n",
      "Epoch: 052, Train Acc: 0.5639, Test Acc: 0.5288\n",
      "Epoch: 053, Train Acc: 0.5552, Test Acc: 0.5181\n",
      "Epoch: 054, Train Acc: 0.5468, Test Acc: 0.4994\n",
      "Epoch: 055, Train Acc: 0.5718, Test Acc: 0.5375\n",
      "Epoch: 056, Train Acc: 0.5782, Test Acc: 0.5442\n",
      "Epoch: 057, Train Acc: 0.5785, Test Acc: 0.5421\n",
      "Epoch: 058, Train Acc: 0.5754, Test Acc: 0.5412\n",
      "Epoch: 059, Train Acc: 0.6099, Test Acc: 0.5679\n",
      "Epoch: 060, Train Acc: 0.5669, Test Acc: 0.5340\n",
      "Epoch: 061, Train Acc: 0.6003, Test Acc: 0.5670\n",
      "Epoch: 062, Train Acc: 0.6129, Test Acc: 0.5838\n",
      "Epoch: 063, Train Acc: 0.6227, Test Acc: 0.5760\n",
      "Epoch: 064, Train Acc: 0.6182, Test Acc: 0.5755\n",
      "Epoch: 065, Train Acc: 0.6091, Test Acc: 0.5702\n",
      "Epoch: 066, Train Acc: 0.6357, Test Acc: 0.6059\n",
      "Epoch: 067, Train Acc: 0.6309, Test Acc: 0.5913\n",
      "Epoch: 068, Train Acc: 0.6402, Test Acc: 0.5989\n",
      "Epoch: 069, Train Acc: 0.6240, Test Acc: 0.5873\n",
      "Epoch: 070, Train Acc: 0.6726, Test Acc: 0.6393\n",
      "Epoch: 071, Train Acc: 0.6730, Test Acc: 0.6308\n",
      "Epoch: 072, Train Acc: 0.6981, Test Acc: 0.6609\n",
      "Epoch: 073, Train Acc: 0.6634, Test Acc: 0.6268\n",
      "Epoch: 074, Train Acc: 0.6940, Test Acc: 0.6644\n",
      "Epoch: 075, Train Acc: 0.6988, Test Acc: 0.6716\n",
      "Epoch: 076, Train Acc: 0.6899, Test Acc: 0.6704\n",
      "Epoch: 077, Train Acc: 0.7267, Test Acc: 0.7014\n",
      "Epoch: 078, Train Acc: 0.6955, Test Acc: 0.6691\n",
      "Epoch: 079, Train Acc: 0.7048, Test Acc: 0.6881\n",
      "Epoch: 080, Train Acc: 0.7089, Test Acc: 0.6854\n",
      "Epoch: 081, Train Acc: 0.7267, Test Acc: 0.7000\n",
      "Epoch: 082, Train Acc: 0.7076, Test Acc: 0.6801\n",
      "Epoch: 083, Train Acc: 0.7462, Test Acc: 0.7244\n",
      "Epoch: 084, Train Acc: 0.7351, Test Acc: 0.7130\n",
      "Epoch: 085, Train Acc: 0.7498, Test Acc: 0.7272\n",
      "Epoch: 086, Train Acc: 0.7403, Test Acc: 0.7176\n",
      "Epoch: 087, Train Acc: 0.7566, Test Acc: 0.7373\n",
      "Epoch: 088, Train Acc: 0.7425, Test Acc: 0.7226\n",
      "Epoch: 089, Train Acc: 0.7596, Test Acc: 0.7396\n",
      "Epoch: 090, Train Acc: 0.7515, Test Acc: 0.7349\n",
      "Epoch: 091, Train Acc: 0.7488, Test Acc: 0.7391\n",
      "Epoch: 092, Train Acc: 0.7480, Test Acc: 0.7300\n",
      "Epoch: 093, Train Acc: 0.7525, Test Acc: 0.7308\n",
      "Epoch: 094, Train Acc: 0.7726, Test Acc: 0.7508\n",
      "Epoch: 095, Train Acc: 0.7648, Test Acc: 0.7410\n",
      "Epoch: 096, Train Acc: 0.7767, Test Acc: 0.7591\n",
      "Epoch: 097, Train Acc: 0.7714, Test Acc: 0.7474\n",
      "Epoch: 098, Train Acc: 0.7700, Test Acc: 0.7458\n",
      "Epoch: 099, Train Acc: 0.7748, Test Acc: 0.7540\n",
      "Epoch: 100, Train Acc: 0.7679, Test Acc: 0.7479\n",
      "Epoch: 101, Train Acc: 0.7607, Test Acc: 0.7426\n",
      "Epoch: 102, Train Acc: 0.7668, Test Acc: 0.7537\n",
      "Epoch: 103, Train Acc: 0.7779, Test Acc: 0.7591\n",
      "Epoch: 104, Train Acc: 0.7507, Test Acc: 0.7321\n",
      "Epoch: 105, Train Acc: 0.7771, Test Acc: 0.7590\n",
      "Epoch: 106, Train Acc: 0.7877, Test Acc: 0.7675\n",
      "Epoch: 107, Train Acc: 0.7815, Test Acc: 0.7591\n",
      "Epoch: 108, Train Acc: 0.7856, Test Acc: 0.7663\n",
      "Epoch: 109, Train Acc: 0.7702, Test Acc: 0.7503\n",
      "Epoch: 110, Train Acc: 0.7799, Test Acc: 0.7649\n",
      "Epoch: 111, Train Acc: 0.7770, Test Acc: 0.7558\n",
      "Epoch: 112, Train Acc: 0.7860, Test Acc: 0.7678\n",
      "Epoch: 113, Train Acc: 0.7855, Test Acc: 0.7662\n",
      "Epoch: 114, Train Acc: 0.7852, Test Acc: 0.7671\n",
      "Epoch: 115, Train Acc: 0.7863, Test Acc: 0.7651\n",
      "Epoch: 116, Train Acc: 0.7774, Test Acc: 0.7627\n",
      "Epoch: 117, Train Acc: 0.7905, Test Acc: 0.7707\n",
      "Epoch: 118, Train Acc: 0.7487, Test Acc: 0.7439\n",
      "Epoch: 119, Train Acc: 0.7915, Test Acc: 0.7734\n",
      "Epoch: 120, Train Acc: 0.7916, Test Acc: 0.7806\n",
      "Epoch: 121, Train Acc: 0.7901, Test Acc: 0.7731\n",
      "Epoch: 122, Train Acc: 0.7714, Test Acc: 0.7514\n",
      "Epoch: 123, Train Acc: 0.7905, Test Acc: 0.7707\n",
      "Epoch: 124, Train Acc: 0.7989, Test Acc: 0.7800\n",
      "Epoch: 125, Train Acc: 0.7927, Test Acc: 0.7750\n",
      "Epoch: 126, Train Acc: 0.7996, Test Acc: 0.7752\n",
      "Epoch: 127, Train Acc: 0.8040, Test Acc: 0.7848\n",
      "Epoch: 128, Train Acc: 0.7915, Test Acc: 0.7732\n",
      "Epoch: 129, Train Acc: 0.7923, Test Acc: 0.7723\n",
      "Epoch: 130, Train Acc: 0.8025, Test Acc: 0.7833\n",
      "Epoch: 131, Train Acc: 0.7970, Test Acc: 0.7752\n",
      "Epoch: 132, Train Acc: 0.7895, Test Acc: 0.7696\n",
      "Epoch: 133, Train Acc: 0.8048, Test Acc: 0.7792\n",
      "Epoch: 134, Train Acc: 0.8080, Test Acc: 0.7893\n",
      "Epoch: 135, Train Acc: 0.8038, Test Acc: 0.7822\n",
      "Epoch: 136, Train Acc: 0.7972, Test Acc: 0.7819\n",
      "Epoch: 137, Train Acc: 0.7911, Test Acc: 0.7707\n",
      "Epoch: 138, Train Acc: 0.7978, Test Acc: 0.7688\n",
      "Epoch: 139, Train Acc: 0.8038, Test Acc: 0.7787\n",
      "Epoch: 140, Train Acc: 0.7891, Test Acc: 0.7696\n",
      "Epoch: 141, Train Acc: 0.8042, Test Acc: 0.7792\n",
      "Epoch: 142, Train Acc: 0.8051, Test Acc: 0.7853\n",
      "Epoch: 143, Train Acc: 0.7846, Test Acc: 0.7638\n",
      "Epoch: 144, Train Acc: 0.8016, Test Acc: 0.7812\n",
      "Epoch: 145, Train Acc: 0.8011, Test Acc: 0.7889\n",
      "Epoch: 146, Train Acc: 0.7919, Test Acc: 0.7705\n",
      "Epoch: 147, Train Acc: 0.8053, Test Acc: 0.7865\n",
      "Epoch: 148, Train Acc: 0.7847, Test Acc: 0.7688\n",
      "Epoch: 149, Train Acc: 0.8122, Test Acc: 0.7907\n",
      "Epoch: 150, Train Acc: 0.8100, Test Acc: 0.7984\n",
      "Epoch: 151, Train Acc: 0.8149, Test Acc: 0.7894\n",
      "Epoch: 152, Train Acc: 0.8074, Test Acc: 0.7909\n",
      "Epoch: 153, Train Acc: 0.8018, Test Acc: 0.7861\n",
      "Epoch: 154, Train Acc: 0.8093, Test Acc: 0.7925\n",
      "Epoch: 155, Train Acc: 0.8066, Test Acc: 0.7877\n",
      "Epoch: 156, Train Acc: 0.8113, Test Acc: 0.7859\n",
      "Epoch: 157, Train Acc: 0.8168, Test Acc: 0.7992\n",
      "Epoch: 158, Train Acc: 0.8161, Test Acc: 0.7894\n",
      "Epoch: 159, Train Acc: 0.8006, Test Acc: 0.7764\n",
      "Epoch: 160, Train Acc: 0.8160, Test Acc: 0.7947\n",
      "Epoch: 161, Train Acc: 0.8167, Test Acc: 0.7934\n",
      "Epoch: 162, Train Acc: 0.8041, Test Acc: 0.7821\n",
      "Epoch: 163, Train Acc: 0.8156, Test Acc: 0.7957\n",
      "Epoch: 164, Train Acc: 0.8132, Test Acc: 0.8014\n",
      "Epoch: 165, Train Acc: 0.8210, Test Acc: 0.7982\n",
      "Epoch: 166, Train Acc: 0.8167, Test Acc: 0.7970\n",
      "Epoch: 167, Train Acc: 0.8160, Test Acc: 0.7946\n",
      "Epoch: 168, Train Acc: 0.8211, Test Acc: 0.7986\n",
      "Epoch: 169, Train Acc: 0.8063, Test Acc: 0.7859\n",
      "Epoch: 170, Train Acc: 0.8181, Test Acc: 0.7957\n",
      "Epoch: 171, Train Acc: 0.8258, Test Acc: 0.8006\n",
      "Epoch: 172, Train Acc: 0.8152, Test Acc: 0.7982\n",
      "Epoch: 173, Train Acc: 0.8123, Test Acc: 0.7902\n",
      "Epoch: 174, Train Acc: 0.8223, Test Acc: 0.7957\n",
      "Epoch: 175, Train Acc: 0.8222, Test Acc: 0.7990\n",
      "Epoch: 176, Train Acc: 0.8218, Test Acc: 0.8046\n",
      "Epoch: 177, Train Acc: 0.8128, Test Acc: 0.7986\n",
      "Epoch: 178, Train Acc: 0.8296, Test Acc: 0.8098\n",
      "Epoch: 179, Train Acc: 0.8343, Test Acc: 0.8098\n",
      "Epoch: 180, Train Acc: 0.8277, Test Acc: 0.8075\n",
      "Epoch: 181, Train Acc: 0.8341, Test Acc: 0.8069\n",
      "Epoch: 182, Train Acc: 0.8249, Test Acc: 0.8021\n",
      "Epoch: 183, Train Acc: 0.8138, Test Acc: 0.7899\n",
      "Epoch: 184, Train Acc: 0.8284, Test Acc: 0.8059\n",
      "Epoch: 185, Train Acc: 0.8340, Test Acc: 0.8152\n",
      "Epoch: 186, Train Acc: 0.8115, Test Acc: 0.7992\n",
      "Epoch: 187, Train Acc: 0.8280, Test Acc: 0.8066\n",
      "Epoch: 188, Train Acc: 0.8173, Test Acc: 0.7936\n",
      "Epoch: 189, Train Acc: 0.8276, Test Acc: 0.8077\n",
      "Epoch: 190, Train Acc: 0.8278, Test Acc: 0.8029\n",
      "Epoch: 191, Train Acc: 0.8248, Test Acc: 0.7992\n",
      "Epoch: 192, Train Acc: 0.8377, Test Acc: 0.8122\n",
      "Epoch: 193, Train Acc: 0.8431, Test Acc: 0.8099\n",
      "Epoch: 194, Train Acc: 0.8214, Test Acc: 0.8051\n",
      "Epoch: 195, Train Acc: 0.8167, Test Acc: 0.7965\n",
      "Epoch: 196, Train Acc: 0.8163, Test Acc: 0.7987\n",
      "Epoch: 197, Train Acc: 0.8327, Test Acc: 0.8085\n",
      "Epoch: 198, Train Acc: 0.8300, Test Acc: 0.8071\n",
      "Epoch: 199, Train Acc: 0.8266, Test Acc: 0.8091\n",
      "Epoch: 200, Train Acc: 0.8381, Test Acc: 0.8173\n",
      "Epoch: 201, Train Acc: 0.8343, Test Acc: 0.8083\n",
      "Epoch: 202, Train Acc: 0.8351, Test Acc: 0.8115\n",
      "Epoch: 203, Train Acc: 0.8364, Test Acc: 0.8119\n",
      "Epoch: 204, Train Acc: 0.8385, Test Acc: 0.8168\n",
      "Epoch: 205, Train Acc: 0.8178, Test Acc: 0.8008\n",
      "Epoch: 206, Train Acc: 0.8240, Test Acc: 0.8048\n",
      "Epoch: 207, Train Acc: 0.8250, Test Acc: 0.8022\n",
      "Epoch: 208, Train Acc: 0.8363, Test Acc: 0.8151\n",
      "Epoch: 209, Train Acc: 0.8052, Test Acc: 0.7833\n",
      "Epoch: 210, Train Acc: 0.8308, Test Acc: 0.8117\n",
      "Epoch: 211, Train Acc: 0.8384, Test Acc: 0.8125\n",
      "Epoch: 212, Train Acc: 0.8230, Test Acc: 0.7998\n",
      "Epoch: 213, Train Acc: 0.8328, Test Acc: 0.8130\n",
      "Epoch: 214, Train Acc: 0.8503, Test Acc: 0.8269\n",
      "Epoch: 215, Train Acc: 0.8327, Test Acc: 0.8098\n",
      "Epoch: 216, Train Acc: 0.8500, Test Acc: 0.8263\n",
      "Epoch: 217, Train Acc: 0.8283, Test Acc: 0.8115\n",
      "Epoch: 218, Train Acc: 0.8274, Test Acc: 0.8051\n",
      "Epoch: 219, Train Acc: 0.8427, Test Acc: 0.8213\n",
      "Epoch: 220, Train Acc: 0.8131, Test Acc: 0.7958\n",
      "Epoch: 221, Train Acc: 0.8384, Test Acc: 0.8155\n",
      "Epoch: 222, Train Acc: 0.8452, Test Acc: 0.8181\n",
      "Epoch: 223, Train Acc: 0.8423, Test Acc: 0.8248\n",
      "Epoch: 224, Train Acc: 0.8428, Test Acc: 0.8208\n",
      "Epoch: 225, Train Acc: 0.8425, Test Acc: 0.8170\n",
      "Epoch: 226, Train Acc: 0.8462, Test Acc: 0.8240\n",
      "Epoch: 227, Train Acc: 0.8456, Test Acc: 0.8260\n",
      "Epoch: 228, Train Acc: 0.8408, Test Acc: 0.8168\n",
      "Epoch: 229, Train Acc: 0.8487, Test Acc: 0.8260\n",
      "Epoch: 230, Train Acc: 0.8473, Test Acc: 0.8223\n",
      "Epoch: 231, Train Acc: 0.8320, Test Acc: 0.8091\n",
      "Epoch: 232, Train Acc: 0.8539, Test Acc: 0.8317\n",
      "Epoch: 233, Train Acc: 0.8393, Test Acc: 0.8247\n",
      "Epoch: 234, Train Acc: 0.8409, Test Acc: 0.8207\n",
      "Epoch: 235, Train Acc: 0.8323, Test Acc: 0.8151\n",
      "Epoch: 236, Train Acc: 0.8545, Test Acc: 0.8295\n",
      "Epoch: 237, Train Acc: 0.8515, Test Acc: 0.8240\n",
      "Epoch: 238, Train Acc: 0.8460, Test Acc: 0.8277\n",
      "Epoch: 239, Train Acc: 0.8527, Test Acc: 0.8253\n",
      "Epoch: 240, Train Acc: 0.8464, Test Acc: 0.8229\n",
      "Epoch: 241, Train Acc: 0.8515, Test Acc: 0.8268\n",
      "Epoch: 242, Train Acc: 0.8351, Test Acc: 0.8101\n",
      "Epoch: 243, Train Acc: 0.8307, Test Acc: 0.8119\n",
      "Epoch: 244, Train Acc: 0.8508, Test Acc: 0.8285\n",
      "Epoch: 245, Train Acc: 0.8417, Test Acc: 0.8231\n",
      "Epoch: 246, Train Acc: 0.8548, Test Acc: 0.8296\n",
      "Epoch: 247, Train Acc: 0.8464, Test Acc: 0.8245\n",
      "Epoch: 248, Train Acc: 0.8361, Test Acc: 0.8165\n",
      "Epoch: 249, Train Acc: 0.8427, Test Acc: 0.8168\n",
      "Epoch: 250, Train Acc: 0.8431, Test Acc: 0.8171\n",
      "Epoch: 251, Train Acc: 0.8359, Test Acc: 0.8205\n",
      "Epoch: 252, Train Acc: 0.8367, Test Acc: 0.8151\n",
      "Epoch: 253, Train Acc: 0.8501, Test Acc: 0.8288\n",
      "Epoch: 254, Train Acc: 0.8548, Test Acc: 0.8252\n",
      "Epoch: 255, Train Acc: 0.8605, Test Acc: 0.8345\n",
      "Epoch: 256, Train Acc: 0.8546, Test Acc: 0.8271\n",
      "Epoch: 257, Train Acc: 0.8479, Test Acc: 0.8255\n",
      "Epoch: 258, Train Acc: 0.8645, Test Acc: 0.8412\n",
      "Epoch: 259, Train Acc: 0.8631, Test Acc: 0.8385\n",
      "Epoch: 260, Train Acc: 0.8678, Test Acc: 0.8433\n",
      "Epoch: 261, Train Acc: 0.8412, Test Acc: 0.8252\n",
      "Epoch: 262, Train Acc: 0.8543, Test Acc: 0.8314\n",
      "Epoch: 263, Train Acc: 0.8714, Test Acc: 0.8426\n",
      "Epoch: 264, Train Acc: 0.8646, Test Acc: 0.8364\n",
      "Epoch: 265, Train Acc: 0.8591, Test Acc: 0.8428\n",
      "Epoch: 266, Train Acc: 0.8462, Test Acc: 0.8292\n",
      "Epoch: 267, Train Acc: 0.8365, Test Acc: 0.8111\n",
      "Epoch: 268, Train Acc: 0.8674, Test Acc: 0.8397\n",
      "Epoch: 269, Train Acc: 0.8589, Test Acc: 0.8282\n",
      "Epoch: 270, Train Acc: 0.8457, Test Acc: 0.8268\n",
      "Epoch: 271, Train Acc: 0.8667, Test Acc: 0.8429\n",
      "Epoch: 272, Train Acc: 0.8506, Test Acc: 0.8247\n",
      "Epoch: 273, Train Acc: 0.8556, Test Acc: 0.8351\n",
      "Epoch: 274, Train Acc: 0.8630, Test Acc: 0.8409\n",
      "Epoch: 275, Train Acc: 0.8555, Test Acc: 0.8353\n",
      "Epoch: 276, Train Acc: 0.8587, Test Acc: 0.8349\n",
      "Epoch: 277, Train Acc: 0.8566, Test Acc: 0.8301\n",
      "Epoch: 278, Train Acc: 0.8677, Test Acc: 0.8393\n",
      "Epoch: 279, Train Acc: 0.8593, Test Acc: 0.8343\n",
      "Epoch: 280, Train Acc: 0.8574, Test Acc: 0.8348\n",
      "Epoch: 281, Train Acc: 0.8601, Test Acc: 0.8338\n",
      "Epoch: 282, Train Acc: 0.8608, Test Acc: 0.8365\n",
      "Epoch: 283, Train Acc: 0.8708, Test Acc: 0.8470\n",
      "Epoch: 284, Train Acc: 0.8558, Test Acc: 0.8298\n",
      "Epoch: 285, Train Acc: 0.8722, Test Acc: 0.8465\n",
      "Epoch: 286, Train Acc: 0.8484, Test Acc: 0.8245\n",
      "Epoch: 287, Train Acc: 0.8593, Test Acc: 0.8338\n",
      "Epoch: 288, Train Acc: 0.8609, Test Acc: 0.8313\n",
      "Epoch: 289, Train Acc: 0.8786, Test Acc: 0.8486\n",
      "Epoch: 290, Train Acc: 0.8718, Test Acc: 0.8471\n",
      "Epoch: 291, Train Acc: 0.8501, Test Acc: 0.8290\n",
      "Epoch: 292, Train Acc: 0.8343, Test Acc: 0.8200\n",
      "Epoch: 293, Train Acc: 0.8727, Test Acc: 0.8439\n",
      "Epoch: 294, Train Acc: 0.8703, Test Acc: 0.8397\n",
      "Epoch: 295, Train Acc: 0.8476, Test Acc: 0.8280\n",
      "Epoch: 296, Train Acc: 0.8631, Test Acc: 0.8425\n",
      "Epoch: 297, Train Acc: 0.8748, Test Acc: 0.8479\n",
      "Epoch: 298, Train Acc: 0.8785, Test Acc: 0.8476\n",
      "Epoch: 299, Train Acc: 0.8705, Test Acc: 0.8514\n",
      "Epoch: 300, Train Acc: 0.8758, Test Acc: 0.8521\n",
      "Epoch: 301, Train Acc: 0.8612, Test Acc: 0.8370\n",
      "Epoch: 302, Train Acc: 0.8728, Test Acc: 0.8470\n",
      "Epoch: 303, Train Acc: 0.8817, Test Acc: 0.8532\n",
      "Epoch: 304, Train Acc: 0.8811, Test Acc: 0.8554\n",
      "Epoch: 305, Train Acc: 0.8760, Test Acc: 0.8454\n",
      "Epoch: 306, Train Acc: 0.8690, Test Acc: 0.8476\n",
      "Epoch: 307, Train Acc: 0.8810, Test Acc: 0.8564\n",
      "Epoch: 308, Train Acc: 0.8788, Test Acc: 0.8490\n",
      "Epoch: 309, Train Acc: 0.8547, Test Acc: 0.8258\n",
      "Epoch: 310, Train Acc: 0.8724, Test Acc: 0.8454\n",
      "Epoch: 311, Train Acc: 0.8625, Test Acc: 0.8410\n",
      "Epoch: 312, Train Acc: 0.8563, Test Acc: 0.8296\n",
      "Epoch: 313, Train Acc: 0.8519, Test Acc: 0.8253\n",
      "Epoch: 314, Train Acc: 0.8802, Test Acc: 0.8519\n",
      "Epoch: 315, Train Acc: 0.8702, Test Acc: 0.8436\n",
      "Epoch: 316, Train Acc: 0.8683, Test Acc: 0.8449\n",
      "Epoch: 317, Train Acc: 0.8790, Test Acc: 0.8538\n",
      "Epoch: 318, Train Acc: 0.8831, Test Acc: 0.8535\n",
      "Epoch: 319, Train Acc: 0.8845, Test Acc: 0.8579\n",
      "Epoch: 320, Train Acc: 0.8703, Test Acc: 0.8428\n",
      "Epoch: 321, Train Acc: 0.8613, Test Acc: 0.8407\n",
      "Epoch: 322, Train Acc: 0.8802, Test Acc: 0.8492\n",
      "Epoch: 323, Train Acc: 0.8817, Test Acc: 0.8569\n",
      "Epoch: 324, Train Acc: 0.8642, Test Acc: 0.8290\n",
      "Epoch: 325, Train Acc: 0.8802, Test Acc: 0.8521\n",
      "Epoch: 326, Train Acc: 0.8843, Test Acc: 0.8556\n",
      "Epoch: 327, Train Acc: 0.8686, Test Acc: 0.8409\n",
      "Epoch: 328, Train Acc: 0.8692, Test Acc: 0.8373\n",
      "Epoch: 329, Train Acc: 0.8831, Test Acc: 0.8546\n",
      "Epoch: 330, Train Acc: 0.8752, Test Acc: 0.8502\n",
      "Epoch: 331, Train Acc: 0.8806, Test Acc: 0.8535\n",
      "Epoch: 332, Train Acc: 0.8836, Test Acc: 0.8631\n",
      "Epoch: 333, Train Acc: 0.8791, Test Acc: 0.8484\n",
      "Epoch: 334, Train Acc: 0.8589, Test Acc: 0.8345\n",
      "Epoch: 335, Train Acc: 0.8662, Test Acc: 0.8407\n",
      "Epoch: 336, Train Acc: 0.8752, Test Acc: 0.8511\n",
      "Epoch: 337, Train Acc: 0.8840, Test Acc: 0.8587\n",
      "Epoch: 338, Train Acc: 0.8790, Test Acc: 0.8449\n",
      "Epoch: 339, Train Acc: 0.8810, Test Acc: 0.8516\n",
      "Epoch: 340, Train Acc: 0.8672, Test Acc: 0.8442\n",
      "Epoch: 341, Train Acc: 0.8864, Test Acc: 0.8572\n",
      "Epoch: 342, Train Acc: 0.8697, Test Acc: 0.8455\n",
      "Epoch: 343, Train Acc: 0.8868, Test Acc: 0.8558\n",
      "Epoch: 344, Train Acc: 0.8858, Test Acc: 0.8574\n",
      "Epoch: 345, Train Acc: 0.8883, Test Acc: 0.8580\n",
      "Epoch: 346, Train Acc: 0.8755, Test Acc: 0.8458\n",
      "Epoch: 347, Train Acc: 0.8833, Test Acc: 0.8567\n",
      "Epoch: 348, Train Acc: 0.8800, Test Acc: 0.8468\n",
      "Epoch: 349, Train Acc: 0.8864, Test Acc: 0.8595\n",
      "CPU times: user 3h 16min 13s, sys: 18.9 s, total: 3h 16min 32s\n",
      "Wall time: 3h 16min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print(result_translator)\n",
    "for epoch in range(350):\n",
    "    train()\n",
    "    train_acc = test(train_loader)\n",
    "    test_acc = test(test_loader)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a008947",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1a008947",
    "outputId": "64a5b296-110d-40eb-8009-4ed243525e64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1553: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.39969459e-12 4.45122048e-14 1.17344090e-09 ... 2.10734642e-13\n",
      "  2.15777374e-09 4.21783761e-25]\n",
      " [2.30897150e-08 3.90035088e-10 4.05156255e-14 ... 2.49647769e-13\n",
      "  5.24641332e-14 8.01613753e-14]\n",
      " [6.59610639e-28 7.46308205e-29 1.20454130e-25 ... 2.00946200e-41\n",
      "  8.41820243e-40 0.00000000e+00]\n",
      " ...\n",
      " [1.01201548e-26 1.97501558e-26 6.16903081e-19 ... 1.41300686e-23\n",
      "  1.35348564e-29 0.00000000e+00]\n",
      " [2.49996586e-15 3.03162430e-15 3.77162841e-12 ... 3.22445385e-14\n",
      "  5.20649356e-18 2.30786238e-21]\n",
      " [3.51482641e-17 5.03458318e-17 1.13439336e-14 ... 3.14129871e-16\n",
      "  2.34714489e-19 5.15687234e-22]]\n",
      "(6240, 187)\n",
      "[16. 14. 23. ... 74. 52. 21.]\n",
      "(6240,)\n",
      "[16. 14. 23. ... 74. 52. 21.]\n",
      "(6240,)\n",
      "The Area Under ROC Curve (AUC) = 0.999384885021119\n",
      "F1 Score = 0.9045689082415276\n",
      "Testing Accuracy = 0.8594551282051283\n",
      "[[22  0  0 ...  0  0  0]\n",
      " [ 1 13  0 ...  0  0  0]\n",
      " [ 0  0 52 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ... 17  0  0]\n",
      " [ 0  0  0 ...  0 18  0]\n",
      " [ 0  0  0 ...  0  0 12]]\n",
      "OrderedDict([('conv1.bias', tensor([ 0.1557, -0.5091, -0.0699, -0.8104, -0.4310,  0.1094,  0.3381,  0.0481,\n",
      "        -0.1932,  0.3225,  0.1196, -0.5572,  0.1110,  0.0745, -0.2374, -0.6565,\n",
      "        -0.3859, -0.6176,  0.1634,  0.4578,  0.0347, -0.5289, -0.0465,  0.0702,\n",
      "        -0.0376,  0.3679,  0.3814,  0.3753,  0.5919, -0.1937, -0.2273, -0.0239,\n",
      "        -0.1302, -0.1853,  0.1415, -0.0818, -0.4546, -0.6591, -0.0905, -0.5590,\n",
      "         0.0777, -0.7727,  0.0233,  0.1907, -0.0451, -0.0287, -0.1848, -0.2086,\n",
      "        -0.2266,  0.0595, -0.2536,  0.8779, -0.3750, -0.0526,  0.0781,  0.3642,\n",
      "        -0.0726,  0.0031,  0.0989, -0.0704,  0.6186,  0.0232,  0.3071,  0.2264,\n",
      "        -0.1870,  0.1619,  0.1217, -0.5807, -0.3442,  0.6070,  0.2487,  0.1327,\n",
      "        -0.2057, -0.1060, -0.3401, -0.0715, -0.3261, -0.3963, -0.5725, -0.3764,\n",
      "        -0.1906, -0.2090, -0.6668, -0.0552, -0.2574, -0.2040, -0.0110, -0.5594,\n",
      "         0.0828, -0.3710, -0.1986,  0.1021, -0.3367, -0.1586,  0.0180,  0.0504,\n",
      "        -0.1964,  0.1317, -0.5314, -0.5581, -0.3488, -0.0763,  0.1661,  0.4329,\n",
      "         0.0579, -0.4066,  0.0380,  0.0731, -0.0468,  0.0438, -0.0851,  0.0028,\n",
      "        -0.3033, -0.4402,  0.5160,  0.0905,  0.1468,  0.0292, -0.0197, -0.6860,\n",
      "        -0.0884,  0.0767, -0.1517, -0.4985,  0.0813, -0.0087,  0.0695, -0.1903,\n",
      "         0.0559,  0.0950, -0.2147, -0.0991, -0.0540, -0.4332, -0.2504,  0.2879,\n",
      "         0.1137, -0.2365,  0.2276, -0.3271, -0.1963,  0.7251, -0.5311,  0.0538,\n",
      "        -0.1627, -0.3350, -0.6737, -0.0825, -0.1721, -0.7681, -0.5251,  0.9683,\n",
      "        -0.5657,  0.0571, -0.5305, -0.3546,  0.2587,  0.1064,  0.1771,  0.5010,\n",
      "        -0.5075,  0.0314,  0.5177, -0.0867,  0.0267,  0.4979, -0.4184, -0.2300,\n",
      "        -0.5039, -0.2352, -0.3621,  0.1211,  0.2047, -0.2411, -0.2366, -0.4550,\n",
      "        -0.1518, -0.1832,  0.4305,  0.0217, -0.3927, -0.2486, -0.1877, -0.9032,\n",
      "        -0.0767, -0.4827,  0.0155, -0.1341, -0.2606,  0.2714, -0.0248,  0.2048,\n",
      "        -0.0229,  0.0289, -0.0194, -0.1557, -0.4321,  0.0119, -0.4449,  0.5855,\n",
      "        -0.2731, -0.2269, -0.5211,  0.7743, -0.3877, -0.1576,  0.0391, -0.0286,\n",
      "        -0.2517, -0.0048, -0.2571, -0.0520, -0.3705, -0.2646, -0.0266, -0.1550,\n",
      "        -0.3043,  0.4253, -0.3672, -0.3490, -0.4093, -0.2987, -0.7093, -0.0733,\n",
      "         0.1392, -0.2494, -0.0398, -0.2150,  0.4395,  0.0157, -0.0184,  0.1339,\n",
      "         0.0075, -0.5906,  0.1734, -0.3070, -0.1138, -0.1268,  0.5973, -0.2868,\n",
      "        -0.0206, -0.0807, -0.9051,  0.0142, -0.2272,  0.0955, -0.2919, -0.1245,\n",
      "        -0.0398, -0.2810, -0.1248,  0.0309,  0.6533, -0.1754,  0.1416, -0.0608,\n",
      "         1.0817, -0.2748,  0.0665,  0.1605, -0.1374, -0.0598, -0.5444, -0.3096,\n",
      "         0.0394,  0.0479, -0.2513,  0.1864, -0.1785, -0.2106, -0.2568,  0.1597,\n",
      "        -0.3271,  0.0763, -0.0251,  0.1728, -0.0760,  0.0029,  0.4625,  0.0822,\n",
      "        -0.5788,  0.3826,  0.0049, -0.3415, -0.0074,  0.4183, -0.0381, -0.1275,\n",
      "        -0.0448, -0.1853, -0.2759, -0.1571,  0.2120,  0.0835,  0.1945, -0.2389,\n",
      "        -0.0571, -0.2417, -0.4922, -0.1698, -0.0880, -0.5608, -0.1569,  0.2465,\n",
      "        -0.2140,  0.1028, -0.3293, -0.4268,  0.0336, -0.4836, -0.4384, -0.1552,\n",
      "        -0.0567, -0.0899, -0.0868, -0.4092, -0.0890, -0.1813, -0.0134, -0.0283,\n",
      "         0.1017,  0.2692, -0.1271, -0.3129,  0.1521,  0.7039, -0.1666, -0.5670,\n",
      "         0.1414, -0.0667, -0.1617, -0.6282, -0.0323,  0.1235,  0.0170,  0.7204,\n",
      "         0.3263,  0.1149,  0.0210, -0.3765, -0.0371,  0.0440, -0.0487,  0.4232,\n",
      "        -0.3722,  0.8277, -0.3604,  0.4298,  0.3249,  0.4451, -0.2481, -0.4782,\n",
      "        -0.0307, -0.4523,  0.6984, -0.6329,  0.3602, -0.2582, -0.3750,  0.3031,\n",
      "         0.0611, -0.1083, -0.4912, -0.1618, -0.6035, -0.6016, -0.4583,  0.1565,\n",
      "         0.1732, -0.1770,  0.3271, -0.5194, -0.3268,  0.0523, -0.1970,  0.1310,\n",
      "        -0.2508, -0.1099,  0.0742, -0.4876, -0.3655, -0.4178,  0.8531, -0.2328,\n",
      "        -0.1117, -0.2139, -0.2541, -0.3387,  0.0776, -0.2531, -0.0657, -0.4213,\n",
      "        -0.3771,  0.2089,  0.0674, -0.1943, -0.0632, -0.3626, -0.4643, -0.3302,\n",
      "        -0.1852,  0.0838,  0.1381, -0.1235, -0.2779, -0.4445,  0.0843, -0.0351,\n",
      "        -0.3905, -0.1680,  0.1185, -0.4557, -0.0963, -0.1006,  0.3033,  0.0968,\n",
      "        -0.2469,  0.0565, -0.2279,  0.4882,  0.2288, -0.1695, -0.2644, -0.1873,\n",
      "        -0.5085, -0.0304, -0.2790, -0.1623,  0.7525,  0.1000, -0.2431, -0.4408,\n",
      "        -0.0130,  0.1834,  0.7428, -0.1824, -0.0276,  0.5120, -0.4331, -0.3328,\n",
      "        -0.1665, -0.4163,  0.0785, -0.0634,  0.0689, -0.1065,  0.5797, -0.0171,\n",
      "        -0.0026,  0.7321, -0.5045, -0.8070, -0.7474,  0.1687, -0.1112, -0.4692,\n",
      "        -0.3411, -0.2200, -0.1217, -0.5525, -0.6404,  0.0885,  0.1100, -0.2214,\n",
      "        -0.2907,  0.2881,  0.3532, -0.1746, -0.2028, -0.4992, -0.0032, -0.4436,\n",
      "        -0.1972, -0.1480, -0.2680, -0.3394, -0.6294, -0.3706,  0.6816, -0.0883,\n",
      "        -0.3285, -0.0348, -0.4305, -0.2511, -0.2828,  0.0594, -0.0919,  0.0124,\n",
      "        -0.0797, -0.4867, -0.3524, -0.0277,  0.9737,  0.1142, -0.3856,  0.0277,\n",
      "        -0.3350, -0.2159,  0.2034, -0.1195, -0.1762,  0.3837, -0.0960,  0.0635,\n",
      "         0.1422, -0.2604,  0.0486, -0.1446, -0.2103, -0.5021, -0.3567, -0.1753],\n",
      "       device='cuda:0')), ('conv1.lins.0.weight', tensor([[-1.0148e-01, -1.0507e-01, -2.5353e+00,  2.1287e-01, -3.8228e-02,\n",
      "          7.9176e-01],\n",
      "        [-1.8694e+00, -5.4961e-01, -2.9679e-01, -1.2424e-01, -1.8319e-01,\n",
      "          7.1767e-01],\n",
      "        [-1.0585e-01,  3.7806e-02, -2.2104e-03, -1.1089e-01, -1.8518e-01,\n",
      "         -1.3227e-01],\n",
      "        ...,\n",
      "        [-1.2220e-01, -5.8287e-01,  1.2988e-01, -5.4073e-01, -1.5551e-01,\n",
      "         -7.4284e-01],\n",
      "        [-6.0731e-01, -9.8181e-01, -2.7763e-01,  6.6022e-01,  3.4962e-01,\n",
      "          1.3423e+00],\n",
      "        [ 1.0975e-01,  1.1646e+00,  1.3313e-02, -3.0634e-01,  1.7901e-01,\n",
      "         -9.3689e-01]], device='cuda:0')), ('conv1.lins.1.weight', tensor([[ 0.1387, -0.0136,  2.3309, -0.1905,  0.0229, -0.5809],\n",
      "        [ 1.6421, -0.0162,  0.6442, -0.0861,  0.6875, -0.3313],\n",
      "        [-0.0199, -0.0408,  0.0501, -0.1156,  0.1647, -0.0301],\n",
      "        ...,\n",
      "        [ 0.2621,  0.2690,  0.0915,  0.0362,  0.3838,  0.7797],\n",
      "        [ 0.5110,  0.2418,  0.0240, -0.0797, -0.0701, -0.6768],\n",
      "        [ 0.1476, -1.0533,  0.0191,  0.3565,  0.1139,  1.1049]],\n",
      "       device='cuda:0')), ('conv1.lins.2.weight', tensor([[-0.0019, -0.0896, -1.6382, -0.4627, -0.1018,  0.6088],\n",
      "        [-0.3088,  0.5685, -0.3839,  0.1827, -0.5320,  0.1436],\n",
      "        [ 0.0901,  0.0672, -0.0929, -0.0193, -0.0552,  0.0023],\n",
      "        ...,\n",
      "        [ 0.0258, -0.4756, -0.0493,  0.6629,  0.0198,  0.1215],\n",
      "        [ 0.1708,  0.7230,  0.1518, -0.3936, -0.3799,  0.0631],\n",
      "        [-0.0948,  0.9639, -0.0433, -0.5206, -0.1136, -1.0976]],\n",
      "       device='cuda:0')), ('conv2.bias', tensor([-3.5632e-01, -4.1768e-02, -7.1398e-02, -1.7626e-02, -5.7298e-03,\n",
      "        -7.4771e-01, -1.8669e+00, -5.4661e-02,  2.0521e-01, -6.6328e-01,\n",
      "        -4.7042e-02, -6.0889e-02, -4.2570e-01, -8.2923e-02, -7.4602e-01,\n",
      "        -2.5308e-01,  2.0572e-01, -7.0437e-02, -1.0099e-01, -6.3195e-02,\n",
      "        -8.6855e-02, -1.4979e-01, -5.6135e-02, -6.4463e-02,  2.7202e-01,\n",
      "         5.7650e-01,  7.3919e-02,  5.8882e-02, -8.8135e-02, -6.1782e-02,\n",
      "        -4.2422e-01, -7.9775e-01, -7.7873e-01, -3.1651e-02,  1.3678e-01,\n",
      "        -3.9941e-02, -8.4934e-02, -1.0420e-01, -5.5831e-02, -7.9152e-01,\n",
      "        -3.7413e-01, -8.7832e-01, -1.0538e+00, -9.0975e-01, -1.0688e-01,\n",
      "        -3.8024e-02, -7.4186e-02, -1.9593e-01, -8.9336e-02, -5.1940e-02,\n",
      "        -4.1637e-02,  1.8396e-01, -9.9994e-01, -1.4827e-01, -9.0517e-02,\n",
      "        -1.5659e-01, -9.6326e-02, -2.7652e-02, -6.2562e-02, -5.9740e-02,\n",
      "         4.6820e-02, -5.3465e-01, -4.3402e-01, -7.3095e-01, -1.9449e-01,\n",
      "        -2.8692e-02, -5.6250e-01, -5.9768e-01, -7.0797e-02, -8.8544e-01,\n",
      "        -3.8347e-02,  2.0143e-01, -4.3774e-01, -6.1688e-02, -2.8855e-01,\n",
      "        -3.6970e-02, -7.3999e-01,  5.4665e-01, -1.0336e+00, -3.6532e-02,\n",
      "        -7.7242e-01, -4.7242e-02, -2.9228e-02, -8.4092e-01, -1.1402e+00,\n",
      "        -1.5489e-01, -6.6184e-02, -6.4122e-02, -3.6184e-02, -2.1203e-02,\n",
      "        -9.2748e-02, -4.5270e-02, -3.2993e-01, -2.0811e-01, -6.2031e-02,\n",
      "        -4.2044e-02, -2.4688e-02,  4.6464e-02, -4.9649e-02, -7.5460e-01,\n",
      "        -1.1995e-02, -8.8240e-02, -3.1360e-01, -1.2164e-01, -2.5641e-01,\n",
      "        -6.1368e-01, -7.2617e-01, -4.2383e-02, -6.3808e-02, -2.4722e-01,\n",
      "        -7.5256e-02, -6.6064e-02, -1.7414e-01, -1.8094e-01, -5.7415e-02,\n",
      "        -7.2733e-02, -1.3006e+00,  8.6773e-02,  4.9367e-02, -5.1475e-02,\n",
      "        -5.3372e-01, -6.7224e-02, -5.7540e-02, -4.8715e-02, -4.7597e-01,\n",
      "         1.4283e-01, -2.4530e-01, -6.0450e-02,  5.0148e-01, -5.2618e-02,\n",
      "        -1.9779e-01, -5.4348e-01, -3.3961e-02,  7.2819e-02, -6.2580e-02,\n",
      "        -2.8972e-01, -2.9400e-01, -7.2238e-02, -5.5387e-02,  1.6366e-01,\n",
      "        -5.5735e-01, -4.3815e-02, -2.8247e-01, -5.5986e-02,  2.8964e-01,\n",
      "        -1.5361e-01, -1.1395e-01, -7.5808e-02,  4.1165e-02, -7.8377e-01,\n",
      "        -1.1763e-01,  2.2092e-01,  5.4448e-01, -1.2769e-01, -9.7106e-01,\n",
      "        -5.8125e-02,  3.2473e-01, -3.1751e-01, -6.3042e-01, -5.5250e-02,\n",
      "        -8.2695e-01, -1.1199e-02, -1.6069e-01,  8.3231e-02, -6.3896e-02,\n",
      "        -2.3581e-02, -3.9375e-02, -4.9979e-01, -5.4284e-02, -2.3606e-01,\n",
      "        -4.0150e-01, -1.9525e-02,  1.2409e-01, -8.9900e-01, -4.0172e-02,\n",
      "        -6.8547e-02, -1.0565e-02, -6.7720e-01, -8.3528e-02, -1.8829e-01,\n",
      "        -5.7472e-02, -1.1163e-03, -7.7946e-01, -4.9139e-02, -7.5793e-02,\n",
      "        -3.8315e-02, -5.4645e-02,  1.2408e-01, -5.4644e-02, -6.5159e-01,\n",
      "        -7.0912e-01,  1.3311e-01, -6.9367e-02, -4.1645e-02, -4.5131e-02,\n",
      "        -3.2188e-01, -2.1695e-02, -5.7222e-01, -1.6106e-01, -2.2475e-01,\n",
      "        -2.3407e-01, -3.6955e-01, -3.7989e-02, -1.7605e-01,  3.0012e-01,\n",
      "         3.6043e-02, -8.1413e-02,  2.5065e-02, -3.3743e-01, -2.0319e-01,\n",
      "        -6.6877e-02, -1.8807e-02, -1.4489e-01, -6.6551e-02, -8.1677e-01,\n",
      "         8.7712e-02, -1.9523e-01, -9.6526e-02, -4.1327e-02, -4.1837e-01,\n",
      "        -1.1227e-01, -2.1828e-02, -5.4268e-02, -7.7391e-01, -3.3512e-02,\n",
      "        -7.0855e-02, -7.3131e-02, -1.3236e-02, -5.6672e-02, -2.3796e-01,\n",
      "         3.8627e-02, -4.7655e-02, -9.8986e-01, -7.6251e-01, -3.3722e-02,\n",
      "        -5.6457e-01, -4.7189e-02, -4.9756e-02, -5.3504e-01, -8.0025e-01,\n",
      "        -8.3956e-01, -4.9345e-02, -9.3893e-02,  9.7065e-02, -1.8984e-01,\n",
      "        -6.4215e-02,  3.6904e-01, -1.2214e-02, -5.8291e-02, -5.3413e-03,\n",
      "        -3.4816e-01, -1.3546e-01, -9.4993e-02, -2.6549e-01,  7.5370e-01,\n",
      "        -5.8420e-02, -5.4770e-02, -3.1924e-02, -1.0004e+00, -5.8783e-02,\n",
      "        -6.8802e-01, -5.5139e-02, -1.0202e+00, -6.4200e-01, -4.9907e-02,\n",
      "        -7.4493e-01, -6.7502e-01, -7.1492e-02, -3.7730e-01, -6.6461e-01,\n",
      "        -5.3146e-01, -7.8835e-01, -4.0556e-01, -2.2415e-01,  5.1902e-01,\n",
      "        -4.2680e-02,  1.3773e-01,  4.0631e-01, -2.8598e-02, -1.1949e+00,\n",
      "        -2.5708e-01, -1.3891e-01, -9.0578e-02, -3.4827e-02, -3.9658e-01,\n",
      "        -2.6504e-01, -5.1933e-02, -1.0539e-01, -3.8725e-01, -4.6395e-02,\n",
      "        -4.8078e-01, -1.1379e+00,  7.4595e-02, -4.6164e-01, -8.5339e-02,\n",
      "         2.2139e-01,  5.6495e-02,  8.3702e-02, -7.7028e-02, -4.8914e-02,\n",
      "        -3.7695e-01, -3.0393e-01, -1.7547e-02, -8.2979e-02, -8.8322e-02,\n",
      "        -1.0531e-01, -1.6775e-01, -4.1754e-02, -7.4628e-02,  5.5892e-02,\n",
      "        -3.2364e-02, -4.7630e-02, -2.9083e-02, -4.5297e-02,  3.8498e-02,\n",
      "         3.5648e-02,  2.1339e-01, -4.7909e-02,  5.0450e-01, -6.5617e-02,\n",
      "        -4.8183e-01, -1.4218e-01, -9.1997e-01, -1.4178e-01, -6.8666e-02,\n",
      "        -6.7170e-02,  3.2016e-01, -1.4753e-01, -4.7816e-02,  2.0986e-01,\n",
      "        -2.5686e-01, -3.8433e-02,  6.9643e-01, -7.8034e-02,  5.4000e-01,\n",
      "         9.7988e-01, -4.8881e-01,  6.8321e-01, -6.7835e-02, -7.4441e-02,\n",
      "        -4.9903e-02, -5.7914e-01, -2.3466e-01, -6.5536e-02, -8.7371e-02,\n",
      "        -3.4830e-02, -4.2919e-02, -1.8348e-02, -9.8522e-01, -5.1161e-02,\n",
      "         4.8474e-02, -1.5335e-01, -1.9028e-02,  2.7271e-01, -7.2230e-02,\n",
      "         2.0404e-01, -5.3365e-02, -7.6373e-01, -9.0979e-02,  9.1400e-02,\n",
      "        -7.8205e-01, -9.1295e-01, -5.1167e-01, -4.7774e-02, -2.2247e-02,\n",
      "        -7.5342e-02, -6.0527e-01, -8.2857e-02, -6.5586e-02,  7.7137e-01,\n",
      "        -9.4425e-01,  5.0743e-01, -5.0040e-01, -2.5632e-02,  7.7990e-02,\n",
      "        -2.5511e-02, -1.2085e-01, -8.6440e-01, -6.0317e-01, -5.4380e-02,\n",
      "        -1.3676e-01,  5.2145e-01,  8.6580e-03,  4.2756e-01, -2.2118e-01,\n",
      "        -1.0969e+00, -6.3625e-02, -1.2535e-01, -6.1556e-02, -7.6010e-02,\n",
      "         1.0466e-01, -4.2139e-01,  2.5967e-01, -9.7840e-01, -6.5071e-02,\n",
      "        -7.7875e-01,  7.2869e-01, -2.1194e-01,  1.0618e-01, -8.5913e-02,\n",
      "         2.9908e-01,  7.6274e-02,  3.1591e-02, -4.4356e-02, -1.4495e-01,\n",
      "        -2.1517e-02, -3.1341e-01, -2.5749e-01, -6.1285e-02, -1.7790e-02,\n",
      "        -1.1336e-01, -6.5831e-01, -8.8999e-01,  4.4992e-03, -4.6596e-02,\n",
      "        -4.0374e-01,  1.4629e-02,  1.6916e-01, -6.2817e-01,  3.4065e-01,\n",
      "        -8.3747e-02, -3.1196e-01,  3.1867e-04, -9.1553e-01, -3.0997e-02,\n",
      "         7.2566e-01, -5.5925e-01, -2.5972e-01, -1.0631e-01, -4.7352e-02,\n",
      "        -4.3617e-02, -9.8810e-02,  1.7977e-01,  8.4592e-02, -4.5595e-01,\n",
      "        -4.5543e-01, -1.7863e-01, -7.1546e-01, -5.4719e-02,  1.2563e-01,\n",
      "        -6.4410e-02, -7.5932e-02,  1.6227e-01,  4.6064e-01, -5.5578e-02,\n",
      "        -3.6845e-01, -3.8460e-02, -8.2422e-01, -4.5341e-01, -1.3575e-01,\n",
      "        -2.5588e-01,  8.6415e-03, -4.2931e-03, -4.1645e-01, -1.1855e+00,\n",
      "        -5.7485e-02, -1.0358e-01, -1.2151e-01, -4.6797e-01, -1.0931e+00,\n",
      "        -1.0924e-01,  5.6327e-01,  1.7238e-01, -6.9795e-02, -2.2009e-01,\n",
      "         3.3595e-02, -6.5620e-02, -6.6741e-01, -5.2542e-02, -4.0005e-02,\n",
      "        -3.3106e-02, -2.3751e-01, -3.8312e-01, -4.0938e-01, -9.7727e-01,\n",
      "        -2.0696e-01, -7.9609e-02, -7.0980e-02,  1.9064e-01, -1.2544e+00,\n",
      "        -2.3182e-01,  5.1222e-02, -1.5204e-01,  5.3858e-01, -1.1335e-01,\n",
      "         1.8560e-01,  8.2498e-01, -2.8081e-01, -2.1121e-01, -5.4164e-01,\n",
      "         3.4595e-01, -2.9561e-01, -5.1394e-02, -3.8645e-02,  1.9842e-01,\n",
      "        -4.7607e-01,  5.3022e-01,  8.2606e-01,  6.2353e-01,  4.6039e-02,\n",
      "         1.4242e-02,  3.3234e-01, -8.9810e-02,  1.0110e-02, -4.4638e-02,\n",
      "        -5.5815e-02,  2.8745e-01, -4.1519e-02, -1.1306e+00, -7.8650e-02,\n",
      "        -3.3930e-02,  1.4364e-01], device='cuda:0')), ('conv2.lins.0.weight', tensor([[-1.7305e+00, -1.4320e-01,  1.8160e-01,  ...,  2.4989e-02,\n",
      "          3.9369e-01, -6.0636e-01],\n",
      "        [ 1.8157e-02, -9.7460e-03,  5.6391e-02,  ..., -3.6634e-02,\n",
      "          3.6687e-02,  5.6612e-02],\n",
      "        [ 1.4686e-02,  1.1460e-03,  7.4726e-02,  ..., -3.0565e-03,\n",
      "         -4.8665e-02,  1.9084e-02],\n",
      "        ...,\n",
      "        [-5.2680e-02,  3.1352e-02, -3.2859e-03,  ...,  2.3005e-02,\n",
      "         -9.9236e-02, -8.2344e-02],\n",
      "        [ 4.9515e-02,  6.2379e-02,  5.2124e-02,  ..., -4.3209e-02,\n",
      "         -3.2471e-02,  3.7247e-02],\n",
      "        [ 1.9564e-01,  6.6757e-01,  1.3352e-02,  ...,  4.3196e-01,\n",
      "          4.4467e-01,  2.1496e-01]], device='cuda:0')), ('conv2.lins.1.weight', tensor([[ 1.7626, -0.0415, -0.0917,  ..., -0.1865, -0.3470,  0.5029],\n",
      "        [ 0.0079,  0.0824, -0.0285,  ..., -0.0198,  0.0554,  0.0331],\n",
      "        [ 0.0505,  0.0185, -0.0660,  ...,  0.0422, -0.0479,  0.0535],\n",
      "        ...,\n",
      "        [ 0.0118,  0.0400, -0.0509,  ...,  0.0365,  0.0168, -0.0619],\n",
      "        [-0.0819, -0.0257,  0.0758,  ...,  0.0201, -0.0572,  0.1002],\n",
      "        [ 0.9358, -0.2959,  0.0315,  ..., -0.7357, -0.1117, -0.3150]],\n",
      "       device='cuda:0')), ('conv2.lins.2.weight', tensor([[-3.0127e-01, -2.4057e-02,  1.3338e-01,  ..., -1.1982e-01,\n",
      "          3.2011e-01, -5.0238e-01],\n",
      "        [-7.7144e-02,  3.2746e-02,  1.7367e-02,  ..., -6.7371e-02,\n",
      "          4.9462e-02,  2.2595e-02],\n",
      "        [-3.7786e-02, -3.4300e-03, -5.5848e-02,  ..., -1.3671e-02,\n",
      "         -6.8529e-03, -1.0266e-01],\n",
      "        ...,\n",
      "        [-4.3790e-04, -4.6617e-03, -7.1293e-02,  ..., -6.2345e-02,\n",
      "         -1.0587e-01,  1.5838e-02],\n",
      "        [-8.6489e-03, -9.1549e-03,  2.8745e-02,  ...,  5.2212e-02,\n",
      "          1.2279e-02, -8.3114e-02],\n",
      "        [-1.8405e+00, -3.3211e-01, -5.0784e-02,  ...,  6.6088e-01,\n",
      "         -1.4759e+00,  2.0232e-01]], device='cuda:0')), ('conv2.lins.3.weight', tensor([[ 0.2324, -0.0876, -0.0974,  ...,  0.0792, -0.6962,  0.2958],\n",
      "        [ 0.0029, -0.0543, -0.0280,  ...,  0.0626,  0.0161,  0.0673],\n",
      "        [-0.0414, -0.0449,  0.0183,  ..., -0.0112,  0.0666, -0.0088],\n",
      "        ...,\n",
      "        [ 0.0439,  0.0973, -0.0040,  ...,  0.0523,  0.0434,  0.0781],\n",
      "        [ 0.0281, -0.0067, -0.0077,  ..., -0.0272,  0.0088, -0.0158],\n",
      "        [-0.7032,  0.0285, -0.0417,  ..., -0.3105,  1.2214, -0.4421]],\n",
      "       device='cuda:0')), ('conv3.bias', tensor([-9.6400e-01, -4.4460e-01, -1.4955e-01, -1.9149e-01, -3.1729e-01,\n",
      "        -2.3603e-02, -8.5739e-01, -5.9738e-01, -5.5684e-01, -2.5379e-01,\n",
      "        -1.5615e-01,  1.2173e-01, -1.9305e-01, -5.7337e-01, -2.3785e-01,\n",
      "         3.8495e-02, -5.0714e-01, -1.7744e-01, -4.8169e-01, -1.3098e-01,\n",
      "        -1.2298e-01, -1.5576e-01, -6.0908e-01, -4.4677e-01, -2.9660e-01,\n",
      "        -5.3144e-01, -2.2144e-01, -7.7288e-02, -8.3941e-01, -8.0458e-01,\n",
      "        -7.4713e-01, -5.7817e-01, -1.5744e-01, -5.1540e-01,  1.1752e-01,\n",
      "        -1.0713e+00,  1.5826e-01, -6.1178e-01,  1.2826e-01, -1.2086e-01,\n",
      "        -3.1283e-01, -6.2657e-01, -1.2984e-01,  2.8087e-01, -2.1264e-02,\n",
      "        -9.0496e-01,  1.7102e-01, -1.5126e-01, -3.4720e-02,  3.9851e-02,\n",
      "        -3.1234e-01, -3.5687e-01, -1.0312e-01,  2.0080e-02, -1.4843e-01,\n",
      "        -3.4414e-01, -1.1906e-01, -4.6662e-01, -8.7312e-01, -1.0162e+00,\n",
      "        -1.3876e-01,  7.3730e-01,  5.6335e-01, -9.8652e-02, -6.5843e-01,\n",
      "        -1.0102e-01, -1.3047e-01, -4.4381e-01, -1.8001e-01, -7.9670e-01,\n",
      "        -1.0363e+00, -6.4582e-01,  1.5557e-01, -2.7078e-01, -2.1589e-01,\n",
      "        -7.9188e-01, -4.7450e-01, -5.2995e-01, -1.0005e-01, -1.2517e-01,\n",
      "        -2.9287e-01,  1.3505e-01, -1.4084e-01, -1.4819e-02,  7.0344e-02,\n",
      "        -8.3590e-01, -2.0152e-01,  6.3028e-02, -1.3169e-01, -7.6111e-01,\n",
      "        -1.0117e-01, -5.6561e-01, -8.3807e-01,  3.7631e-01, -5.5453e-01,\n",
      "         3.4347e-02, -4.3879e-01, -4.5900e-01, -1.1397e+00, -3.9642e-01,\n",
      "         2.2373e-01, -2.1913e-01, -2.7054e-01, -1.5543e-01, -5.2730e-01,\n",
      "        -4.0285e-01, -4.9815e-01, -6.1895e-01, -2.2627e-01, -4.9435e-01,\n",
      "        -1.0200e+00, -5.4423e-01,  7.7363e-02,  1.2293e-01, -2.6680e-01,\n",
      "        -8.9034e-01, -2.6018e-01, -1.6940e-01, -4.9152e-01,  6.7803e-02,\n",
      "        -4.7776e-01, -6.1773e-01,  1.0718e-01,  2.5164e-02, -5.4187e-01,\n",
      "         3.8717e-03,  5.7971e-01, -8.3597e-01,  1.2237e-01, -1.1736e-01,\n",
      "        -1.9320e-01, -7.5453e-01, -1.2312e-01, -3.6533e-01,  8.3860e-02,\n",
      "        -9.3139e-02,  3.7868e-01, -9.4085e-02, -1.6850e-01, -3.7848e-01,\n",
      "         1.9358e-01, -1.2362e-01, -5.9725e-01, -9.6116e-01, -1.2000e-02,\n",
      "        -1.9771e-01, -6.2475e-01, -2.3631e-01,  1.5573e-01, -2.6341e-01,\n",
      "        -1.6084e-01, -1.6680e-01, -3.2644e-01, -6.9237e-01, -3.4822e-01,\n",
      "        -3.4526e-01, -3.3933e-01,  9.1290e-02, -6.5845e-01, -6.6251e-01,\n",
      "        -8.9805e-01, -7.4813e-02, -3.7531e-01, -6.2152e-02, -2.6220e-02,\n",
      "         9.3013e-02, -2.5925e-02, -4.0296e-01, -4.6473e-01, -3.5572e-01,\n",
      "        -3.9436e-01, -5.6805e-02,  3.7912e-02,  7.6198e-02, -1.8316e-01,\n",
      "        -1.1886e-01, -6.3985e-01, -6.4713e-01, -1.2844e-01, -7.1936e-01,\n",
      "        -2.7073e-01, -4.9907e-01, -4.6684e-01, -8.6860e-02, -6.2924e-01,\n",
      "        -1.1557e-01, -5.1523e-01, -3.6844e-01, -2.4835e-01, -2.4038e-01,\n",
      "        -1.9157e-01, -2.0727e-01, -4.4744e-01, -9.4329e-02, -6.3604e-01,\n",
      "         4.6132e-01, -1.4725e-01, -2.5889e-01, -4.3382e-01, -1.0762e-01,\n",
      "        -1.7047e-01, -3.8173e-01,  2.7826e-01, -6.1292e-01, -1.9838e-01,\n",
      "         1.7837e-01, -2.2972e-01, -2.7075e-01, -3.5202e-01, -7.8649e-01,\n",
      "        -2.5829e-01, -2.9636e-02, -1.5086e-01, -3.2062e-01, -4.6923e-01,\n",
      "        -8.4333e-02, -1.2311e-01, -2.9645e-01, -2.5534e-01, -5.6906e-01,\n",
      "        -1.4590e-01, -3.4934e-01, -1.7066e-01, -5.2028e-02, -3.1374e-01,\n",
      "        -1.1127e-01, -5.0120e-01, -5.9911e-01, -8.9877e-02, -1.6278e-01,\n",
      "         1.4231e-01, -2.0554e-01, -1.4471e-01,  3.1096e-01, -4.7752e-01,\n",
      "        -5.4492e-01, -5.3184e-02, -1.0869e-01, -2.4970e-02,  4.0333e-01,\n",
      "        -1.8827e-02, -6.9611e-01, -2.1181e-01, -3.7378e-01, -1.3894e-01,\n",
      "        -6.0215e-02, -6.9704e-01, -2.7874e-01,  1.0585e-01, -9.2580e-02,\n",
      "        -4.7298e-01, -5.5780e-01, -8.0695e-01, -1.8391e-01, -8.5857e-02,\n",
      "        -6.1166e-01, -6.5408e-01, -1.3206e-01, -6.4239e-02, -3.7931e-02,\n",
      "        -2.6972e-01, -6.7214e-01, -3.7534e-01, -1.1779e-01, -2.5188e-01,\n",
      "        -3.7387e-01,  3.1632e-01, -7.6587e-01, -2.8137e-01,  3.3332e-01,\n",
      "        -6.0383e-01, -2.1694e-01, -3.9974e-02, -8.1943e-01, -2.2413e-01,\n",
      "         2.4808e-01, -2.1523e-01, -6.3984e-01, -2.8620e-01, -8.1367e-01,\n",
      "        -5.1475e-01, -5.2299e-01, -9.8502e-02, -1.0185e+00, -1.5063e-01,\n",
      "        -3.0353e-01,  4.3003e-01, -5.3624e-01, -3.1802e-01, -2.4432e-01,\n",
      "        -5.4609e-01, -2.7051e-01, -4.8607e-01, -7.9607e-01, -1.2128e-01,\n",
      "        -4.7146e-02,  9.3174e-02, -3.7331e-01,  9.7955e-02, -2.8741e-01,\n",
      "        -5.5044e-01, -6.6313e-01, -1.2264e-01, -7.6629e-02,  3.1869e-01,\n",
      "        -2.5999e-01, -6.6265e-01,  5.2185e-02, -3.0961e-01, -4.9471e-01,\n",
      "        -4.1481e-01, -3.9026e-01, -5.6128e-01,  2.4490e-01, -5.4731e-01,\n",
      "        -9.3172e-01, -9.9599e-02, -2.7582e-03, -3.2842e-01, -6.6855e-01,\n",
      "         2.7675e-01, -4.8432e-01, -5.4368e-01, -1.3354e-01, -3.9945e-02,\n",
      "        -5.4825e-01, -1.0986e+00, -1.0912e-01, -8.1157e-01,  4.0195e-01,\n",
      "        -2.9149e-01, -4.4232e-01, -4.2800e-01, -3.2421e-01, -4.6095e-01,\n",
      "         2.0672e-01, -3.6804e-01, -4.9870e-01,  1.5086e-01,  1.9196e-02,\n",
      "        -4.6524e-01,  2.0451e-01, -1.3818e-01,  2.2508e-05, -1.1943e-01,\n",
      "        -1.0490e+00, -5.5833e-01, -3.3479e-01,  2.1731e-01, -3.1498e-01,\n",
      "        -5.7519e-01, -5.2340e-02, -8.6352e-01, -2.0759e-01, -2.9635e-01,\n",
      "        -1.2568e-01, -2.4466e-01,  4.9870e-01,  2.3393e-02, -3.1613e-01,\n",
      "        -7.3850e-01, -5.5383e-01, -1.0686e+00, -7.6224e-02, -2.8649e-02,\n",
      "        -1.2327e-02, -5.7954e-01, -6.5291e-01, -7.0201e-01, -3.4106e-01,\n",
      "        -1.7639e-01,  2.7441e-01, -7.8909e-01, -5.8680e-01, -2.5481e-01,\n",
      "        -6.4783e-02, -1.3317e-01,  2.8867e-01, -1.0268e+00, -3.0675e-01,\n",
      "        -2.2816e-01,  7.4104e-03, -3.4034e-01,  7.0592e-02, -3.4999e-01,\n",
      "        -3.0792e-01, -6.2651e-01, -3.8117e-01,  2.0050e-01, -5.7132e-02,\n",
      "        -3.2846e-01, -4.8730e-01, -5.5936e-01, -4.3904e-01,  1.9248e-01,\n",
      "        -1.1157e-01,  1.4901e-02,  4.9190e-02,  1.5199e-02, -3.3798e-01,\n",
      "        -2.9911e-01, -8.7463e-02, -3.9922e-01, -6.5389e-01, -1.2987e-01,\n",
      "        -2.3963e-01, -1.7806e-01,  6.0955e-02, -9.2310e-01, -4.1236e-01,\n",
      "        -3.0317e-01,  1.9401e-01, -6.8131e-01, -5.1238e-01, -3.4537e-01,\n",
      "         2.7961e-01, -5.7893e-01, -2.0318e-01, -1.2626e-01, -7.5415e-01,\n",
      "         1.0964e-01, -9.1737e-01, -1.9387e-01, -6.9217e-01, -6.4257e-02,\n",
      "        -3.0378e-01, -5.6062e-01, -5.6355e-01,  2.0679e-01, -1.0343e+00,\n",
      "        -7.8587e-01, -5.9506e-01, -4.5793e-01,  3.5344e-02, -3.0756e-01,\n",
      "        -4.1609e-01, -3.6023e-01, -7.8757e-02,  7.7412e-02, -3.3548e-01,\n",
      "        -4.2900e-01, -6.0770e-01, -8.7030e-02, -8.1862e-01, -4.5461e-01,\n",
      "        -8.8222e-01,  2.0699e-02, -1.4686e-01, -3.2983e-01,  2.1692e-01,\n",
      "        -6.8690e-02, -5.3532e-02, -7.5506e-02, -6.3180e-01, -5.1378e-01,\n",
      "         1.2025e-01, -4.0942e-01, -4.9274e-01, -4.5519e-02,  5.0381e-02,\n",
      "        -1.0377e-01, -1.7957e-01, -9.3349e-02, -4.8345e-01,  2.1519e-02,\n",
      "        -2.5144e-01, -1.4726e-01,  1.7156e-01, -1.3767e-01, -6.3850e-01,\n",
      "        -8.4978e-01, -1.3117e-01, -1.3306e-01, -6.3784e-01,  6.7209e-01,\n",
      "        -6.6774e-01, -1.9913e-01, -6.6455e-01, -1.0939e+00, -1.3606e-01,\n",
      "         5.3191e-02,  1.6957e-01,  6.3802e-02, -9.6553e-01, -4.7324e-01,\n",
      "        -1.8102e-01, -5.2094e-01, -5.3736e-01, -3.9500e-01, -9.1577e-02,\n",
      "        -1.7645e-01, -4.4043e-01, -1.4217e-01,  1.7914e-01, -1.1166e-01,\n",
      "         3.1592e-02, -1.4925e-01, -3.2459e-01, -6.5207e-01, -1.2767e-01,\n",
      "        -5.2192e-01, -3.2428e-01,  1.8668e-02, -7.1607e-01, -4.8559e-01,\n",
      "        -3.2309e-01, -8.1328e-01, -1.1230e-01, -4.1290e-01, -8.4478e-01,\n",
      "        -1.8976e-01, -5.9730e-01], device='cuda:0')), ('conv3.lins.0.weight', tensor([[-9.5962e-01, -4.4032e-02, -5.5935e-02,  ..., -1.1147e-02,\n",
      "         -2.1068e-02,  5.0925e-01],\n",
      "        [-5.9301e-01,  7.1279e-02, -2.6381e-02,  ...,  4.6167e-02,\n",
      "         -2.7481e-02,  1.2014e+00],\n",
      "        [-4.5226e-04,  5.0659e-02, -4.7866e-03,  ...,  6.4714e-02,\n",
      "          2.7066e-03,  3.8086e-02],\n",
      "        ...,\n",
      "        [-9.4231e-01, -2.4315e-02,  6.5190e-02,  ...,  3.6772e-02,\n",
      "         -6.4539e-02,  1.6322e-02],\n",
      "        [-1.4556e-02,  1.9815e-02, -6.6471e-02,  ..., -4.3133e-02,\n",
      "         -1.2398e-02, -3.7300e-01],\n",
      "        [ 5.3800e-01, -8.1394e-02,  3.5226e-02,  ...,  4.7875e-02,\n",
      "         -6.1819e-02,  2.9028e-02]], device='cuda:0')), ('conv3.lins.1.weight', tensor([[-1.2804e-01, -1.0719e-02, -1.1788e-01,  ..., -5.0398e-02,\n",
      "         -2.4038e-02, -3.4945e-01],\n",
      "        [ 2.7761e-01, -3.1127e-02,  1.0007e-01,  ..., -4.4247e-02,\n",
      "          3.1475e-02,  5.3915e-01],\n",
      "        [-5.9690e-02, -6.1887e-03,  4.5015e-02,  ...,  9.3836e-03,\n",
      "          4.9875e-02,  1.0590e-01],\n",
      "        ...,\n",
      "        [ 1.7832e+00,  1.7934e-02,  1.0190e-02,  ..., -2.7964e-02,\n",
      "          3.8172e-02,  3.3764e-01],\n",
      "        [-3.3307e-01,  6.1816e-02, -1.1556e-03,  ...,  3.5964e-02,\n",
      "          3.8974e-02,  2.4211e-01],\n",
      "        [ 2.9610e-02, -3.3541e-02,  6.2713e-02,  ...,  3.1735e-02,\n",
      "          8.7622e-02,  2.9048e-03]], device='cuda:0')), ('conv3.lins.2.weight', tensor([[ 0.1341, -0.0709,  0.0381,  ..., -0.0503,  0.0244,  0.6279],\n",
      "        [ 1.2333, -0.0035, -0.0805,  ...,  0.0178, -0.0282, -1.3915],\n",
      "        [ 0.0573,  0.0233,  0.0328,  ..., -0.0709, -0.0401, -0.0764],\n",
      "        ...,\n",
      "        [-0.0919, -0.0154, -0.0310,  ...,  0.0301, -0.0518,  0.8445],\n",
      "        [ 0.4988,  0.0576, -0.0485,  ...,  0.0773,  0.0759, -0.1883],\n",
      "        [-0.3066,  0.0111,  0.0408,  ..., -0.0682, -0.0232, -0.2112]],\n",
      "       device='cuda:0')), ('conv3.lins.3.weight', tensor([[-0.0855, -0.0672,  0.0531,  ...,  0.0144,  0.0332, -0.1782],\n",
      "        [-0.8582,  0.0595,  0.1088,  ...,  0.0234, -0.0347, -0.3199],\n",
      "        [-0.0302,  0.0332,  0.0332,  ..., -0.0453,  0.0230,  0.0088],\n",
      "        ...,\n",
      "        [-0.2216, -0.0044,  0.0462,  ...,  0.0321,  0.0565, -0.5618],\n",
      "        [-0.2911,  0.0520, -0.0033,  ...,  0.0611,  0.0453,  0.1358],\n",
      "        [-0.1168,  0.0776,  0.0028,  ...,  0.0683,  0.0297,  0.3965]],\n",
      "       device='cuda:0')), ('conv3.lins.4.weight', tensor([[ 0.1305, -0.0529,  0.0283,  ...,  0.0389,  0.0225,  0.0970],\n",
      "        [-0.0351, -0.0195,  0.0193,  ..., -0.0075,  0.0313,  0.2106],\n",
      "        [ 0.0615,  0.0398, -0.0526,  ...,  0.0152,  0.0497,  0.0312],\n",
      "        ...,\n",
      "        [ 1.1240,  0.0796,  0.0120,  ...,  0.0391, -0.0565, -0.9726],\n",
      "        [ 0.5907,  0.0277,  0.0236,  ...,  0.0464, -0.0599, -0.7129],\n",
      "        [-0.0779,  0.0647, -0.0680,  ...,  0.0653,  0.0319,  0.3422]],\n",
      "       device='cuda:0')), ('lin.weight', tensor([[-2.7996e+00, -3.7501e+00, -2.2439e-03,  ..., -1.8903e+00,\n",
      "         -8.7320e-01, -1.4444e-01],\n",
      "        [-3.1613e+00, -8.8638e-01, -1.3581e-02,  ...,  1.1234e+00,\n",
      "         -5.0744e-01, -2.1707e-01],\n",
      "        [-2.1963e-01, -1.5200e-01, -3.5384e-02,  ...,  1.9989e-01,\n",
      "         -1.9433e+00,  2.6148e-01],\n",
      "        ...,\n",
      "        [ 1.9997e-01, -2.9898e+00, -2.8748e-02,  ..., -1.0203e+00,\n",
      "         -3.6081e-01, -2.3399e-01],\n",
      "        [ 6.0188e-01, -4.1295e+00,  5.1479e-03,  ...,  2.6927e-01,\n",
      "         -3.1948e-01,  1.6993e-01],\n",
      "        [-2.3328e+00,  2.0842e-01, -2.1515e-02,  ...,  4.4828e-01,\n",
      "         -6.7373e-01,  5.8536e-01]], device='cuda:0')), ('lin.bias', tensor([-2.0939e+00, -1.4743e+00, -1.1552e+00, -9.0380e-01,  2.0235e-01,\n",
      "        -1.8053e-01, -1.7016e-01,  7.1181e-01,  7.1074e-01, -1.1907e-01,\n",
      "         1.7749e-01, -6.7678e-01, -7.4981e-03,  3.2373e-01,  2.1521e-01,\n",
      "        -8.2669e-02, -4.0499e-01,  1.6019e-01, -7.0782e-02, -5.0576e-01,\n",
      "         2.6957e-01,  2.2675e-01, -1.7517e-01, -3.8439e-01, -1.3496e-01,\n",
      "        -1.4975e-01, -1.0005e-02, -3.5916e-01, -6.7433e-01, -4.0155e-02,\n",
      "        -4.1129e-01, -2.1019e-01, -2.4728e-01, -1.7885e-01, -1.4685e-01,\n",
      "        -1.0510e-01,  3.5100e-01, -4.7304e-01, -3.5641e-02,  7.1183e-01,\n",
      "        -6.5966e-01, -3.0353e-01,  3.3910e-01, -7.0884e-01, -4.5418e-01,\n",
      "         8.2496e-02,  1.7450e-01, -8.2249e-01, -1.0171e+00, -4.8688e-01,\n",
      "        -3.0681e-01, -6.7158e-01, -2.6061e-01, -3.4778e-02, -5.2175e-01,\n",
      "        -4.5041e-01, -5.5248e-01, -6.4294e-01, -3.7702e-01, -1.1628e+00,\n",
      "        -8.2666e-01,  2.6760e-02, -4.6011e-01, -4.1079e-02, -5.7012e-01,\n",
      "        -2.9902e-01, -7.6343e-01, -4.8671e-01, -1.0557e+00, -3.8747e-01,\n",
      "        -1.9832e-01, -4.6651e-01,  6.0301e-01, -5.3477e-01, -4.6884e-01,\n",
      "         2.8972e-01,  2.5888e-01, -2.8000e-01,  4.5168e-01, -2.7689e-02,\n",
      "        -5.7914e-01, -4.3068e-01, -6.2205e-01,  1.6272e+00,  1.3706e+00,\n",
      "         7.0609e-01,  3.9643e-01,  1.9687e-01, -4.3222e-01,  2.0550e+00,\n",
      "         2.8147e+00,  1.0107e-01,  1.2802e-01, -5.1205e-02,  2.3376e-02,\n",
      "         9.2055e-02, -4.8731e-01, -4.2030e-01, -5.5640e-01, -6.3892e-01,\n",
      "        -1.0403e+00, -1.2798e+00, -5.3305e-01,  4.8558e-01, -1.3012e-01,\n",
      "        -6.1239e-01,  8.0381e-01,  5.2697e-01, -2.2969e-01, -1.8337e-01,\n",
      "        -1.0624e-01, -1.7060e-01, -1.6839e-01,  6.1006e-01,  1.3487e+00,\n",
      "         7.5389e-01,  1.5552e-01,  3.1780e-01, -1.7015e-01, -4.9765e-01,\n",
      "        -4.6832e-01,  9.4026e-02, -5.1986e-01, -1.0213e+00, -2.7406e-01,\n",
      "        -4.0454e-01, -2.7148e-01,  2.1593e-01,  2.8680e-01, -6.8375e-03,\n",
      "        -8.1461e-03, -7.3925e-01, -6.9604e-02, -3.1769e-02,  2.8471e+00,\n",
      "         1.4819e+00, -4.7632e-01, -1.1584e+00, -6.2172e-01, -3.3350e-01,\n",
      "        -4.0491e-01, -4.2290e-01,  8.1553e-02, -9.3127e-03, -6.5147e-01,\n",
      "        -7.0315e-01,  4.8792e-02,  3.1454e-01,  4.9180e-01,  5.3074e-01,\n",
      "         7.8456e-01,  6.0029e-01, -2.1611e-01, -1.7815e-01, -6.7732e-01,\n",
      "        -7.6361e-01, -6.7629e-01, -1.3561e-01, -2.4970e-01,  1.1837e+00,\n",
      "         2.5536e-01, -1.7226e-01, -9.4571e-02, -5.1923e-01, -7.7544e-01,\n",
      "        -7.2209e-01, -1.3931e-03,  8.9796e-02,  4.3158e-01, -1.2235e-01,\n",
      "        -2.4902e-01,  1.0959e-04, -5.7651e-01, -1.7113e-01, -4.2062e-01,\n",
      "         2.0653e-01, -3.4772e-01, -4.7029e-01,  3.6278e-01,  4.9563e-02,\n",
      "        -6.1923e-02, -8.6412e-01, -8.8620e-01, -6.2714e-02, -1.4356e-01,\n",
      "        -4.3555e-01,  8.1178e-01], device='cuda:0'))])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1200x1200 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#AUC Metric for PyTorch Models\n",
    "#from torcheval.metrics.aggregation.auc import AUC\n",
    "#metric = AUC()\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn import Softmax\n",
    "\n",
    "Soft_list = np.array([[]]).reshape((0,187))\n",
    "values_list = np.array([])\n",
    "pred_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "   model.eval()\n",
    "   model.to(device)\n",
    "   for data in test_loader:\n",
    "       data = data.to(device)\n",
    "       out = model(data.x, data.edge_index, data.batch)\n",
    "       pred= out.argmax(dim=1)\n",
    "       softmax = Softmax()\n",
    "       softout = softmax(out)\n",
    "       y = data.y\n",
    "       Soft_list = np.concatenate((Soft_list, softout.cpu().numpy()), axis=0)\n",
    "       pred_list = np.concatenate((pred_list, pred.cpu().numpy()), axis=0)\n",
    "       # print(Soft_list)\n",
    "       values_list= np.concatenate((values_list, y.cpu().numpy()), axis=None)\n",
    "  # print(out)\n",
    "  # print('*'*888)\n",
    "  # print(softout)\n",
    "  # print('*'*888)\n",
    "print(Soft_list)\n",
    "print(Soft_list.shape)\n",
    "print(values_list)\n",
    "print(values_list.shape)\n",
    "print(pred_list)\n",
    "print(pred_list.shape)\n",
    "\n",
    "print(\"The Area Under ROC Curve (AUC) =\", roc_auc_score(values_list, Soft_list, multi_class='ovr'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score =\", f1_score(values_list, pred_list, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Testing Accuracy =\",accuracy_score(values_list, pred_list))\n",
    "\n",
    "#Generates Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(values_list, pred_list)#, labels=classes\n",
    "print(cm)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# plt.imshow(cm, interpolation = 'none')\n",
    "plt.rcParams['figure.figsize'] = (12, 12)\n",
    "# plt.figure(figsize=(25, 25))\n",
    "plt.imshow(cm, interpolation = 'none', cmap = 'binary')\n",
    "# plt.imshow(cm, interpolation = 'none', cmap = 'cividis')\n",
    "\n",
    "plt.colorbar()\n",
    "\n",
    "# plt.title('Matrix Visualization')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "\n",
    "plt.plot()\n",
    "\n",
    "\n",
    "#Saves the model state\n",
    "print(model.state_dict())\n",
    "torch.save(model.state_dict(), 'model_342_finishtraininglater.pt')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

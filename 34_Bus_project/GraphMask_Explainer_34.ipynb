{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3sNImT5swwku",
    "outputId": "4fd79a70-fc37-4259-ba70-c16f11aeef8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch_geometric\n",
      "  Downloading torch_geometric-2.5.3-py3-none-any.whl.metadata (64 kB)\n",
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.5)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.26.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.13.1)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2024.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.10.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.32.3)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.3.2)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.7.4)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.5.0)\n",
      "Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torch_geometric\n",
      "Successfully installed torch_geometric-2.5.3\n",
      "Collecting captum\n",
      "  Downloading captum-0.7.0-py3-none-any.whl.metadata (26 kB)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.26.4)\n",
      "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.4.0+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from captum) (4.66.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2024.6.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n",
      "Downloading captum-0.7.0-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: captum\n",
      "Successfully installed captum-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_geometric\n",
    "!pip install captum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "1b22pHi7x9NL",
    "outputId": "ba9132bb-13af-4cdf-dd28-11b278594f7c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-2c596184-2941-4e68-8f0c-225c35cd5325\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-2c596184-2941-4e68-8f0c-225c35cd5325\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving numpy_data.zip to numpy_data.zip\n",
      "Saving model_34.pt to model_34.pt\n",
      "Archive:  numpy_data.zip\n",
      "  inflating: AdjacencyMatrix.npy     \n",
      "  inflating: new_data.npy            \n",
      "  inflating: new_labels.npy          \n",
      "  inflating: NodeIndex.npy           \n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "#jupytext --to ipynb Comparative_Cheb_GCN.py\n",
    "# Don't forget to upload model as well\n",
    "!unzip numpy_data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Fo9FGRNyOQu",
    "outputId": "2634c68a-30c3-4552-b196-bbed32f52911"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): ChebConv(6, 256, K=3, normalization=sym)\n",
      "  (conv2): ChebConv(256, 256, K=4, normalization=sym)\n",
      "  (conv3): ChebConv(256, 256, K=5, normalization=sym)\n",
      "  (lin): Linear(in_features=256, out_features=24, bias=True)\n",
      ")\n",
      "/content/model_34.pt\n",
      "ChebConv(256, 256, K=5, normalization=sym)\n",
      "OrderedDict([('conv1.bias', tensor([ 6.2356e-02, -2.1974e-01, -4.6219e-02, -1.7084e-01, -2.6293e-01,\n",
      "        -2.6260e-01, -8.5107e-02, -1.3529e-01, -7.8290e-02, -2.3142e-01,\n",
      "        -3.1671e-01, -1.5598e-01,  8.2232e-02,  1.0987e-01, -2.9260e-01,\n",
      "        -2.3244e-01, -1.3441e-01, -2.2913e-01, -2.0324e-01, -1.9280e-01,\n",
      "        -9.8404e-02, -1.4911e-03,  3.1328e-02,  8.7063e-02,  3.2462e-02,\n",
      "        -8.5882e-02,  1.2128e-01, -3.3120e-02,  2.4983e-02, -6.0169e-02,\n",
      "        -3.1656e-01, -3.1808e-01, -2.3165e-01,  1.0383e-01, -2.1685e-01,\n",
      "        -3.6769e-01, -3.2775e-01,  1.6007e-01,  2.7386e-01,  6.3639e-04,\n",
      "         6.4094e-02,  1.3523e-02, -2.7450e-01, -5.7802e-02, -2.0482e-01,\n",
      "        -1.1098e-01, -7.0231e-02, -6.3954e-02,  6.9434e-02, -2.2122e-01,\n",
      "         2.2109e-02, -8.5038e-02,  4.8618e-02,  7.6985e-02, -4.3317e-02,\n",
      "        -2.8630e-01, -3.5957e-01, -2.9967e-01, -2.2773e-01, -2.5614e-01,\n",
      "        -1.5607e-01, -3.1174e-01,  1.9149e-02, -5.8772e-02, -3.6648e-01,\n",
      "         3.2992e-02,  1.1905e-01, -1.0571e-01, -1.9280e-01, -1.3025e-01,\n",
      "        -2.0790e-02,  1.5906e-01, -4.2416e-02, -1.9914e-01,  6.6816e-02,\n",
      "        -6.3359e-02, -4.7074e-01, -1.5159e-02, -1.4022e-01,  5.7422e-02,\n",
      "        -1.2797e-01, -6.9457e-02, -9.8404e-03,  5.2183e-02, -1.8985e-01,\n",
      "        -5.1061e-01,  8.5822e-03,  2.0259e-02, -7.7145e-02,  1.3776e-01,\n",
      "         7.5367e-02, -1.6612e-01, -2.2590e-02, -1.3270e-01, -2.7397e-01,\n",
      "        -1.6879e-01, -7.1043e-02,  1.0161e-01, -1.4881e-01, -2.0571e-01,\n",
      "        -1.9379e-01,  6.0242e-02,  8.0732e-03,  6.3409e-02,  9.4185e-02,\n",
      "        -3.6125e-01,  2.2888e-02,  1.0707e-01,  3.8874e-02,  7.8683e-02,\n",
      "         2.9536e-03, -2.8461e-01,  5.7248e-02, -2.6085e-02, -2.6750e-01,\n",
      "        -2.4618e-01,  3.4029e-03, -2.2914e-01, -9.2907e-03, -2.1978e-01,\n",
      "         5.6318e-02, -8.3183e-02, -8.4186e-02, -3.9547e-01, -2.1443e-01,\n",
      "         1.4138e-01, -9.5643e-02, -3.1328e-01,  1.3767e-01, -2.0314e-01,\n",
      "         2.8979e-01, -4.2295e-02,  2.2342e-01,  1.6947e-01, -1.3841e-01,\n",
      "        -1.2787e-02, -4.1337e-01, -2.8005e-01, -3.2205e-02,  1.2167e-01,\n",
      "        -5.6731e-03,  8.5858e-02,  2.0489e-03, -2.8279e-01,  1.1976e-01,\n",
      "        -1.3916e-01, -3.3144e-02, -1.0607e-01, -7.9796e-02, -3.2388e-01,\n",
      "        -3.8482e-01, -1.5920e-01,  2.7629e-02,  1.0369e-01, -3.6351e-01,\n",
      "        -1.8542e-01,  8.1027e-03,  3.7201e-02,  1.3003e-01, -2.0426e-03,\n",
      "         1.4442e-01, -1.9617e-01, -2.6541e-01,  3.4265e-02, -9.7128e-02,\n",
      "        -2.8160e-01, -2.5457e-01,  3.3014e-02, -2.0563e-01,  7.3977e-02,\n",
      "        -7.8606e-02,  8.1844e-02, -7.5490e-02,  4.1513e-02, -2.4708e-01,\n",
      "        -1.4486e-01, -1.9621e-01, -3.2766e-01, -2.7505e-01, -3.2798e-02,\n",
      "        -3.6092e-01, -1.5089e-01,  7.7087e-02, -1.4843e-01, -4.6642e-01,\n",
      "        -1.0745e-01, -2.8963e-01,  1.3123e-01, -2.7176e-01, -1.4039e-01,\n",
      "         1.9187e-02,  5.9398e-02,  1.0150e-01, -1.0567e-01, -2.4896e-01,\n",
      "         1.0667e-01, -2.2239e-01,  6.4944e-02, -4.7782e-01, -2.3200e-01,\n",
      "         2.2199e-02, -8.8190e-02, -4.1976e-01, -4.7851e-02, -3.2382e-01,\n",
      "         4.2849e-02,  3.9722e-02,  1.0520e-01, -4.7053e-02, -7.8541e-02,\n",
      "         6.5023e-02, -1.7359e-01, -2.3735e-01,  4.6819e-02, -1.5885e-01,\n",
      "        -8.0666e-02, -2.1603e-01, -1.3179e-01,  8.1806e-02,  1.5263e-01,\n",
      "        -1.4736e-01, -2.7287e-01, -4.6941e-02,  5.7955e-02, -5.1442e-02,\n",
      "        -3.8572e-01, -3.5024e-01, -2.3612e-01,  9.2791e-02, -3.4094e-02,\n",
      "         1.1702e-01, -1.9333e-01,  8.1222e-02,  8.5880e-02,  7.2900e-02,\n",
      "        -1.2619e-01, -2.1354e-01,  2.7556e-04,  2.0331e-01,  1.8978e-01,\n",
      "        -1.6265e-01, -1.7066e-01,  1.5428e-02,  3.4326e-02, -1.7752e-01,\n",
      "        -1.8815e-02,  1.5281e-01, -1.1813e-01, -1.7178e-02, -2.0185e-01,\n",
      "         1.6206e-01, -1.6329e-01,  8.7048e-02,  5.3786e-02, -2.6274e-01,\n",
      "        -1.8866e-01])), ('conv1.lins.0.weight', tensor([[ 0.1510,  0.3358, -3.8396,  0.3849,  0.0725, -0.1035],\n",
      "        [-0.0809,  0.2080,  0.0530,  0.0629,  0.0185, -0.1081],\n",
      "        [ 0.1117,  0.6946, -0.3851, -1.5782,  0.1810, -0.8685],\n",
      "        ...,\n",
      "        [-0.0237, -0.3592,  0.0980,  0.5236, -4.6292, -3.5244],\n",
      "        [-0.0077,  0.3834,  0.0627,  0.9099,  0.0230,  1.0941],\n",
      "        [ 0.0940,  0.8627,  0.0826,  0.2770, -0.3272, -0.2555]])), ('conv1.lins.1.weight', tensor([[-7.3603e-02, -7.8032e-01,  3.0315e+00, -4.0864e-01, -1.0114e-01,\n",
      "          1.6019e-01],\n",
      "        [ 3.6538e-03, -6.4342e-03,  1.3706e-01, -1.9447e-01, -2.0852e-02,\n",
      "          2.6610e-02],\n",
      "        [-1.2893e-01, -7.9885e-01,  4.9950e-01,  1.2288e+00, -2.9762e-01,\n",
      "          6.0901e-01],\n",
      "        ...,\n",
      "        [-1.2961e-01,  2.1959e-01, -4.7980e-02, -1.4781e-01,  3.3989e+00,\n",
      "          3.9870e+00],\n",
      "        [-6.2286e-04, -4.7278e-01, -7.6573e-02, -7.2124e-01, -5.6356e-02,\n",
      "         -8.0469e-01],\n",
      "        [-8.1451e-02, -6.8374e-01, -2.2437e-02, -6.9606e-01,  1.1684e-02,\n",
      "          1.1780e-01]])), ('conv1.lins.2.weight', tensor([[ 6.4133e-02,  3.9685e-01, -1.2379e+00,  2.8836e-01, -1.4192e-01,\n",
      "          6.9615e-02],\n",
      "        [-1.4020e-01, -6.3574e-02,  1.1012e-03, -3.3811e-01, -4.5181e-02,\n",
      "          1.7114e-01],\n",
      "        [ 2.4497e-01,  8.0290e-01, -3.7024e-01, -8.9914e-01,  1.7652e-01,\n",
      "         -1.1999e+00],\n",
      "        ...,\n",
      "        [ 1.4166e-01, -3.3641e-01, -1.2186e-01, -3.7719e-01, -9.5969e-01,\n",
      "         -3.5764e+00],\n",
      "        [ 3.7658e-02,  4.1611e-01,  2.7165e-02,  8.8000e-01, -7.3606e-02,\n",
      "          6.2175e-01],\n",
      "        [-2.0161e-01,  1.9376e-01, -8.1584e-02,  5.6365e-01,  4.7853e-03,\n",
      "          1.3714e-01]])), ('conv2.bias', tensor([ 0.1835, -0.1311, -0.0535,  0.2531, -0.3053, -0.1655,  0.2731, -0.3838,\n",
      "        -0.3752, -0.1034, -0.2429, -0.1901, -0.0777, -0.0677, -0.3485,  0.1946,\n",
      "        -0.5374, -0.0492,  0.2142, -0.0787, -0.0361, -0.1214,  0.0960, -0.5251,\n",
      "         0.1642, -0.1454,  0.0301, -0.2942, -0.0236, -0.0683, -0.0340,  0.3360,\n",
      "        -0.2121, -0.0200, -0.2135, -0.0919, -0.0651, -0.2210,  0.0781,  0.2862,\n",
      "        -0.0698, -0.2661, -0.4469, -0.3373, -0.0553, -0.1275, -0.0503, -0.0449,\n",
      "        -0.0400,  0.4199, -0.3099, -0.4312, -0.2944, -0.0866, -0.4734, -0.2450,\n",
      "         0.0921, -0.0796, -0.1812, -0.0379, -0.0483,  0.1339, -0.0704, -0.0148,\n",
      "        -0.0817, -0.0431, -0.3929, -0.3900,  0.1888, -0.4867, -0.1600, -0.2535,\n",
      "        -0.2815, -0.3845, -0.2986,  0.1284,  0.0218, -0.2732, -0.0867, -0.1955,\n",
      "        -0.3435, -0.2995, -0.1320, -0.2710,  0.0538, -0.0806, -0.0337, -0.3648,\n",
      "        -0.3799, -0.3423,  0.2722,  0.1160,  0.2134, -0.2558, -0.0415,  0.2752,\n",
      "         0.5428, -0.2731, -0.1487,  0.0243, -0.0995, -0.1936,  0.1786, -0.0711,\n",
      "        -0.2267, -0.1513, -0.3383, -0.3210,  0.0782, -0.1183, -0.4843, -0.1322,\n",
      "        -0.0639, -0.1666, -0.1088,  0.0580,  0.2008,  0.2986,  0.1927, -0.1751,\n",
      "         0.1651, -0.3039,  0.2086,  0.4556, -0.1955,  0.4207, -0.4389,  0.0372,\n",
      "         0.1585, -0.2254, -0.3493, -0.1342, -0.6117, -0.3772, -0.3203, -0.1884,\n",
      "        -0.0258, -0.2597, -0.1156,  0.0479, -0.1112,  0.1286, -0.0061,  0.1194,\n",
      "        -0.0414,  0.0492, -0.1664, -0.0223, -0.3265, -0.0444, -0.2147,  0.1383,\n",
      "         0.0892, -0.3928, -0.3714, -0.2710,  0.3431,  0.0136, -0.1542, -0.3079,\n",
      "         0.4526, -0.4188, -0.3754, -0.1712,  0.0888, -0.2722, -0.1846,  0.1651,\n",
      "        -0.0290, -0.2884, -0.1053,  0.2377, -0.1681,  0.3292, -0.0475,  0.2275,\n",
      "         0.1155, -0.1745, -0.1289,  0.4776,  0.1490,  0.1337, -0.2662,  0.1254,\n",
      "        -0.0784, -0.4187, -0.0395,  0.1235, -0.2767,  0.2685,  0.0111,  0.2924,\n",
      "        -0.0555, -0.1473, -0.0485,  0.0760, -0.3921,  0.0577, -0.5782,  0.0880,\n",
      "         0.2179, -0.1905, -0.0750,  0.2168, -0.2533, -0.1631, -0.1696,  0.1792,\n",
      "        -0.1170,  0.1352,  0.1290,  0.2487, -0.0792, -0.2381, -0.0673, -0.2230,\n",
      "         0.0147, -0.0963, -0.1318,  0.3591,  0.0697, -0.0662, -0.0409, -0.0448,\n",
      "        -0.2419, -0.0841, -0.3444, -0.0751, -0.0394,  0.1089,  0.1085, -0.4035,\n",
      "        -0.3302, -0.4248, -0.0446, -0.1544,  0.1182, -0.2886, -0.4145, -0.0416,\n",
      "        -0.3885,  0.4979, -0.1956, -0.0620, -0.0583, -0.2937, -0.3309, -0.3302,\n",
      "         0.3468, -0.3725, -0.2540, -0.1361, -0.0407, -0.1620,  0.4478, -0.4455])), ('conv2.lins.0.weight', tensor([[ 0.7044, -0.0979, -1.0667,  ..., -0.9464, -0.0098,  0.1151],\n",
      "        [-0.0928,  0.0663, -0.7690,  ..., -0.1171,  0.7617, -0.3050],\n",
      "        [ 0.0420,  0.0550, -0.0585,  ..., -0.0014, -0.0283,  0.0963],\n",
      "        ...,\n",
      "        [ 0.0460,  0.1173,  0.2166,  ..., -0.6145,  0.1709, -0.2688],\n",
      "        [-0.4699,  0.0940,  0.4618,  ..., -0.3967,  0.1709, -0.4499],\n",
      "        [ 0.4366, -0.1580, -0.0830,  ..., -0.0786,  0.2463, -0.0464]])), ('conv2.lins.1.weight', tensor([[-0.5327,  0.1613,  0.7498,  ...,  1.2596, -0.0901, -0.1508],\n",
      "        [ 0.0664, -0.0933,  1.4193,  ...,  1.1432, -0.8631,  0.5735],\n",
      "        [ 0.0689, -0.0824,  0.0300,  ...,  0.0224, -0.0287, -0.0342],\n",
      "        ...,\n",
      "        [-0.0090, -0.1682, -0.1640,  ...,  0.6833, -0.0508,  0.4958],\n",
      "        [ 0.3691, -0.1188, -0.3800,  ...,  0.2443, -0.1084,  0.2155],\n",
      "        [-0.2631,  0.2915,  0.0168,  ...,  0.5045, -0.1374,  0.2668]])), ('conv2.lins.2.weight', tensor([[ 3.3606e-01, -1.5803e-01, -1.2572e-01,  ..., -1.5611e+00,\n",
      "          2.3520e-01,  2.0284e-01],\n",
      "        [-2.9124e-01,  5.0617e-02, -2.3335e+00,  ..., -1.2669e+00,\n",
      "          6.1126e-01, -4.2973e-01],\n",
      "        [ 1.3245e-02, -4.9892e-02,  1.8566e-03,  ...,  1.5497e-02,\n",
      "          2.8620e-02,  5.3050e-02],\n",
      "        ...,\n",
      "        [-4.7300e-02, -1.4475e-01,  2.7960e-01,  ...,  3.6052e-02,\n",
      "         -4.0662e-02, -4.9738e-01],\n",
      "        [-2.7490e-01,  3.9154e-02, -1.2168e-01,  ..., -4.9314e-01,\n",
      "          9.4351e-02, -7.8650e-01],\n",
      "        [ 3.7607e-01, -6.8904e-02, -5.8678e-02,  ..., -3.3004e-01,\n",
      "          3.0240e-01, -3.5635e-02]])), ('conv2.lins.3.weight', tensor([[-0.4863,  0.0031, -0.1574,  ..., -0.3411, -0.4701, -0.0110],\n",
      "        [ 0.1449,  0.0323,  1.5836,  ...,  0.6112, -0.5388,  0.5403],\n",
      "        [ 0.0242,  0.0947, -0.0292,  ..., -0.0795,  0.0302, -0.0759],\n",
      "        ...,\n",
      "        [ 0.3829, -0.0251, -0.2665,  ...,  0.4326, -0.0545,  0.4551],\n",
      "        [ 1.6823,  0.1466,  0.4619,  ...,  0.5758, -0.1709,  1.0452],\n",
      "        [ 0.2219,  0.0519,  0.1800,  ...,  0.7383, -0.5781, -0.0437]])), ('conv3.bias', tensor([-3.6720e-02, -9.8716e-03, -2.1727e-01, -5.3753e-02,  3.0794e-01,\n",
      "        -4.6197e-02, -5.3082e-02,  8.3383e-02, -1.5587e-01,  6.4955e-02,\n",
      "         9.1949e-03, -1.6723e-01, -5.5027e-02,  3.4665e-01,  2.0517e-01,\n",
      "         4.3343e-02, -8.8480e-02,  2.2995e-03, -3.9648e-02, -4.9497e-02,\n",
      "        -4.4395e-02,  6.1865e-02, -4.9370e-01, -2.3546e-01, -1.2077e-01,\n",
      "        -4.9135e-02, -3.0599e-02, -3.2471e-01, -3.1754e-01, -6.4887e-02,\n",
      "        -9.0505e-02,  3.1624e-01, -5.1886e-02, -3.1662e-01, -3.5594e-01,\n",
      "         2.9850e-01, -1.2599e-01, -5.0971e-01,  1.1665e-01,  2.6611e-01,\n",
      "        -2.2015e-02, -2.3887e-01,  1.0434e-01, -7.9436e-02, -4.2927e-01,\n",
      "        -4.3064e-02, -7.6206e-02, -2.2234e-02, -3.6046e-02, -2.8288e-01,\n",
      "        -1.1037e-02, -1.5990e-01, -1.8653e-01, -1.0626e-01, -3.8247e-02,\n",
      "         5.4197e-02, -7.4591e-01,  1.5922e-01, -3.2033e-01,  1.5753e-01,\n",
      "        -1.2052e-01, -2.3500e-01, -3.1993e-01, -7.4109e-02,  1.3759e-02,\n",
      "         7.8799e-02, -5.4149e-02, -1.6859e-01, -3.8120e-01,  2.2829e-02,\n",
      "        -7.8904e-02,  5.1262e-02, -6.8076e-02, -1.8846e-01, -1.3228e-01,\n",
      "         3.2828e-01, -5.4944e-02, -1.1820e-01, -2.8367e-03, -1.1592e-01,\n",
      "         3.5043e-01, -2.4922e-01, -5.3948e-02, -4.0234e-01, -6.0760e-01,\n",
      "        -4.2072e-01,  5.2675e-02, -4.3006e-01, -3.3483e-01, -2.0742e-01,\n",
      "         1.0394e-01, -6.7346e-02, -1.2538e-01, -9.0705e-02, -3.1383e-01,\n",
      "        -1.6299e-01, -2.4147e-01, -1.7489e-01, -6.5709e-02, -1.4396e-01,\n",
      "        -3.9101e-01,  1.0769e-01, -3.1768e-01, -1.6187e-01, -4.4744e-02,\n",
      "        -2.0234e-01, -1.2027e-01,  4.0553e-02, -5.6034e-01, -4.3250e-01,\n",
      "        -8.4847e-02,  1.8486e-01, -1.0536e-01,  3.6634e-01, -5.1036e-02,\n",
      "        -3.7907e-01,  7.7791e-02, -4.4954e-02, -1.4257e-01, -1.9797e-01,\n",
      "        -2.8305e-02, -2.6889e-01, -2.2541e-01,  3.6188e-01,  2.2442e-01,\n",
      "        -8.2529e-02,  4.1905e-02,  1.2651e-02, -3.4368e-02, -1.6592e-01,\n",
      "        -1.8834e-01,  6.5413e-02, -4.3607e-02,  5.5409e-02, -2.8576e-01,\n",
      "        -2.6597e-01,  2.4447e-02, -3.5555e-02,  6.2591e-02, -4.6133e-02,\n",
      "        -9.2751e-02, -1.8825e-01,  1.4546e-02, -3.0869e-01, -3.5810e-01,\n",
      "        -2.8251e-01, -1.0149e-01, -1.0897e-01, -1.7727e-01, -1.7374e-01,\n",
      "        -2.8741e-02,  2.9033e-01, -7.4852e-02,  2.4215e-02,  5.4461e-02,\n",
      "         3.7303e-01, -1.7880e-01, -2.6543e-02, -1.2446e-01,  1.8752e-01,\n",
      "        -1.0654e-01, -1.9094e-01,  1.4389e-01, -1.6140e-02, -4.1130e-02,\n",
      "        -9.6596e-02, -1.4723e-01, -4.1811e-01, -5.8782e-01,  4.5719e-04,\n",
      "        -3.7268e-02, -1.6410e-01, -7.6868e-02, -1.3058e-01, -4.4918e-01,\n",
      "        -2.2672e-01, -4.1643e-03,  3.9620e-03, -3.2461e-02, -4.0870e-02,\n",
      "        -1.9758e-01, -2.2028e-01, -3.6715e-02, -3.9486e-01, -3.5364e-02,\n",
      "        -3.1885e-01,  2.7409e-01, -3.0415e-02,  1.3919e-01, -1.0006e-03,\n",
      "        -6.7228e-02, -1.4888e-02, -8.5544e-02, -4.3797e-02,  2.5067e-01,\n",
      "        -2.9965e-01, -9.2550e-02, -6.0890e-02, -2.9221e-01, -3.1843e-01,\n",
      "        -7.0763e-02, -2.4764e-01, -6.3292e-02,  7.8573e-02, -1.8884e-01,\n",
      "         1.3226e-01, -3.4620e-01,  8.2477e-02, -2.4267e-01, -4.2831e-01,\n",
      "        -1.0994e-01, -5.8758e-01, -1.1391e-01,  2.6791e-01, -2.5883e-01,\n",
      "        -3.3694e-01,  4.0025e-02, -4.2297e-02, -8.5525e-02, -4.4629e-01,\n",
      "        -4.4721e-01, -1.1825e-01, -1.1381e-01, -6.4541e-02, -6.6058e-02,\n",
      "         2.9922e-01, -2.1282e-01, -3.1922e-01, -8.8790e-02, -8.1306e-02,\n",
      "        -2.1832e-01, -3.6414e-02,  1.4091e-01, -4.3148e-01, -1.8874e-01,\n",
      "        -2.3839e-01, -6.0549e-04, -2.2725e-01,  2.4569e-01,  5.2762e-02,\n",
      "        -3.9677e-01,  6.4471e-02, -1.2411e-01,  5.1674e-01, -3.7647e-02,\n",
      "        -1.5844e-02, -3.4426e-01, -9.0046e-02,  1.5640e-02, -1.0563e-02,\n",
      "        -1.1938e-01, -2.0165e-01, -6.0575e-02, -1.0123e-01, -6.3267e-01,\n",
      "        -3.9134e-02])), ('conv3.lins.0.weight', tensor([[-0.0070, -0.0530,  0.0842,  ...,  0.0536, -0.0851, -0.0336],\n",
      "        [ 0.0461, -0.8678, -0.0396,  ...,  0.0574, -0.7613, -0.6911],\n",
      "        [ 0.1476, -0.0819, -0.0889,  ..., -0.0232, -0.1210, -0.1310],\n",
      "        ...,\n",
      "        [ 0.3822, -0.1337,  0.0432,  ..., -0.2596, -0.1999, -0.0399],\n",
      "        [ 0.2162, -0.2810,  0.0563,  ..., -0.1618, -1.1216,  0.1723],\n",
      "        [-0.0278, -0.0336, -0.1025,  ..., -0.0456, -0.0694,  0.0848]])), ('conv3.lins.1.weight', tensor([[ 0.0222, -0.0352,  0.0154,  ...,  0.0264,  0.1557, -0.0558],\n",
      "        [ 0.0032,  1.0040,  0.0560,  ..., -0.0750,  0.5962,  0.3113],\n",
      "        [-0.3115,  0.6580,  0.0473,  ..., -0.1268,  0.3939,  0.2164],\n",
      "        ...,\n",
      "        [-0.5093,  0.0875, -0.0136,  ...,  0.0747,  0.2338,  0.0271],\n",
      "        [-0.2626,  0.1905,  0.0390,  ...,  0.1734,  1.0135, -0.0897],\n",
      "        [-0.0027,  0.0534, -0.0538,  ...,  0.1292,  0.0789,  0.1160]])), ('conv3.lins.2.weight', tensor([[-0.0990, -0.0834, -0.0325,  ...,  0.0549, -0.1106,  0.0477],\n",
      "        [ 0.0541, -0.6221,  0.0205,  ...,  0.0265, -0.1565, -0.0911],\n",
      "        [ 0.2963, -0.8976,  0.0517,  ..., -0.0491, -0.7256, -0.1334],\n",
      "        ...,\n",
      "        [ 0.4245,  0.0182,  0.0255,  ...,  0.2198, -0.2028,  0.0129],\n",
      "        [ 0.1302, -0.2852,  0.0023,  ..., -0.0461, -0.6920,  0.1640],\n",
      "        [-0.0674,  0.0075, -0.0143,  ..., -0.0963, -0.0111, -0.0603]])), ('conv3.lins.3.weight', tensor([[-0.0181,  0.1039, -0.0972,  ...,  0.0506,  0.0262, -0.0193],\n",
      "        [ 0.1396,  0.2532,  0.0202,  ...,  0.2280, -0.7139, -0.0287],\n",
      "        [-0.3334,  0.3332,  0.0433,  ...,  0.4114,  0.9828, -0.0756],\n",
      "        ...,\n",
      "        [-0.4089, -0.3368, -0.0594,  ..., -0.1407,  0.3117, -0.0962],\n",
      "        [ 0.0442,  0.2084, -0.0468,  ...,  0.0209,  0.0714,  0.1075],\n",
      "        [ 0.0181,  0.0680,  0.0175,  ...,  0.1725, -0.0388,  0.1152]])), ('conv3.lins.4.weight', tensor([[ 0.0277,  0.0656,  0.0584,  ...,  0.0535, -0.0962,  0.0190],\n",
      "        [-0.1840, -0.0215, -0.0577,  ..., -0.5599,  1.6809,  0.2573],\n",
      "        [ 0.2755, -0.2999, -0.0837,  ..., -0.1955, -0.9847,  0.0722],\n",
      "        ...,\n",
      "        [ 0.2687,  0.3010,  0.0063,  ...,  0.1855, -0.2641,  0.3487],\n",
      "        [ 0.2507, -0.1252, -0.0422,  ...,  0.2628,  0.1812, -0.1596],\n",
      "        [-0.0270, -0.0857, -0.0722,  ..., -0.0094,  0.0351, -0.0508]])), ('lin.weight', tensor([[ 8.1560e-03,  2.2216e-01,  1.6329e-01,  ..., -1.1629e+00,\n",
      "         -5.1708e-01,  4.3249e-02],\n",
      "        [ 2.1028e-03,  4.5839e-01, -4.2015e-01,  ..., -4.0773e-01,\n",
      "          1.4627e-01,  2.8424e-02],\n",
      "        [-8.3354e-03,  7.6632e-02, -2.6907e+00,  ..., -4.9772e+00,\n",
      "         -5.4209e-01, -4.3997e-02],\n",
      "        ...,\n",
      "        [-4.8563e-02, -3.9468e-01, -1.5824e-01,  ..., -1.3722e-01,\n",
      "         -2.9032e+00, -9.8051e-02],\n",
      "        [-4.8194e-02, -1.3908e+00, -9.1118e-01,  ..., -5.2566e-01,\n",
      "         -1.1507e+00, -1.0766e-01],\n",
      "        [-5.8897e-02,  6.9461e-01,  1.6337e-01,  ...,  2.5317e-01,\n",
      "         -4.5710e-01, -8.0652e-03]])), ('lin.bias', tensor([ 0.6177,  0.4130, -0.2471, -0.5474, -1.0062,  0.0868, -0.7316, -0.3286,\n",
      "         0.2930, -0.6672,  0.2402, -0.3264, -0.4438, -0.6546, -0.4178,  0.2398,\n",
      "         0.5954, -0.1729,  0.2483, -1.0360,  0.0751, -0.1852, -0.8108,  0.0624]))])\n",
      "2.4.0+cu121\n",
      "/content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-39715202c0aa>:96: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_save_location, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "torch.float32\n",
      "5468\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5469\n",
      "12\n",
      "13\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5470\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5471\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5472\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5473\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5474\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5475\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5476\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5477\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5478\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5479\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5480\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5481\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5482\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5483\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5484\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5485\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5486\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5487\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5488\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5489\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5490\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5491\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5492\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5493\n",
      "12\n",
      "1\n",
      "4\n",
      "4\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5494\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5495\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5496\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5497\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5498\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5499\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5500\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5501\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5502\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5503\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5504\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5505\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5506\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5507\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5508\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5509\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5510\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5511\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5512\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5513\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5514\n",
      "12\n",
      "1\n",
      "18\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5515\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5516\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5517\n",
      "12\n",
      "6\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5518\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5519\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5520\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5521\n",
      "12\n",
      "1\n",
      "21\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5522\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5523\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5524\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5525\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5526\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5527\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5528\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5529\n",
      "12\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5530\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5531\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5532\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5533\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5534\n",
      "12\n",
      "1\n",
      "17\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5535\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5536\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5537\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5538\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5539\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5540\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5541\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5542\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5543\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5544\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5545\n",
      "12\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5546\n",
      "12\n",
      "13\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5547\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5548\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5549\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5550\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5551\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5552\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5553\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5554\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5555\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5556\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5557\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5558\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5559\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5560\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5561\n",
      "12\n",
      "1\n",
      "14\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5562\n",
      "12\n",
      "6\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5563\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5564\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5565\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5566\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5567\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5568\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5569\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5570\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5571\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5572\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5573\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5574\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5575\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5576\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5577\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5578\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5579\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5580\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5581\n",
      "12\n",
      "1\n",
      "18\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5582\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5583\n",
      "12\n",
      "13\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5584\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5585\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5586\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5587\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5588\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5589\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5590\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5591\n",
      "12\n",
      "13\n",
      "7\n",
      "7\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5592\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5593\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5594\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5595\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5596\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5597\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5598\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5599\n",
      "12\n",
      "1\n",
      "17\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5600\n",
      "0\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5601\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5602\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5603\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5604\n",
      "1\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5605\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5606\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5607\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5608\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5609\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5610\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5611\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5612\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5613\n",
      "12\n",
      "12\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5614\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5615\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5616\n",
      "12\n",
      "6\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5617\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5618\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5619\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5620\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5621\n",
      "12\n",
      "13\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5622\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5623\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5624\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5625\n",
      "12\n",
      "1\n",
      "11\n",
      "11\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5626\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5627\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5628\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5629\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5630\n",
      "1\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5631\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5632\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5633\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5634\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5635\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5636\n",
      "12\n",
      "13\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5637\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5638\n",
      "12\n",
      "12\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5639\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5640\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5641\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5642\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5643\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5644\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5645\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5646\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5647\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5648\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5649\n",
      "1\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5650\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5651\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5652\n",
      "12\n",
      "1\n",
      "4\n",
      "4\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5653\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5654\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5655\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5656\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5657\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5658\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5659\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5660\n",
      "12\n",
      "1\n",
      "22\n",
      "22\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5661\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5662\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5663\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5664\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5665\n",
      "1\n",
      "21\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5666\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5667\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5668\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5669\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5670\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5671\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5672\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5673\n",
      "12\n",
      "13\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5674\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5675\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5676\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5677\n",
      "1\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5678\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5679\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5680\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5681\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5682\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5683\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5684\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5685\n",
      "12\n",
      "1\n",
      "21\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5686\n",
      "1\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5687\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5688\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5689\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5690\n",
      "12\n",
      "12\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5691\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5692\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5693\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5694\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5695\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5696\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5697\n",
      "1\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5698\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5699\n",
      "12\n",
      "13\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5700\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5701\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5702\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5703\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5704\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5705\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5706\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5707\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5708\n",
      "12\n",
      "1\n",
      "9\n",
      "9\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5709\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5710\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5711\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5712\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5713\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5714\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5715\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5716\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5717\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5718\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5719\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5720\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5721\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5722\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5723\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5724\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5725\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5726\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5727\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5728\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5729\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5730\n",
      "12\n",
      "0\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5731\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5732\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5733\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5734\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5735\n",
      "1\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5736\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5737\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5738\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5739\n",
      "12\n",
      "1\n",
      "18\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5740\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5741\n",
      "12\n",
      "13\n",
      "21\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5742\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5743\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5744\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5745\n",
      "12\n",
      "13\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5746\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5747\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5748\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5749\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5750\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5751\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5752\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5753\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5754\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5755\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5756\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5757\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5758\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5759\n",
      "12\n",
      "1\n",
      "17\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5760\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5761\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5762\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5763\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5764\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5765\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5766\n",
      "12\n",
      "13\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5767\n",
      "12\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5768\n",
      "12\n",
      "1\n",
      "21\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5769\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5770\n",
      "1\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5771\n",
      "12\n",
      "13\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5772\n",
      "1\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5773\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5774\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5775\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5776\n",
      "12\n",
      "13\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5777\n",
      "12\n",
      "13\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5778\n",
      "12\n",
      "13\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5779\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5780\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5781\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5782\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5783\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5784\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5785\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5786\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5787\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5788\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5789\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5790\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5791\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5792\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5793\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5794\n",
      "12\n",
      "13\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5795\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5796\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5797\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5798\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5799\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5800\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5801\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5802\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5803\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5804\n",
      "12\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5805\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5806\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5807\n",
      "12\n",
      "1\n",
      "16\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5808\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5809\n",
      "12\n",
      "1\n",
      "9\n",
      "9\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5810\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5811\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5812\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5813\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5814\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5815\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5816\n",
      "1\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5817\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5818\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5819\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5820\n",
      "12\n",
      "13\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5821\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5822\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5823\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5824\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5825\n",
      "12\n",
      "21\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5826\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5827\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5828\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5829\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5830\n",
      "12\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5831\n",
      "12\n",
      "13\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5832\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5833\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5834\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5835\n",
      "12\n",
      "13\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5836\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5837\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5838\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5839\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5840\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5841\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5842\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5843\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5844\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5845\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5846\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5847\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5848\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5849\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5850\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5851\n",
      "12\n",
      "13\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5852\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5853\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5854\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5855\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5856\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5857\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5858\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5859\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5860\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5861\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5862\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5863\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5864\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5865\n",
      "12\n",
      "0\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5866\n",
      "12\n",
      "1\n",
      "22\n",
      "22\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5867\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5868\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5869\n",
      "12\n",
      "13\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5870\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5871\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5872\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5873\n",
      "12\n",
      "1\n",
      "19\n",
      "19\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5874\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5875\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5876\n",
      "12\n",
      "1\n",
      "7\n",
      "7\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5877\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5878\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5879\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5880\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5881\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5882\n",
      "12\n",
      "6\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5883\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5884\n",
      "12\n",
      "15\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5885\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5886\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5887\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5888\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5889\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5890\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5891\n",
      "15\n",
      "13\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5892\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5893\n",
      "1\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5894\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5895\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5896\n",
      "12\n",
      "13\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5897\n",
      "12\n",
      "13\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5898\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5899\n",
      "21\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5900\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5901\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5902\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5903\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5904\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5905\n",
      "12\n",
      "1\n",
      "22\n",
      "22\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5906\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5907\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5908\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5909\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5910\n",
      "1\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5911\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5912\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5913\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5914\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5915\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5916\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5917\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5918\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5919\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5920\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5921\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5922\n",
      "12\n",
      "18\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5923\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5924\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5925\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5926\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5927\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5928\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5929\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5930\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5931\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5932\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5933\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5934\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5935\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5936\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5937\n",
      "12\n",
      "13\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5938\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5939\n",
      "12\n",
      "6\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5940\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5941\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5942\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5943\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5944\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5945\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5946\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5947\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5948\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5949\n",
      "12\n",
      "13\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5950\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5951\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5952\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5953\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5954\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5955\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5956\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5957\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5958\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5959\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5960\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5961\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5962\n",
      "12\n",
      "1\n",
      "20\n",
      "20\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5963\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5964\n",
      "1\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5965\n",
      "12\n",
      "13\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5966\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5967\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5968\n",
      "12\n",
      "1\n",
      "17\n",
      "17\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5969\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5970\n",
      "12\n",
      "1\n",
      "21\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5971\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5972\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5973\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5974\n",
      "1\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5975\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5976\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5977\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5978\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5979\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5980\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5981\n",
      "12\n",
      "1\n",
      "20\n",
      "20\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5982\n",
      "12\n",
      "13\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5983\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5984\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5985\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5986\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5987\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5988\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5989\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5990\n",
      "12\n",
      "1\n",
      "19\n",
      "19\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5991\n",
      "12\n",
      "1\n",
      "21\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5992\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5993\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5994\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5995\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5996\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5997\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5998\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "5999\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6000\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6001\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6002\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6003\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6004\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6005\n",
      "12\n",
      "1\n",
      "18\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6006\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6007\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6008\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6009\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6010\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6011\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6012\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6013\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6014\n",
      "0\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6015\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6016\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6017\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6018\n",
      "12\n",
      "1\n",
      "14\n",
      "14\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6019\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6020\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6021\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6022\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6023\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6024\n",
      "1\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6025\n",
      "12\n",
      "13\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6026\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6027\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6028\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6029\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6030\n",
      "12\n",
      "1\n",
      "11\n",
      "11\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6031\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6032\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6033\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6034\n",
      "12\n",
      "1\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6035\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6036\n",
      "1\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6037\n",
      "12\n",
      "13\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6038\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6039\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6040\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6041\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6042\n",
      "12\n",
      "13\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6043\n",
      "12\n",
      "1\n",
      "3\n",
      "3\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6044\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6045\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6046\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6047\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6048\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6049\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6050\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6051\n",
      "12\n",
      "1\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6052\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6053\n",
      "12\n",
      "13\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6054\n",
      "12\n",
      "1\n",
      "18\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6055\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6056\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6057\n",
      "12\n",
      "13\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6058\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6059\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6060\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6061\n",
      "12\n",
      "13\n",
      "5\n",
      "5\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6062\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6063\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6064\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6065\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6066\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6067\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6068\n",
      "12\n",
      "1\n",
      "23\n",
      "23\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6069\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6070\n",
      "12\n",
      "1\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6071\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6072\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6073\n",
      "12\n",
      "13\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6074\n",
      "12\n",
      "1\n",
      "15\n",
      "15\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6075\n",
      "12\n",
      "8\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6076\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6077\n",
      "12\n",
      "1\n",
      "8\n",
      "8\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6078\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6079\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6080\n",
      "12\n",
      "1\n",
      "0\n",
      "0\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6081\n",
      "12\n",
      "13\n",
      "2\n",
      "2\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6082\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6083\n",
      "12\n",
      "13\n",
      "6\n",
      "6\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6084\n",
      "12\n",
      "1\n",
      "21\n",
      "21\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6085\n",
      "12\n",
      "1\n",
      "1\n",
      "1\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6086\n",
      "12\n",
      "1\n",
      "13\n",
      "13\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6087\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6088\n",
      "12\n",
      "1\n",
      "12\n",
      "12\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6089\n",
      "12\n",
      "1\n",
      "18\n",
      "18\n",
      "#########\n",
      "torch.float32\n",
      "torch.float32\n",
      "6090\n",
      "12\n",
      "1\n",
      "16\n",
      "16\n",
      "#########\n",
      "[12 12 12 ... 12 12 12]\n",
      "[1 1 1 ... 1 1 1]\n",
      "[ 1  1 21 ... 12 18 16]\n",
      "[ 1  1 21 ... 12 18 16]\n",
      "Positive Fidelity =  0.9394088669950739\n",
      "Negative Fidelity =  0.9339901477832513\n",
      "0.12335207072742753\n",
      "[10]\n",
      "[0.9394088669950739]\n",
      "[0.9339901477832513]\n",
      "[0.12335207072742753]\n",
      "  DummyExplainer\n",
      "/content/DummyExplainer_topk_results.csv\n",
      "   TopK  Fidelity+  Fidelity-  Characterization\n",
      "0    10   0.939409    0.93399          0.123352\n",
      "Run Time =  294.7390646934509\n",
      "<torch_geometric.loader.dataloader.DataLoader object at 0x7d09d2d18700>\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "#\n",
    "#!pip install torch_geometric\n",
    "#!pip install pdf2image\n",
    "#!apt-get install poppler-utils\n",
    "## !pip install captum\n",
    "#\n",
    "#from google.colab import files\n",
    "#files.upload()\n",
    "#\n",
    "#jupytext --to ipynb Comparative_Cheb_GCN.py\n",
    "# Don't forget to upload model.pth as well\n",
    "#!unzip numpy_data.zip\n",
    "\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.nn import GCNConv, Set2Set\n",
    "from torch_geometric.explain import GNNExplainer\n",
    "import torch_geometric.transforms as T\n",
    "import torch\n",
    "#import torch.nn.functional as F\n",
    "import os\n",
    "from tqdm import tqdm, trange\n",
    "#\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Loading saved model architecture\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.pool import global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        # self.conv1 = GCNConv(6, 512, improved = True)\n",
    "        # self.conv2 = GCNConv(512, 256, improved = True)\n",
    "\n",
    "        # self.conv1 = ChebConv(6, 512, K = 2)\n",
    "        # self.conv2 = ChebConv(512,256, K = 2)\n",
    "\n",
    "        self.conv1 = ChebConv(6, 256, K = 3)\n",
    "        self.conv2 = ChebConv(256,256, K = 4)\n",
    "        self.conv3 = ChebConv(256, 256, K = 5)\n",
    "        # self.fc1 = Linear(256, 512)\n",
    "        # self.fc2 = Linear(512, 256)\n",
    "\n",
    "        # self.conv1 = GATConv(6, 256, heads = 4)\n",
    "        # self.conv2 = GATConv(256*4,64, heads = 1, concat=False)\n",
    "\n",
    "        # self.conv1 = TransformerConv(6, 200, heads = 3)\n",
    "        # self.conv2 = TransformerConv(200*3,256, heads = 1, concat=False)\n",
    "\n",
    "        self.lin = Linear(256, 24)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # = global_max_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # x = self.fc1(x)\n",
    "        # x = x.relu()\n",
    "        # x = self.fc2(x)\n",
    "        # x = x.relu()\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.2, training=self.training) #0.5\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=464)\n",
    "print(model)\n",
    "\n",
    "# print(os.getcwd()+ \"/model.pth\")\n",
    "# model_save_location = os.getcwd()+ \"/model.pth\"\n",
    "print(os.getcwd()+ \"/model_34.pt\")\n",
    "model_save_location = os.getcwd()+ \"/model_34.pt\"\n",
    "model.load_state_dict(torch.load(model_save_location, map_location=torch.device('cpu')))\n",
    "print(model.conv3)\n",
    "print(model.state_dict())\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "print(os.getcwd())\n",
    "\n",
    "# +\n",
    "import time\n",
    "#Extract Training Data\n",
    "#Loads IEEE34 Bus Simulation Data\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "numpy_data = np.load(path+\"/new_data.npy\",allow_pickle = True)\n",
    "numpy_data = np.squeeze(numpy_data)\n",
    "print(numpy_data)\n",
    "print(numpy_data.shape)\n",
    "print(numpy_data.size)\n",
    "\n",
    "value_data = np.load(path+\"/new_labels.npy\",allow_pickle = True)\n",
    "print(value_data)\n",
    "print(value_data.shape)\n",
    "print(value_data.size)\n",
    "\n",
    "\n",
    "NodeIndex = np.load(path+\"/NodeIndex.npy\",allow_pickle = True)\n",
    "print(NodeIndex)\n",
    "print(NodeIndex.shape)\n",
    "print(NodeIndex.size)\n",
    "\n",
    "AdjacencyMatrix = np.load(path+\"/AdjacencyMatrix.npy\",allow_pickle = True)\n",
    "print(AdjacencyMatrix)\n",
    "print(AdjacencyMatrix.shape)\n",
    "print(AdjacencyMatrix.size)\n",
    "\n",
    "# +\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "encoder = ['SourceBus', '800', '802', '806', '808', '810', '812', '814', '814r', '850',\n",
    "'816', '818', '824', '820', '822', '826', '828', '830', '854', '832', '858',\n",
    "'834', '860', '842', '836', '840', '862', '844', '846', '848', '852r', '888', '856', '852', '864', '838', '890']\n",
    "print(encoder)\n",
    "print(len(encoder))\n",
    "research_paper_decoder = [0,0,1,2,3,4,5,6,6,6,6,7,8,9,10,11,8,12,12,13,14,15,21,15,16,16,16,17,18,18,13,13,19,13,20,22,23]\n",
    "\n",
    "FaultLocationLabels = value_data[:,3]\n",
    "\n",
    "for n in range(len(FaultLocationLabels)):\n",
    "    FaultLocationLabels[n]=research_paper_decoder[encoder.index(str(FaultLocationLabels[n]))]\n",
    "\n",
    "y = FaultLocationLabels.astype(\"int64\")\n",
    "y = torch.from_numpy(y)\n",
    "x = numpy_data.astype(\"float32\")\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "NodeIndex = NodeIndex.astype(\"int64\")\n",
    "NodeIndex = NodeIndex.T\n",
    "NodeIndex = torch.from_numpy(NodeIndex)\n",
    "\n",
    "print(y)\n",
    "print(y.dtype)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "print(NodeIndex)\n",
    "print(NodeIndex.dtype)\n",
    "#\n",
    "print(x[0])\n",
    "print(normalize(x[0]))\n",
    "#\n",
    "result_translator = np.unique(FaultLocationLabels.astype(\"int64\")).tolist()\n",
    "print()\n",
    "total_data_list = []\n",
    "for n in range(len(x)):\n",
    "    #print(x[n])\n",
    "    #print(y[n])\n",
    "    DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True) #Testing with non-normalized data\n",
    "    #DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True)\n",
    "    DataObject.is_undirected = True\n",
    "    total_data_list.append(DataObject)\n",
    "#print('Y'*888)\n",
    "#print(total_data_list[0].x)\n",
    "\n",
    "print()\n",
    "#print(f'Dataset: {total_data_list}:')\n",
    "print('===================')\n",
    "print(f'Number of graphs: {len(total_data_list)}')\n",
    "print(f'Number of features: {total_data_list[0].num_features}')\n",
    "#print(f'Number of classes: {total_data_list[0].num_classes}')\n",
    "\n",
    "data = total_data_list[0]  # Get the first graph object.\n",
    "#print(data)\n",
    "#print(data.y)\n",
    "\n",
    "print()\n",
    "print(NormalizeFeatures(data.x))\n",
    "print(data.x)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "from random import shuffle\n",
    "torch.manual_seed(12345)\n",
    "#total_data_list = total_data_list.shuffle()\n",
    "shuffle(total_data_list)\n",
    "\n",
    "train_dataset = total_data_list[:14640] #9150 is half\n",
    "test_dataset = total_data_list[14640:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "\n",
    "print(test_dataset[0].x)\n",
    "print(test_dataset[0])\n",
    "#print(train_dataset[0].x)\n",
    "\n",
    "for sample in range(len(test_dataset)):\n",
    "    # noise = np.random.normal(1,0.03, size = (37,6)) #0.09 #Uncomment for noise\n",
    "    # noise = noise.astype(\"float32\")\n",
    "    # noise = torch.from_numpy(noise)\n",
    "    # test_dataset[sample].x = noise*test_dataset[sample].x\n",
    "    test_dataset[sample].x = normalize(test_dataset[sample].x)\n",
    "    #print(sample)\n",
    "\n",
    "print('Y'*888)\n",
    "#print(train_dataset[0].x)\n",
    "print(test_dataset[0].x)\n",
    "print(test_dataset[0])\n",
    "##noise = np.random.normal(1,0.09, size = (16,6))\n",
    "##noise = noise.astype(\"float32\")\n",
    "##noise = torch.from_numpy(noise)\n",
    "print('=============================================================')\n",
    "##\n",
    "print(train_dataset[0].x)\n",
    "print(train_dataset[0])\n",
    "#print(train_dataset[0].x)\n",
    "\n",
    "for sample in range(len(train_dataset)):\n",
    "#    noise = np.random.normal(1,0.09, size = (16,6))\n",
    "#    #    noise = np.random.normal(1,0.09, size = (1,96))\n",
    "#    noise = noise.astype(\"float32\")\n",
    "#    noise = torch.from_numpy(noise)\n",
    "##    print(noise)\n",
    "##    X_test[sample] = X_test[sample]*noise[0]\n",
    "#    train_dataset[sample].x = noise*train_dataset[sample].x\n",
    "    train_dataset[sample].x = normalize(train_dataset[sample].x)\n",
    "    #print(sample)\n",
    "\n",
    "print('Y'*888)\n",
    "#print(train_dataset[0].x)\n",
    "print(train_dataset[0].x)\n",
    "print(train_dataset[0])\n",
    "##print(noise*test_dataset[0].x)\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "print(\"$\"*200)\n",
    "\n",
    "#################################################################################\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.datasets import ExplainerDataset\n",
    "from torch_geometric.datasets.graph_generator import BAGraph\n",
    "from torch_geometric.explain import Explainer, GNNExplainer, GraphMaskExplainer\n",
    "from torch_geometric.explain import DummyExplainer, PGExplainer\n",
    "from torch_geometric.nn import GCN\n",
    "from torch_geometric.utils import k_hop_subgraph\n",
    "\n",
    "\n",
    "##dataset = ExplainerDataset(\n",
    "##    graph_generator=BAGraph(num_nodes=300, num_edges=5),\n",
    "##    motif_generator='house',\n",
    "##    num_motifs=80,\n",
    "##    transform=T.Constant(),\n",
    "##)\n",
    "#data = test_dataset[0]\n",
    "#print(data)\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.explain import Explainer, Explanation\n",
    "from torch_geometric.explain.config import ExplanationType, ModelMode\n",
    "\n",
    "\n",
    "pos_fidelity_list = []\n",
    "neg_fidelity_list = []\n",
    "characterization_list = []\n",
    "#topk_list = [1,5,10,15]\n",
    "topk_list = [10]\n",
    "# topk_list = [1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80]\n",
    "for topk in topk_list:\n",
    "    explainer = Explainer(\n",
    "        model=model,\n",
    "    #    algorithm=GNNExplainer(epochs=200),\n",
    "    #    algorithm=PGExplainer(epochs=2),\n",
    "        # algorithm=GraphMaskExplainer(num_layers = 3, epochs=200),\n",
    "       algorithm=DummyExplainer(),\n",
    "        explanation_type='model',\n",
    "        node_mask_type='attributes',\n",
    "        edge_mask_type='object',\n",
    "        model_config=dict(\n",
    "            mode='multiclass_classification',\n",
    "            task_level='graph',\n",
    "            return_type='log_probs',\n",
    "        ),\n",
    "        threshold_config=dict(\n",
    "#            threshold_type='topk',\n",
    "            threshold_type='topk_hard',\n",
    "            value =topk,\n",
    "        ),\n",
    "    )\n",
    "    count = 0\n",
    "    explain_y_hat_list = []\n",
    "    complement_y_hat_list = []\n",
    "    y_list = []\n",
    "    y_hat_list = []\n",
    "\n",
    "    for batch in test_loader:\n",
    "        count+=1\n",
    "        Edge_Index = batch.edge_index[:,:72]\n",
    "        Batch = batch.batch[:37]\n",
    "        label1 = batch.y[0].numpy().tolist()#\n",
    "        input = batch.x[0:37]#\n",
    "\n",
    "        #Uncomment lines below if running the PGExplainer (only supports phenomenon-based explanation)\n",
    "    #    for epoch in range(2):\n",
    "    #        loss = explainer.algorithm.train(epoch=epoch, model=model, x = input, edge_index=Edge_Index, batch = Batch)\n",
    "\n",
    "        explanation = explainer(x = input, edge_index = Edge_Index, batch = Batch)#, target = label1\n",
    "        y_hat = model(input, Edge_Index, Batch).argmax(dim=1).item()\n",
    "    #path = 'subgraph.pdf'\n",
    "    #explanation.visualize_graph(path)\n",
    "        node_mask = explanation.get('node_mask')\n",
    "        edge_mask = explanation.get('edge_mask')\n",
    "\n",
    "        #Two lines below focus on converting the soft masks to hard masks\n",
    "     #   node_mask = torch.tensor((node_mask-node_mask.mean())>0, dtype=torch.float32)\n",
    "    #    edge_mask = torch.tensor((edge_mask-edge_mask.mean())>0, dtype=torch.float32)\n",
    "\n",
    "#        print(node_mask)\n",
    "#        print(edge_mask)\n",
    "    #    print(\"7\"*88)\n",
    "        kwargs = {key: explanation[key] for key in explanation._model_args}\n",
    "        print(node_mask.dtype)\n",
    "        print(edge_mask.dtype)\n",
    "    #    y = explanation.target\n",
    "        y = label1\n",
    "        explain_y_hat = explainer.get_masked_prediction(\n",
    "                explanation.x,\n",
    "                explanation.edge_index,\n",
    "                node_mask,\n",
    "                edge_mask,\n",
    "                **kwargs,\n",
    "            )\n",
    "        explain_y_hat = explainer.get_target(explain_y_hat)\n",
    "        complement_y_hat = explainer.get_masked_prediction(\n",
    "            explanation.x,\n",
    "            explanation.edge_index,\n",
    "            1. - node_mask if node_mask is not None else None,\n",
    "            1. - edge_mask if edge_mask is not None else None,\n",
    "            **kwargs,\n",
    "        )\n",
    "        complement_y_hat = explainer.get_target(complement_y_hat)\n",
    "\n",
    "        explain_y_hat_list.append(explain_y_hat.numpy()[0])\n",
    "        complement_y_hat_list.append(complement_y_hat.numpy()[0])\n",
    "        y_list.append(y)\n",
    "        y_hat_list.append(y_hat)\n",
    "\n",
    "        print(count)\n",
    "        print(explain_y_hat.numpy()[0])\n",
    "        print(complement_y_hat.numpy()[0])\n",
    "        print(y)\n",
    "        print(y_hat)\n",
    "        print(\"#########\")\n",
    "\n",
    "#        if count > 1:\n",
    "#            break\n",
    "\n",
    "    explain_y_hat_list = np.array(explain_y_hat_list)\n",
    "    complement_y_hat_list = np.array(complement_y_hat_list)\n",
    "    y_list = np.array(y_list)\n",
    "    y_hat_list = np.array(y_hat_list)\n",
    "\n",
    "    print(explain_y_hat_list )\n",
    "    print(complement_y_hat_list )\n",
    "    print(y_list )\n",
    "    print(y_hat_list)\n",
    "    #.astype('float').mean()\n",
    "\n",
    "    pos_fidelity = 1. - (complement_y_hat_list == y_hat_list).astype('float').mean()\n",
    "    neg_fidelity = 1. - (explain_y_hat_list == y_hat_list).astype('float').mean()\n",
    "    print(\"Positive Fidelity = \", pos_fidelity) #positive fidelity reflects necessary explanation\n",
    "    # It's better for positive fidelity to be 1\n",
    "    print(\"Negative Fidelity = \", neg_fidelity) #negative fidelity reflects sufficient explanation\n",
    "    # It's better for negative fidelity to be 0\n",
    "    CharacterizationScore = 1./((0.5/pos_fidelity)+(0.5/(1-neg_fidelity)))\n",
    "    print(CharacterizationScore)\n",
    "\n",
    "    pos_fidelity_list.append(pos_fidelity)\n",
    "    neg_fidelity_list.append(neg_fidelity)\n",
    "    characterization_list.append(CharacterizationScore)\n",
    "\n",
    "print(topk_list)\n",
    "print(pos_fidelity_list)\n",
    "print(neg_fidelity_list)\n",
    "print(characterization_list)\n",
    "\n",
    "Topk_Results = pd.DataFrame(data = {'TopK' : topk_list,\n",
    "    'Fidelity+' : pos_fidelity_list,\n",
    "    'Fidelity-' : neg_fidelity_list,\n",
    "    'Characterization' : characterization_list})\n",
    "\n",
    "#Topk_Results.to_csv(path+'/'+str(explainer.algorithm)[:-2]+'_topk_results.csv', index = False)\n",
    "Topk_Results.to_csv(path+'/'+str(explainer.algorithm)[:-2]+'_topk_hard_results.csv', index = False)\n",
    "#.attribution_method_class.__name__\n",
    "print(\"  \"+str(explainer.algorithm)[:-2])\n",
    "#print(type(explainer.algorithm.attribution_method_class))\n",
    "print(path+'/'+str(explainer.algorithm)[:-2]+'_topk_results.csv')\n",
    "print(Topk_Results)\n",
    "\n",
    "end_time = time.time()\n",
    "print(\"Run Time = \", end_time-start_time)\n",
    "\n",
    "print(test_loader)\n",
    "\n",
    "\n",
    "\n",
    "# from IPython.display import IFrame\n",
    "\n",
    "\n",
    "\n",
    "# # Specify the path to your PDF file\n",
    "# pdf_path = 'subgraph.pdf'\n",
    "\n",
    "# # Display the PDF inline\n",
    "# IFrame(pdf_path, width=600, height=300)\n",
    "\n",
    "# from pdf2image import convert_from_path\n",
    "\n",
    "# images = convert_from_path(\"subgraph.pdf\")\n",
    "# images[0]  # first page\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "#from pdf2image import convert_from_path\n",
    "#from IPython.display import display, Image\n",
    "#\n",
    "#images = convert_from_path(\"subgraph.pdf\")\n",
    "#for i, image in enumerate(images):\n",
    "#    fname = \"image\" + str(i) + \".png\"\n",
    "#    image.save(fname, \"PNG\")\n",
    "#Image(fname, width=800, height=1500)\n",
    "#\n",
    "#print(NodeIndex)\n",
    "#print(NodeIndex.shape)\n",
    "#print(NodeIndex.size)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

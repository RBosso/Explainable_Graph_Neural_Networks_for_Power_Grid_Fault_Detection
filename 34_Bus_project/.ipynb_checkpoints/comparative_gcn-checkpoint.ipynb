{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e058b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch_geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4cdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "files.upload()\n",
    "\n",
    "#jupytext --to ipynb Comparative_Cheb_GCN.py\n",
    "!unzip numpy_data.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749e9b06",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Comparative Chebyshev GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cebe6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "import networkx as nx\n",
    "os.environ['TORCH'] = torch.__version__\n",
    "print(torch.__version__)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10072f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "#Extract Training Data\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "numpy_data = np.load(path+\"/new_data.npy\",allow_pickle = True)\n",
    "numpy_data = np.squeeze(numpy_data)\n",
    "print(numpy_data)\n",
    "print(numpy_data.shape)\n",
    "print(numpy_data.size)\n",
    "\n",
    "value_data = np.load(path+\"/new_labels.npy\",allow_pickle = True)\n",
    "print(value_data)\n",
    "print(value_data.shape)\n",
    "print(value_data.size)\n",
    "\n",
    "\n",
    "NodeIndex = np.load(path+\"/NodeIndex.npy\",allow_pickle = True)\n",
    "print(NodeIndex)\n",
    "print(NodeIndex.shape)\n",
    "print(NodeIndex.size)\n",
    "\n",
    "AdjacencyMatrix = np.load(path+\"/AdjacencyMatrix.npy\",allow_pickle = True)\n",
    "print(AdjacencyMatrix)\n",
    "print(AdjacencyMatrix.shape)\n",
    "print(AdjacencyMatrix.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32518fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "encoder = ['SourceBus', '800', '802', '806', '808', '810', '812', '814', '814r', '850', \n",
    "'816', '818', '824', '820', '822', '826', '828', '830', '854', '832', '858', \n",
    "'834', '860', '842', '836', '840', '862', '844', '846', '848', '852r', '888', '856', '852', '864', '838', '890']\n",
    "print(encoder)\n",
    "print(len(encoder))\n",
    "research_paper_decoder = [0,0,1,2,3,4,5,6,6,6,6,7,8,9,10,11,8,12,12,13,14,15,21,15,16,16,16,17,18,18,13,13,19,13,20,22,23]\n",
    "\n",
    "FaultLocationLabels = value_data[:,3]\n",
    "\n",
    "for n in range(len(FaultLocationLabels)):\n",
    "    FaultLocationLabels[n]=research_paper_decoder[encoder.index(str(FaultLocationLabels[n]))]\n",
    "\n",
    "y = FaultLocationLabels.astype(\"int64\")\n",
    "y = torch.from_numpy(y)\n",
    "x = numpy_data.astype(\"float32\")\n",
    "x = torch.from_numpy(x)\n",
    "\n",
    "NodeIndex = NodeIndex.astype(\"int64\")\n",
    "NodeIndex = NodeIndex.T\n",
    "NodeIndex = torch.from_numpy(NodeIndex)\n",
    "\n",
    "print(y)\n",
    "print(y.dtype)\n",
    "print(x)\n",
    "print(x.dtype)\n",
    "print(NodeIndex)\n",
    "print(NodeIndex.dtype)\n",
    "#\n",
    "print(x[0])\n",
    "print(normalize(x[0]))\n",
    "#\n",
    "result_translator = np.unique(FaultLocationLabels.astype(\"int64\")).tolist()\n",
    "print()\n",
    "total_data_list = []\n",
    "for n in range(len(x)):\n",
    "    #print(x[n])\n",
    "    #print(y[n])\n",
    "    DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True) #Testing with non-normalized data\n",
    "    #DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True)\n",
    "    DataObject.is_undirected = True\n",
    "    total_data_list.append(DataObject)\n",
    "#print('Y'*888)\n",
    "#print(total_data_list[0].x)\n",
    "\n",
    "print()\n",
    "#print(f'Dataset: {total_data_list}:')\n",
    "print('===================')\n",
    "print(f'Number of graphs: {len(total_data_list)}')\n",
    "print(f'Number of features: {total_data_list[0].num_features}')\n",
    "#print(f'Number of classes: {total_data_list[0].num_classes}')\n",
    "\n",
    "data = total_data_list[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(NormalizeFeatures(data.x))\n",
    "print(data.x)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "from random import shuffle\n",
    "torch.manual_seed(12345)\n",
    "#total_data_list = total_data_list.shuffle()\n",
    "shuffle(total_data_list)\n",
    "\n",
    "train_dataset = total_data_list[:3975] #3975\n",
    "test_dataset = total_data_list[3975:]\n",
    "\n",
    "print(f'Number of training graphs: {len(train_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "\n",
    "print(test_dataset[0].x)\n",
    "print(test_dataset[0])\n",
    "#print(train_dataset[0].x)\n",
    "\n",
    "for sample in range(len(test_dataset)):\n",
    "    noise = np.random.normal(1,0.06, size = (37,6)) #0.09\n",
    "    #    noise = np.random.normal(1,0.09, size = (1,96))\n",
    "    noise = noise.astype(\"float32\")\n",
    "    noise = torch.from_numpy(noise)\n",
    "#    print(noise)\n",
    "#    X_test[sample] = X_test[sample]*noise[0]\n",
    "    test_dataset[sample].x = noise*test_dataset[sample].x\n",
    "    test_dataset[sample].x = normalize(test_dataset[sample].x)\n",
    "    #print(sample)\n",
    "\n",
    "print('Y'*888)\n",
    "#print(train_dataset[0].x)\n",
    "print(test_dataset[0].x)\n",
    "print(test_dataset[0])\n",
    "##noise = np.random.normal(1,0.09, size = (16,6))\n",
    "##noise = noise.astype(\"float32\")\n",
    "##noise = torch.from_numpy(noise)\n",
    "print('=============================================================')\n",
    "##\n",
    "print(train_dataset[0].x)\n",
    "print(train_dataset[0])\n",
    "#print(train_dataset[0].x)\n",
    "\n",
    "for sample in range(len(train_dataset)):\n",
    "#    noise = np.random.normal(1,0.09, size = (16,6))\n",
    "#    #    noise = np.random.normal(1,0.09, size = (1,96))\n",
    "#    noise = noise.astype(\"float32\")\n",
    "#    noise = torch.from_numpy(noise)\n",
    "##    print(noise)\n",
    "##    X_test[sample] = X_test[sample]*noise[0]\n",
    "#    train_dataset[sample].x = noise*train_dataset[sample].x\n",
    "    train_dataset[sample].x = normalize(train_dataset[sample].x)\n",
    "    #print(sample)\n",
    "\n",
    "print('Y'*888)\n",
    "#print(train_dataset[0].x)\n",
    "print(train_dataset[0].x)\n",
    "print(train_dataset[0])\n",
    "##print(noise*test_dataset[0].x)\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=18, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=18, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9937fda1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import ChebConv\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn import TransformerConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn.pool import global_max_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        #self.conv1 = GCNConv(6, 256, improved = True)\n",
    "        #self.conv2 = GCNConv(256, 64, improved = True)\n",
    "\n",
    "        self.conv1 = ChebConv(6, 256, K = 2)\n",
    "        self.conv2 = ChebConv(256,64, K = 2)\n",
    "\n",
    "        # self.conv1 = GATConv(6, 256, heads = 4)\n",
    "        # self.conv2 = GATConv(256*4,64, heads = 1, concat=False)\n",
    "\n",
    "        # self.conv1 = TransformerConv(6, 256, heads = 3)\n",
    "        # self.conv2 = TransformerConv(256*3,64, heads = 1, concat=False)\n",
    "\n",
    "        self.lin = Linear(64, 24)\n",
    "\n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # 1. Obtain node embeddings\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "       #x = self.conv3(x, edge_index)\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "        # = global_max_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.lin(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=464)\n",
    "print(model)\n",
    "\n",
    "from IPython.display import Javascript\n",
    "#display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
    "\n",
    "model = GCN(hidden_channels=464)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "         loss = criterion(out, data.y)  # Compute the loss.\n",
    "         loss.backward()  # Derive gradients.\n",
    "         optimizer.step()  # Update parameters based on gradients.\n",
    "         optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "def test(loader):\n",
    "     model.eval()\n",
    "     if model.training is True:\n",
    "         print(\"Model is Not Testing Properly\")\n",
    "     correct = 0\n",
    "     for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "         out = model(data.x, data.edge_index, data.batch)\n",
    "         pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "         correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "     return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ae627",
   "metadata": {},
   "outputs": [],
   "source": [
    " %%time\n",
    " print(result_translator)\n",
    " for epoch in range(1, 99):\n",
    "     train()\n",
    "     train_acc = test(train_loader)\n",
    "     test_acc = test(test_loader)\n",
    "     print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089dd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC Metric for PyTorch Models\n",
    "#from torcheval.metrics.aggregation.auc import AUC\n",
    "#metric = AUC()\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.nn import Softmax\n",
    "\n",
    "Soft_list = np.array([[]]).reshape((0,24))\n",
    "values_list = np.array([])\n",
    "pred_list = np.array([])\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data in test_loader:\n",
    "        out = model(data.x, data.edge_index, data.batch)\n",
    "        pred= out.argmax(dim=1)\n",
    "        softmax = Softmax()\n",
    "        softout = softmax(out)\n",
    "        y = data.y\n",
    "        Soft_list = np.concatenate((Soft_list, softout.numpy()), axis=0)\n",
    "        pred_list = np.concatenate((pred_list, pred.numpy()), axis=0)\n",
    "        # print(Soft_list)\n",
    "        values_list= np.concatenate((values_list, y.numpy()), axis=None)\n",
    "   # print(out)\n",
    "   # print('*'*888)\n",
    "   # print(softout)\n",
    "   # print('*'*888)\n",
    "print(Soft_list)\n",
    "print(Soft_list.shape)\n",
    "print(values_list)\n",
    "print(values_list.shape)\n",
    "print(pred_list)\n",
    "print(pred_list.shape)\n",
    "\n",
    "print(\"The Area Under ROC Curve (AUC) =\", roc_auc_score(values_list, Soft_list, multi_class='ovr'))\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print(\"F1 Score =\", f1_score(values_list, pred_list, average = 'macro'))\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(\"Testing Accuracy =\",accuracy_score(values_list, pred_list))\n",
    "\n",
    "# Confusion Matrix for PyTorch Models\n",
    "encoder = ['SourceBus', '800', '802', '806', '808', '810', '812', '814', '814r', '850', \n",
    "'816', '818', '824', '820', '822', '826', '828', '830', '854', '832', '858', \n",
    "'834', '860', '842', '836', '840', '862', '844', '846', '848', '852r', '888', '856', '852', '864', '838', '890']\n",
    "print(encoder)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "confusion_mat_labels = [\"Src/800\", \"802\",\"806\",\"808\",\"810\",\"812\",\"814/850/816\",\"818\",\"824/828\",\"820\",\"822\",\"826\",\"830/854\",\"852/832/888\",\"858\",\n",
    "        \"834/842\",\"836/840/862\",\"844\",\"846/848\",\"856\",\"864\",\"860\",\"838\",\"890\"]\n",
    "\n",
    "values_list = values_list.astype(object)\n",
    "pred_list = pred_list.astype(object)\n",
    "\n",
    "for faults in range(24):\n",
    "    pred_list[pred_list==faults] = confusion_mat_labels[faults]\n",
    "    values_list[values_list==faults] = confusion_mat_labels[faults]\n",
    "\n",
    "print(confusion_mat_labels)\n",
    "print(np.unique(values_list))\n",
    "print(np.unique(pred_list))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(values_list, pred_list, labels=confusion_mat_labels)#, labels=classes\n",
    "ConfusionMatrixDisplay(cm, display_labels=confusion_mat_labels).plot(xticks_rotation = 35).ax_.set_title(\"Cheb GCN Confusion Matrix\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

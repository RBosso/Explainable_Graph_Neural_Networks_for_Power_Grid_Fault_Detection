# -*- coding: utf-8 -*-
"""GraphMask_Explainer_123.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WfIOcO-Lrf8ZppfkbbMCakxl-0dFWEbG
"""

# !pip install torch_geometric
# !pip install captum
#
#from google.colab import files
#files.upload()
#
##jupytext --to ipynb Comparative_Cheb_GCN.py
## Don't forget to upload model as well
# !unzip numpy_data.zip

import time
start_time = time.time()

#
# !pip install torch_geometric
# !pip install pdf2image
# !apt-get install poppler-utils
# # !pip install captum
#
#from google.colab import files
#files.upload()
#
#jupytext --to ipynb Comparative_Cheb_GCN.py
# Don't forget to upload model.pth as well
# !unzip numpy_data.zip

from torch_geometric.data import Data, DataLoader
from torch_geometric.datasets import TUDataset, Planetoid
from torch_geometric.nn import GCNConv, Set2Set
from torch_geometric.explain import GNNExplainer
import torch_geometric.transforms as T
import torch
#import torch.nn.functional as F
import os
from tqdm import tqdm, trange
#
import matplotlib.pyplot as plt

#Loading saved model architecture
from torch.nn import Linear
import torch.nn.functional as F
from torch_geometric.nn import GCNConv
from torch_geometric.nn import ChebConv
from torch_geometric.nn import GATConv
from torch_geometric.nn import TransformerConv
from torch_geometric.nn import global_mean_pool
from torch_geometric.nn.pool import global_max_pool


class GCN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GCN, self).__init__()
        torch.manual_seed(12345)
        # self.conv1 = GCNConv(6, 512, improved = True)
        # self.conv2 = GCNConv(512, 256, improved = True)

        # self.conv1 = ChebConv(6, 512, K = 2)
        # self.conv2 = ChebConv(512,256, K = 2)

        self.conv1 = ChebConv(6, 256, K = 3)
        self.conv2 = ChebConv(256,256, K = 4)
        self.conv3 = ChebConv(256, 256, K = 5)
        # self.fc1 = Linear(256, 512)
        # self.fc2 = Linear(512, 256)

        # self.conv1 = GATConv(6, 256, heads = 4)
        # self.conv2 = GATConv(256*4,64, heads = 1, concat=False)

        # self.conv1 = TransformerConv(6, 200, heads = 3)
        # self.conv2 = TransformerConv(200*3,256, heads = 1, concat=False)

        self.lin = Linear(256, 120)

    def forward(self, x, edge_index, batch):
        # 1. Obtain node embeddings
        x = self.conv1(x, edge_index)
        x = x.relu()

        x = self.conv2(x, edge_index)
        x = x.relu()
        x = self.conv3(x, edge_index)
        x = x.relu()

        # 2. Readout layer
        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]
        # = global_max_pool(x, batch)  # [batch_size, hidden_channels]

        # x = self.fc1(x)
        # x = x.relu()
        # x = self.fc2(x)
        # x = x.relu()

        # 3. Apply a final classifier
        x = F.dropout(x, p=0.2, training=self.training) #0.5
        x = self.lin(x)

        return x

model = GCN(hidden_channels=464)
print(model)

# print(os.getcwd()+ "/model.pth")
# model_save_location = os.getcwd()+ "/model.pth"
print(os.getcwd()+ "/model_123.pt")
model_save_location = os.getcwd()+ "/model_123.pt"
model.load_state_dict(torch.load(model_save_location, map_location=torch.device('cpu')))
print(model.conv3)
print(model.state_dict())

import numpy as np
import pandas as pd
import time
import matplotlib.pyplot as plt
import os
import torch
import networkx as nx
os.environ['TORCH'] = torch.__version__
print(torch.__version__)
print(os.getcwd())

# +
import time
#Extract Training Data
#Loads IEEE34 Bus Simulation Data

path = os.getcwd()

numpy_data = np.load(path+"/new_data.npy",allow_pickle = True)
numpy_data = np.squeeze(numpy_data)
print(numpy_data)
print(numpy_data.shape)
print(numpy_data.size)

value_data = np.load(path+"/new_labels.npy",allow_pickle = True)
print(value_data)
print(value_data.shape)
print(value_data.size)


NodeIndex = np.load(path+"/NodeIndex.npy",allow_pickle = True)
print(NodeIndex)
print(NodeIndex.shape)
print(NodeIndex.size)

AdjacencyMatrix = np.load(path+"/AdjacencyMatrix.npy",allow_pickle = True)
print(AdjacencyMatrix)
print(AdjacencyMatrix.shape)
print(AdjacencyMatrix.size)

# +
from torch_geometric.data import Data
from torch_geometric.loader import DataLoader
from torch_geometric.transforms import NormalizeFeatures
from torch.nn.functional import normalize


encoder = ['150', '150R', '149', '1', '2', '3', '7', '4', '5', '6', '8', '12',
'9', '13', '9R', '14', '34', '18', '11', '10', '15', '16', '17', '19',
'21', '20', '22', '23', '24', '25', '25R', '26', '28', '27', '31',
'33', '29', '30', '250', '32', '35', '36', '40', '37', '38', '39',
'41', '42', '43', '44', '45', '47', '46', '48', '49', '50', '51',
'151', '52', '53', '54', '55', '57', '56', '58', '60', '59', '61',
'62', '63', '64', '65', '66', '67', '68', '72', '97', '69', '70',
'71', '73', '76', '74', '75', '77', '86', '78', '79', '80', '81',
'82', '84', '83', '85', '87', '88', '89', '90', '91', '92', '93',
'94', '95', '96', '98', '99', '100', '450', '197', '101', '102',
'105', '103', '104', '106', '108', '107', '109', '300', '110',
'111', '112', '113', '114', '135', '152', '160R', '160', '61S', '610']
#encoder = ['SourceBus', '800', '802', '806', '808', '810', '812', '814', '814r', '850',
#'816', '818', '824', '820', '822', '826', '828', '830', '854', '832', '858',
#'834', '860', '842', '836', '840', '862', '844', '846', '848', '852r', '888', '856', '852', '864', '838', '890']
print(encoder)
print(len(encoder))
#research_paper_decoder = [0,0,1,2,3,4,5,6,6,6,6,7,8,9,10,11,8,12,12,13,14,15,21,15,16,16,16,17,18,18,13,13,19,13,20,22,23]
research_paper_decoder = [0,0,0,1,2,3,4,5,6,7,8,9,10,11,10,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,72,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,14,11,61,61,63,119]
print(research_paper_decoder)
print(len(research_paper_decoder))
FaultLocationLabels = value_data[:,3]

for n in range(len(FaultLocationLabels)):
    FaultLocationLabels[n]=research_paper_decoder[encoder.index(str(FaultLocationLabels[n]).upper())]



y = FaultLocationLabels.astype("int64")
y = torch.from_numpy(y)
x = numpy_data.astype("float32")
x = torch.from_numpy(x)

NodeIndex = NodeIndex.astype("int64")
NodeIndex = NodeIndex.T
NodeIndex = torch.from_numpy(NodeIndex)

print(y)
print(y.dtype)
print(x)
print(x.dtype)
print(NodeIndex)
print(NodeIndex.dtype)
#
print(x[0])
print(normalize(x[0]))
#
result_translator = np.unique(FaultLocationLabels.astype("int64")).tolist()
print()
total_data_list = []
for n in range(len(x)):
    #print(x[n])
    #print(y[n])
    DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True) #Testing with non-normalized data
    #DataObject = Data(x = x[n], edge_index = NodeIndex, y = y[n], is_undirected = True)
    DataObject.is_undirected = True
    total_data_list.append(DataObject)
#print('Y'*888)
#print(total_data_list[0].x)

print()
#print(f'Dataset: {total_data_list}:')
print('===================')
print(f'Number of graphs: {len(total_data_list)}')
print(f'Number of features: {total_data_list[0].num_features}')
#print(f'Number of classes: {total_data_list[0].num_classes}')

data = total_data_list[0]  # Get the first graph object.
#print(data)
#print(data.y)

print()
print(NormalizeFeatures(data.x))
print(data.x)
print('=============================================================')

# Gather some statistics about the first graph.
print(f'Number of nodes: {data.num_nodes}')
print(f'Number of edges: {data.num_edges}')
print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')
print(f'Has isolated nodes: {data.has_isolated_nodes()}')
print(f'Has self-loops: {data.has_self_loops()}')
print(f'Is undirected: {data.is_undirected()}')

from random import shuffle
torch.manual_seed(12345)
#total_data_list = total_data_list.shuffle()
shuffle(total_data_list)

train_dataset = total_data_list[:26000] #9150 is half
test_dataset = total_data_list[26000:]

print(f'Number of training graphs: {len(train_dataset)}')
print(f'Number of test graphs: {len(test_dataset)}')


print(test_dataset[0].x)
print(test_dataset[0])
#print(train_dataset[0].x)

for sample in range(len(test_dataset)):
    # noise = np.random.normal(1,0.03, size = (37,6)) #0.09 #Uncomment for noise
    # noise = noise.astype("float32")
    # noise = torch.from_numpy(noise)
    # test_dataset[sample].x = noise*test_dataset[sample].x
    test_dataset[sample].x = normalize(test_dataset[sample].x)
    #print(sample)

print('Y'*888)
#print(train_dataset[0].x)
print(test_dataset[0].x)
print(test_dataset[0])
##noise = np.random.normal(1,0.09, size = (16,6))
##noise = noise.astype("float32")
##noise = torch.from_numpy(noise)
print('=============================================================')
##
print(train_dataset[0].x)
print(train_dataset[0])
#print(train_dataset[0].x)

for sample in range(len(train_dataset)):
#    noise = np.random.normal(1,0.09, size = (16,6))
#    #    noise = np.random.normal(1,0.09, size = (1,96))
#    noise = noise.astype("float32")
#    noise = torch.from_numpy(noise)
##    print(noise)
##    X_test[sample] = X_test[sample]*noise[0]
#    train_dataset[sample].x = noise*train_dataset[sample].x
    train_dataset[sample].x = normalize(train_dataset[sample].x)
    #print(sample)

print('Y'*888)
#print(train_dataset[0].x)
print(train_dataset[0].x)
print(train_dataset[0])
##print(noise*test_dataset[0].x)
# Gather some statistics about the first graph.
print(f'Number of nodes: {data.num_nodes}')
print(f'Number of edges: {data.num_edges}')
print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')
print(f'Has isolated nodes: {data.has_isolated_nodes()}')
print(f'Has self-loops: {data.has_self_loops()}')
print(f'Is undirected: {data.is_undirected()}')


train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)

print("$"*200)

#################################################################################
import torch
import torch.nn.functional as F
from sklearn.metrics import roc_auc_score
from sklearn.model_selection import train_test_split
from tqdm import tqdm

import torch_geometric.transforms as T
from torch_geometric.datasets import ExplainerDataset
from torch_geometric.datasets.graph_generator import BAGraph
from torch_geometric.explain import Explainer, GNNExplainer, GraphMaskExplainer
from torch_geometric.explain import DummyExplainer, PGExplainer
from torch_geometric.nn import GCN
from torch_geometric.utils import k_hop_subgraph


##dataset = ExplainerDataset(
##    graph_generator=BAGraph(num_nodes=300, num_edges=5),
##    motif_generator='house',
##    num_motifs=80,
##    transform=T.Constant(),
##)
#data = test_dataset[0]
#print(data)

from typing import Tuple

import torch
from torch import Tensor

from torch_geometric.explain import Explainer, Explanation
from torch_geometric.explain.config import ExplanationType, ModelMode


pos_fidelity_list = []
neg_fidelity_list = []
characterization_list = []
#topk_list = [1,5,10,15]
topk_list = [50]
# topk_list = [1,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80]
for topk in topk_list:
    explainer = Explainer(
        model=model,
    #    algorithm=GNNExplainer(epochs=200),
    #    algorithm=PGExplainer(epochs=2),
        # algorithm=GraphMaskExplainer(num_layers = 3, epochs=200),
       algorithm=DummyExplainer(),
        explanation_type='model',
        node_mask_type='attributes',
        edge_mask_type='object',
        model_config=dict(
            mode='multiclass_classification',
            task_level='graph',
            return_type='log_probs',
        ),
        threshold_config=dict(
#            threshold_type='topk',
            threshold_type='topk_hard',
            value =topk,
        ),
    )
    count = 0
    explain_y_hat_list = []
    complement_y_hat_list = []
    y_list = []
    y_hat_list = []

    for batch in test_loader:
        count+=1
        Edge_Index = batch.edge_index[:,:252]
        Batch = batch.batch[:130]
        label1 = batch.y[0].numpy().tolist()#
        input = batch.x[0:130]#

        #Uncomment lines below if running the PGExplainer (only supports phenomenon-based explanation)
    #    for epoch in range(2):
    #        loss = explainer.algorithm.train(epoch=epoch, model=model, x = input, edge_index=Edge_Index, batch = Batch)

        y_hat = model(input, Edge_Index, Batch).argmax(dim=1).item()
        explanation = explainer(x = input, edge_index = Edge_Index, batch = Batch)#, target = label1
    #path = 'subgraph.pdf'
    #explanation.visualize_graph(path)
        node_mask = explanation.get('node_mask')
        edge_mask = explanation.get('edge_mask')

        #Two lines below focus on converting the soft masks to hard masks
     #   node_mask = torch.tensor((node_mask-node_mask.mean())>0, dtype=torch.float32)
    #    edge_mask = torch.tensor((edge_mask-edge_mask.mean())>0, dtype=torch.float32)

#        print(node_mask)
#        print(edge_mask)
    #    print("7"*88)
        kwargs = {key: explanation[key] for key in explanation._model_args}
        print(node_mask.dtype)
        print(edge_mask.dtype)
    #    y = explanation.target
        y = label1
        explain_y_hat = explainer.get_masked_prediction(
                explanation.x,
                explanation.edge_index,
                node_mask,
                edge_mask,
                **kwargs,
            )
        explain_y_hat = explainer.get_target(explain_y_hat)
        complement_y_hat = explainer.get_masked_prediction(
            explanation.x,
            explanation.edge_index,
            1. - node_mask if node_mask is not None else None,
            1. - edge_mask if edge_mask is not None else None,
            **kwargs,
        )
        complement_y_hat = explainer.get_target(complement_y_hat)

        explain_y_hat_list.append(explain_y_hat.numpy()[0])
        complement_y_hat_list.append(complement_y_hat.numpy()[0])
        y_list.append(y)
        y_hat_list.append(y_hat)

        print(count)
        print(explain_y_hat.numpy()[0])
        print(complement_y_hat.numpy()[0])
        print(y)
        print(y_hat)
        print("#########")

#        if count > 1:
#            break

    explain_y_hat_list = np.array(explain_y_hat_list)
    complement_y_hat_list = np.array(complement_y_hat_list)
    y_list = np.array(y_list)
    y_hat_list = np.array(y_hat_list)

    print(explain_y_hat_list )
    print(complement_y_hat_list )
    print(y_list )
    print(y_hat_list)
    #.astype('float').mean()

    pos_fidelity = 1. - (complement_y_hat_list == y_hat_list).astype('float').mean()
    neg_fidelity = 1. - (explain_y_hat_list == y_hat_list).astype('float').mean()
    print("Positive Fidelity = ", pos_fidelity) #positive fidelity reflects necessary explanation
    # It's better for positive fidelity to be 1
    print("Negative Fidelity = ", neg_fidelity) #negative fidelity reflects sufficient explanation
    # It's better for negative fidelity to be 0
    CharacterizationScore = 1./((0.5/pos_fidelity)+(0.5/(1-neg_fidelity)))
    print(CharacterizationScore)

    pos_fidelity_list.append(pos_fidelity)
    neg_fidelity_list.append(neg_fidelity)
    characterization_list.append(CharacterizationScore)

print(topk_list)
print(pos_fidelity_list)
print(neg_fidelity_list)
print(characterization_list)

Topk_Results = pd.DataFrame(data = {'TopK' : topk_list,
    'Fidelity+' : pos_fidelity_list,
    'Fidelity-' : neg_fidelity_list,
    'Characterization' : characterization_list})

#Topk_Results.to_csv(path+'/'+str(explainer.algorithm)[:-2]+'_topk_results.csv', index = False)
Topk_Results.to_csv(path+'/'+str(explainer.algorithm)[:-2]+'_topk_hard_results.csv', index = False)
#.attribution_method_class.__name__
print("  "+str(explainer.algorithm)[:-2])
#print(type(explainer.algorithm.attribution_method_class))
print(path+'/'+str(explainer.algorithm)[:-2]+'_topk_results.csv')
print(Topk_Results)

end_time = time.time()
print("Run Time = ", end_time-start_time)

print(test_loader)



# from IPython.display import IFrame



# # Specify the path to your PDF file
# pdf_path = 'subgraph.pdf'

# # Display the PDF inline
# IFrame(pdf_path, width=600, height=300)

# from pdf2image import convert_from_path

# images = convert_from_path("subgraph.pdf")
# images[0]  # first page




#
#from pdf2image import convert_from_path
#from IPython.display import display, Image
#
#images = convert_from_path("subgraph.pdf")
#for i, image in enumerate(images):
#    fname = "image" + str(i) + ".png"
#    image.save(fname, "PNG")
#Image(fname, width=800, height=1500)
#
#print(NodeIndex)
#print(NodeIndex.shape)
#print(NodeIndex.size)
